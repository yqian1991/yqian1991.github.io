<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu of Daphne</title>
  
  <subtitle>春秋笔法·丹枫嫩寒</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yqian1991.github.io/"/>
  <updated>2020-12-20T14:43:22.496Z</updated>
  <id>http://yqian1991.github.io/</id>
  
  <author>
    <name>Yu Qian</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>圣诞 + 新家</title>
    <link href="http://yqian1991.github.io/Life/%E5%9C%A3%E8%AF%9E-%E6%96%B0%E5%AE%B6/"/>
    <id>http://yqian1991.github.io/Life/圣诞-新家/</id>
    <published>2020-12-20T14:39:17.000Z</published>
    <updated>2020-12-20T14:43:22.496Z</updated>
    
    <content type="html"><![CDATA[<p>潘雨辰小朋友画的圣诞树<br><img src="./Christmas.jpeg" alt="Christmas Tree"></p><p>听说我们要搬新家了，潘雨辰小朋友把我们的新家外观画出来了，真的挺像。<br><img src="./newHome.png" alt="New Home"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;潘雨辰小朋友画的圣诞树&lt;br&gt;&lt;img src=&quot;./Christmas.jpeg&quot; alt=&quot;Christmas Tree&quot;&gt;&lt;/p&gt;
&lt;p&gt;听说我们要搬新家了，潘雨辰小朋友把我们的新家外观画出来了，真的挺像。&lt;br&gt;&lt;img src=&quot;./newHome.png&quot; al
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="We" scheme="http://yqian1991.github.io/tags/We/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications (9) - Consistency and Consensus</title>
    <link href="http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-9-Consistency-and-Consensus/"/>
    <id>http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-9-Consistency-and-Consensus/</id>
    <published>2020-11-10T04:01:54.000Z</published>
    <updated>2020-11-11T01:45:09.582Z</updated>
    
    <content type="html"><![CDATA[<p>This is my personal learning notes from reading &lt;<designing data-intensive="" applications="">&gt; Chapter 8 and 9.</designing></p><p>This chapter is probably the hardest one in this book to understand.</p><p>The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.</p><p>one of the most important abstractions for distributed systems is <em>consensus</em>, getting all of the nodes to agree on something and reliably reaching consensus in spite of network faults and process failures.</p><h1 id="Consistency-Guarantees"><a href="#Consistency-Guarantees" class="headerlink" title="Consistency Guarantees"></a>Consistency Guarantees</h1><h2 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h2><p>A strong consistency model is to make replicated data appear as if there were only a single copy, and to make all operations act on it atomically.</p><p>In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written.</p><p>But this model may not be very efficient, while weaker consistency models can be much faster, so this trade-off is important for latency-sensitive systems. Actually, most of the current systems don’t implement linearizability.</p><h2 id="Causality-Consistency-and-Ordering-Guarantees"><a href="#Causality-Consistency-and-Ordering-Guarantees" class="headerlink" title="Causality Consistency and Ordering Guarantees"></a>Causality Consistency and Ordering Guarantees</h2><p>Causal consistency instead is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p><p>Unlike linearizability, which puts all operations in a single, totally ordered timeline, causality provides a weaker consistency model: some things can be concurrent, so the version history is like a timeline with branching and merging. This also implies that <em>ordering</em> is very important.</p><p>How to capture casual ordering</p><ul><li>Sequence number ordering</li><li>Non casual sequence number generators</li><li>Lamport timestamp</li></ul><p>In order to implement uniqueness constraint, it’s not sufficient to have a total ordering of operations, you also need to know when that order is finalized, this is what <code>total order broadcast</code> can do.</p><h2 id="Total-order-broadcast"><a href="#Total-order-broadcast" class="headerlink" title="Total order broadcast"></a>Total order broadcast</h2><p>Total order broadcast is usually described as a protocol for exchanging messages between nodes, it is what needed for database replication.</p><p>Properties of total order broadcast:</p><ul><li><p>Reliable delivery: No messages are lost, if a message is delivered to one node, it is delivered to all nodes.</p></li><li><p>Total ordered delivery: Messages are delivered to every node in the same order.</p></li></ul><p>These properties actually can be implemented based on consensus algorithms.</p><h2 id="Eventual-Consistency"><a href="#Eventual-Consistency" class="headerlink" title="Eventual Consistency"></a>Eventual Consistency</h2><p>This is widely adopted in industry. for example Amazon DynamoDB.</p><h1 id="Distributed-Transactions-and-Consensus"><a href="#Distributed-Transactions-and-Consensus" class="headerlink" title="Distributed Transactions and Consensus"></a>Distributed Transactions and Consensus</h1><p>Consensus provides total order broadcast, and therefore can be used to implement linearizable atomic operations in a fault-tolerant way.</p><h2 id="Two-phase-commit"><a href="#Two-phase-commit" class="headerlink" title="Two-phase commit"></a>Two-phase commit</h2><p>2PC is a simple consensus algorithm, first we need to differentiate it with two phase locking.</p><p><em>Two-phase locking</em> is used in databases to ensure serializability, meaning that outcome of concurrent operations can be seen as the outcome of the same operations performed in some sequential order. The algorithm describes how a transaction in a database should deal with locking a part of the database in order to read or write.</p><p><em>Two-phase commit</em> is used in distributed systems in a process of distributed transaction with a single coordinator and several followers.</p><ul><li><p>The first phase: The coordinator first asks all of the followers to commit a transactions, then every follower executes the command and if it succeeds then it replies with an agreement to commit, otherwise it replies with an abort. The voting phase.</p></li><li><p>The second phase: When all agreement messages are received, coordinator commits the transaction. If there is an abort message from any follower, the coordinator aborts the transaction.</p></li></ul><p>What if coordinator crashed?</p><h2 id="Fault-tolerant-consensus"><a href="#Fault-tolerant-consensus" class="headerlink" title="Fault tolerant consensus"></a>Fault tolerant consensus</h2><p>Fault tolerant consensus algorithms compared to 2PC should have the following properties:</p><ul><li><p>Uniform agreement: No two nodes decide differently.</p></li><li><p>Integrity: No node decides twice.</p></li><li><p>Validity: If a node decides value v, then v was proposed by some node.</p></li><li><p>Termination: Every node that does not crash eventually decides some value.</p></li></ul><p>Zookeeper ZAB, Paxos and Raft are some popular consensus algorithms.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my personal learning notes from reading &amp;lt;&lt;designing data-intensive=&quot;&quot; applications=&quot;&quot;&gt;&amp;gt; Chapter 8 and 9.&lt;/designing&gt;&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications (7) - Transactions</title>
    <link href="http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-7-Trsansactions/"/>
    <id>http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-7-Trsansactions/</id>
    <published>2020-10-29T22:10:43.000Z</published>
    <updated>2020-12-15T02:26:49.322Z</updated>
    
    <content type="html"><![CDATA[<p>This is my personal learning notes from reading &lt;<designing data-intensive="" applications="">&gt; Chapter 7.</designing></p><h1 id="What-is-transaction"><a href="#What-is-transaction" class="headerlink" title="What is transaction?"></a>What is transaction?</h1><p>A transaction consists of multiple operations, only consider a transaction succeed when all the operations succeed. We are familiar with this when we learning database knowledge.</p><p>But in distributed environment, it means more. For example, when client fire a request to the distributed system, to return a response to the client, the backend may needs to fire multiple service calls to other services, any of them fail during the process may cause no response to the client, so we need to handle transactions in this semantic as well.</p><p>Transaction has the following properties:</p><ul><li><p>Atomicity: The smallest operation that can’t be separated further. In case of errors, database should abort the transaction, all changes already made must be discarded.</p></li><li><p>Isolation: Concurrent executions are isolated from each other, they can’t step on others’ toe.</p></li><li><p>Consistency: A transaction can only bring the database from one valid state to another, this actually relies on the app to achieve, for example application needs to define some rules on your table to define a valid operation (is amount allowed to be below 0? etc).</p></li><li><p>Durability: Avoid data loss, usually backup and disaster recovery should be in place.</p></li></ul><h1 id="What-makes-transaction-difficult"><a href="#What-makes-transaction-difficult" class="headerlink" title="What makes transaction difficult?"></a>What makes transaction difficult?</h1><p>Most of databases have good support of atomicity and isolation, while here we should think more than database level. Things become more difficult when we take application and business requirements into consideration. For example:</p><ul><li>If a transaction fail, what the application should do? (retry but keep it idempotent)</li><li>How to handle race condition if transaction has weak isolation? there are so many different cases of race conditions.</li><li>Write skew and phantoms: two transactions update two different objects, but violate business rules.</li></ul><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>Concurrency control(avoid race condition) maybe the most difficult problem in transaction. Different isolation levels are used to solve this problems:</p><h2 id="Read-Committed"><a href="#Read-Committed" class="headerlink" title="Read Committed"></a>Read Committed</h2><p>The most basic level of transaction isolation and it is also the default setting for many databases used to prevent <code>Dirty reads/writes</code>.</p><p>When reading/writing to database, only <code>committed</code> data can be seen/overwritten.</p><ul><li>Dirty reads: Transaction 1 modifies a row. Transaction 2 reads the same row before Transaction 1 commits.</li><li>Dirty writes: Transaction 1 modifies row A, transaction 2 modifies row A after transaction modified A. then transaction 1 wants to modify row B and check row A again, but now row A contains the data that modified by transaction 2 which is totally flawed.</li></ul><h2 id="Snapshot-isolation-and-repeatable-read"><a href="#Snapshot-isolation-and-repeatable-read" class="headerlink" title="Snapshot isolation and repeatable read"></a>Snapshot isolation and repeatable read</h2><p>Read committed can’t solve <code>read skew</code>: read values in the middle of another transaction, and after that transaction finished, the read values are changed (non repeatable read).</p><p>This indicates that before a transaction finished, we shouldn’t allow to read the intermediate data.</p><p>Snapshot isolation ensures the database to return old values(from a snapshot) that before a transaction, this means that database needs to maintain multiple version of object(Multiple Version Concurrency Control).</p><p>The read committed and snapshot isolation levels have been primarily about the guarantees of what a read-only transaction can see in the presence of concurrent writes.</p><p>But what about write, two transactions want to do read-modify-write at the same time, one of them could be lost.</p><ul><li>Use atomic write operation.</li><li>Explicit locking: Explicitly lock objects that are going to be updated, if any other transaction tries to concurrently read the same object, it is forced to wait until the first read-modify-write cycle has completed.</li><li>Detect/resolve lost update: This is useful in a distributed system that data has multiple replicas, then conflict resolution mentioned in previous blogs can be used.</li></ul><h2 id="Serializability"><a href="#Serializability" class="headerlink" title="Serializability"></a>Serializability</h2><p><code>Write skew/phantom read</code>: two transactions read the same object but update different objects. in real world, a ticketing system needs to handle this issue carefully(how to not issue ticket to two people when there is only one left).</p><p>To handle this problem, previous solution won’t work because two transactions are updating different objects. A very naive way of handling this issue is to avoid concurrency, transform two transactions to execute serially. But it’s very obviously that it has performance concerns.</p><p>Serialization is also the strongest isolation level, writers don’t just block other writers; they also block readers and vice versa.</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://tikv.github.io/deep-dive-tikv/distributed-transaction/distributed-algorithms.html" target="_blank" rel="noopener">https://tikv.github.io/deep-dive-tikv/distributed-transaction/distributed-algorithms.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my personal learning notes from reading &amp;lt;&lt;designing data-intensive=&quot;&quot; applications=&quot;&quot;&gt;&amp;gt; Chapter 7.&lt;/designing&gt;&lt;/p&gt;
&lt;h1 id=&quot;
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications (6) - Data Partition</title>
    <link href="http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-6-Data-Partition/"/>
    <id>http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-6-Data-Partition/</id>
    <published>2020-10-29T19:48:46.000Z</published>
    <updated>2020-10-29T20:52:12.495Z</updated>
    
    <content type="html"><![CDATA[<p>This is my personal learning notes from reading &lt;<designing data-intensive="" applications="">&gt; Chapter 6.</designing></p><h1 id="What-is-data-partition"><a href="#What-is-data-partition" class="headerlink" title="What is data partition?"></a>What is data partition?</h1><p>Partitioning means instead of parking all the data in the same place, we divide them into multiple pieces, and each piece of data stores on its own partition.</p><p>The purpose of data partition:</p><ul><li>Scalability</li></ul><h1 id="What-makes-partitioning-difficult"><a href="#What-makes-partitioning-difficult" class="headerlink" title="What makes partitioning difficult?"></a>What makes partitioning difficult?</h1><ul><li>How to assign data to a partition? we want to avoid skewed partition, hot spot partition.</li></ul><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><h2 id="Partitioning-by-key-range"><a href="#Partitioning-by-key-range" class="headerlink" title="Partitioning by key range"></a>Partitioning by key range</h2><p>This is to pre-assign a range of keys to each partition, but certain assigned keys can lead to hot spot still. To resolve this, you can add timestamp to the key.</p><p>This way makes it fit better for range queries as data can be sorted inside each partition plus we know the location of keys.</p><h2 id="Partitioning-by-hash-of-key"><a href="#Partitioning-by-hash-of-key" class="headerlink" title="Partitioning by hash of key"></a>Partitioning by hash of key</h2><p>Refer to Consistent hashing, this is introduced by the creation of Amazon DynamoDB, Cassandra benefits the same way.</p><h2 id="Rebalancing-parititons"><a href="#Rebalancing-parititons" class="headerlink" title="Rebalancing parititons"></a>Rebalancing parititons</h2><p>This probably falls to DevOps world, Rebalancing partitions is needed when:</p><ul><li>A machine failed, you need to move the data to other nodes</li><li>Simply want to add more nodes to balance the data because of query throughput increase</li></ul><p>To achieve this you can:</p><ul><li><p>Use fixed number of partitions, but make sure number of partitions are more than the nodes, so when rebalancing, you just need to move around the entire partition to different nodes, data partitioning strategy is not affected.</p></li><li><p>Dynamic partitioning: This is good for key-ranged partitioning.</p></li><li><p>Partitioning Proportionally to nodes: When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. This is basically what Cassandra does, this way works because of hash based partitioning.</p></li></ul><h2 id="Request-Routing"><a href="#Request-Routing" class="headerlink" title="Request Routing"></a>Request Routing</h2><p>This basically requires the cluster to provide information about the partition of a certain key.<br>A central coordinator like Zookeeper can be used.</p><p>Some databases also use a decentralized way which means every node keeps a copy of the cluster information so that requests can be served by any node, Cassandra uses Gossip protocol to keep every node updated.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my personal learning notes from reading &amp;lt;&lt;designing data-intensive=&quot;&quot; applications=&quot;&quot;&gt;&amp;gt; Chapter 6.&lt;/designing&gt;&lt;/p&gt;
&lt;h1 id=&quot;
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications (5) - Data Replication</title>
    <link href="http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-5-Data-Replication/"/>
    <id>http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-5-Data-Replication/</id>
    <published>2020-10-29T18:25:18.000Z</published>
    <updated>2020-10-29T19:28:51.298Z</updated>
    
    <content type="html"><![CDATA[<p>This is my personal learning notes from reading &lt;<designing data-intensive="" applications="">&gt; Chapter 5.</designing></p><h1 id="What-is-replication"><a href="#What-is-replication" class="headerlink" title="What is replication?"></a>What is replication?</h1><p>Replication means keeping a copy of the exact same data on multiple places that are connected via network.</p><p>The purpose of replication:</p><ul><li>Fault tolerance: If part of the machine failed, other machines can still work.</li><li>Scale reads: Reads can be balanced to all replications to achieve read volume scaling, also you can select the closest replications as possible to serve reads.</li></ul><h1 id="What-makes-replication-difficult"><a href="#What-makes-replication-difficult" class="headerlink" title="What makes replication difficult?"></a>What makes replication difficult?</h1><p>If all machines store the same static data that never change, then replication is just a one hot effort, but what if the data is constantly changing?</p><ul><li>How to make sure changes applied to all replicas?</li><li>How to handle replication failures?</li></ul><p>To answer these questions, you’d also determine your target first. replication can happen for a database, API server, cache etc.</p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><h2 id="Leaders-followers-Replication-Architecture"><a href="#Leaders-followers-Replication-Architecture" class="headerlink" title="Leaders/followers Replication Architecture"></a>Leaders/followers Replication Architecture</h2><p>Write requests go to leader only, leader responsible for broadcasting new data to all replicas. Traditional relational database uses this way.</p><ul><li><p>Replication log or write ahead logs: The media of changed data capture.</p></li><li><p>Leader election: The mechanism to ensure leader always exists, leader exists to coordinate the replication. Algorithms of leader consensus: ZAB, RAFT, PAXOS.</p></li><li><p>Asynchronous VS Synchronous replication: The mechanism of applying data changes</p></li><li><p>Eventual Consistency: The promise/goal of distributed replication process.</p></li><li><p>Distributed transaction:</p></li><li><p>Conflict resolution: The problem arise during concurrent data update in a distributed environment, Conflict must be resolved in a way to ensure eventual consistency. Algorithms include: Last Write Win, Operational transformation, CRDT etc.</p></li><li><p>Multi-leader clusters: say we have two data centres, each data centre has a cluster with leader/followers setup,  and we need to sync data between these two data centres.</p></li></ul><h2 id="Non-Leader-Replication-Architecture"><a href="#Non-Leader-Replication-Architecture" class="headerlink" title="Non-Leader Replication Architecture"></a>Non-Leader Replication Architecture</h2><p>Read/Write requests go to all replicas at the same time, Amazon DynamoDB and Cassandra belong to this category.</p><p>Since all replicas are treated the same, to improve efficiency, quorum consistency for read/write is usually implemented, this means a read/write request is succeed only if <code>w</code> out of <code>n</code> total replicas succeed.</p><p>Then for the failed nodes, they need to catch up:</p><ul><li><p>Read repair: Repair missing data upon client requests.</p></li><li><p>Anti-entropy process: backend job to periodically check lag behind data and repair.</p></li></ul><p>The real implementation is way more complicated as you also need to consider what if the defined quorum can’t be achieved, a <code>hinted handoff</code> solution can be adopted.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my personal learning notes from reading &amp;lt;&lt;designing data-intensive=&quot;&quot; applications=&quot;&quot;&gt;&amp;gt; Chapter 5.&lt;/designing&gt;&lt;/p&gt;
&lt;h1 id=&quot;
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications (4) - Data Encoding and Evolution</title>
    <link href="http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-4-Data-Encoding-and-Evolution/"/>
    <id>http://yqian1991.github.io/System-Design/Designing-Data-Intensive-Applications-4-Data-Encoding-and-Evolution/</id>
    <published>2020-10-29T14:41:00.000Z</published>
    <updated>2020-10-29T17:18:45.838Z</updated>
    
    <content type="html"><![CDATA[<p>This is my personal learning notes from reading &lt;<designing data-intensive="" applications="">&gt; Chapter 4</designing></p><h1 id="Why-we-need-data-encoding"><a href="#Why-we-need-data-encoding" class="headerlink" title="Why we need data encoding?"></a>Why we need data encoding?</h1><p>Every application is built to working on some kind of data, think about the input, output or intermediate storage during process, they are all data. But based on usage, format, storage type, there are different types/representations of data.</p><p>For a system, there are usually two different representations of data:</p><ul><li>Data in memory: Usually optimized for efficient access and computation by CPU.</li><li>Data in persistent storage or over network: Usually needs to consider the efficiency, security during the transfer.</li></ul><p>You will find that a system needs to transfer between these two representations frequently. For example, when you create a Person object to represents person information, then you probably need to send it through http to client, store it to database, save it to cache etc, but can you still use the same object/class representation? usually not. Here is where data encoding comes in.</p><p>Encoding(serialization or marshalling) is defined as the translation from in-memory representation to a byte sequence.</p><p>The reverse process of encoding(serialization or marshalling) is called parsing (deserialization, unmarshalling).</p><h1 id="Language-Specific-Formats"><a href="#Language-Specific-Formats" class="headerlink" title="Language-Specific Formats"></a>Language-Specific Formats</h1><p>Since data encoding is just a common requirement, many languages/libraries provide built-in support.</p><ul><li>Java: <code>java.io.Serializable</code></li><li>Ruby: <code>Marshal</code></li><li>Python: <code>pickle</code></li><li>…</li></ul><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>Convenient</li></ul><h2 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h2><ul><li>Specific to a language, not support by other languages, not good for transmit.</li><li>Not optimized for efficiency: Based on the data you are working on</li><li>Lack of features: schema, versioning, compatibility.</li><li>Security concerns: language specific encoding method is too generic that it allows to instantiate arbitrary classes (because of no concept of schema)</li></ul><h1 id="Json-XML-CSV"><a href="#Json-XML-CSV" class="headerlink" title="Json, XML, CSV"></a>Json, XML, CSV</h1><h2 id="Advantages-1"><a href="#Advantages-1" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>More human readable</li><li>Widely supported</li></ul><h2 id="Disadvantages-1"><a href="#Disadvantages-1" class="headerlink" title="Disadvantages"></a>Disadvantages</h2><ul><li>Ambiguity: hard to distinguish different data types</li><li>Weak schema support</li><li>Somewhat complex</li></ul><p>Needless to say, Json/XML/CSV is still widely used in current applications, because of the ease of use, human readable.</p><h1 id="Binary-encoding"><a href="#Binary-encoding" class="headerlink" title="Binary encoding"></a>Binary encoding</h1><p>For data intensive applications, they often needs to transfer a huge amount of data between different processes and not always need human readable, in this case, Json/XML/CSV will become a bottleneck in terms of efficiency, thus a binary encoding can help.</p><p>Binary encoding transforms data to a byte sequence in a compact way.</p><ul><li><p>MessagePack: Saving more space than Json, but not a huge gain because every character in Json needs to be encoded.</p></li><li><p>Thrift: Requires a defined schema, so it can save more spaces than MessagePack by using tag to represent fields. It relies on code generation to manipulate data which is good for code static check.</p></li><li><p>Protocol Buffers: Very similar to Thrift at it’s core.</p></li><li><p>Avro: Schema required too, but it enhanced further than Thrift by compacting values together without field tags, so it can save even more space than Thrift. Because only values are compacted, it gives Avro another advantage that you can always generate new schema based on the data you received (Dynamically Generated Schema).</p></li></ul><h2 id="Schema-evolution"><a href="#Schema-evolution" class="headerlink" title="Schema evolution"></a>Schema evolution</h2><p>This topic is about how to handle schema changing but maintain forward/backward compatibility,</p><p>For Thrift and Protocol Buffer:</p><ul><li>When adding new field, also assign a new tag number, but make it optional</li><li>When removing a field, you can only remove an optional field</li><li>When changing data type of existing field: Totally depends on the data types support in specific technology, also checking the precision and truncation rules.</li></ul><p>For Avro:</p><ul><li>The key idea with Avro is that the writer’s schema and the reader’s schema don’t have to be the same—they only need to be compatible</li></ul><h1 id="Scenarios-where-data-encoding-and-schema-evolution-involved"><a href="#Scenarios-where-data-encoding-and-schema-evolution-involved" class="headerlink" title="Scenarios where data encoding and schema evolution involved"></a>Scenarios where data encoding and schema evolution involved</h1><ul><li>Design a database</li><li>Design API contract (REST, RPC): API versioning</li><li>Messaging passing: messages pub/sub through Kafka, Pulsa, RabbitMQ, Actor model(Akka)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my personal learning notes from reading &amp;lt;&lt;designing data-intensive=&quot;&quot; applications=&quot;&quot;&gt;&amp;gt; Chapter 4&lt;/designing&gt;&lt;/p&gt;
&lt;h1 id=&quot;W
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Domain Driven Design 101</title>
    <link href="http://yqian1991.github.io/System-Design/Domain-Driven-Design-101/"/>
    <id>http://yqian1991.github.io/System-Design/Domain-Driven-Design-101/</id>
    <published>2020-10-26T21:15:57.000Z</published>
    <updated>2020-10-26T23:46:39.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Reactive-Architecture"><a href="#Reactive-Architecture" class="headerlink" title="Reactive Architecture"></a>Reactive Architecture</h1><p>Reactive means a series of design principles for creating cohesive systems, it’s mostly guided by the famous Reactive manifesto:</p><ul><li>Responsive: This is the overall goal of the system, it needs to be responsive in any circumstances.</li><li>Elastic: The system needs to be responsive even under a ‘heavy’ load</li><li>Resilient: The system needs to be responsive under failure</li><li>Message driven: This is the key to achieve above goals</li></ul><h2 id="Reactive-Programming"><a href="#Reactive-Programming" class="headerlink" title="Reactive Programming"></a>Reactive Programming</h2><p>This is the difference between architecture and programming, architecture is usually a guideline/principles of designing a system, while programming is an implementation strategy of a reactive system.</p><p>It is <em>event driven</em> approach used in a single node/single micro service for data flow management and logic</p><p>For example, callback, declarative methods(map(), filter()).</p><p>Cool, then what is event driven:</p><ul><li>An event listener must be attached to the event source in order to consume it.</li><li>An event represents an action/fact</li></ul><p><em>Akka</em> is a great example that practises reactive programming.</p><h2 id="Reactive-System"><a href="#Reactive-System" class="headerlink" title="Reactive System"></a>Reactive System</h2><p>A reactive system is message driven, designed at architect level for cloud native, distributed system.</p><p>It can also be interpreted as a system that followed reactive architecture and its implementation may  involve reactive programming.</p><h1 id="Domain-Driven-Design"><a href="#Domain-Driven-Design" class="headerlink" title="Domain Driven Design"></a>Domain Driven Design</h1><p>DDD is an architectural approach to design large systems which has similar goal as reactive system.</p><p>DDD has a systematic theory to architect a system, by saying this, you will see a lot concepts using DDD.</p><h2 id="Identity-Domains"><a href="#Identity-Domains" class="headerlink" title="Identity Domains"></a>Identity Domains</h2><p>Domain experts model the real business requirements to domains, a domain usually defined based on  business context/knowledge, for example, in a retail company, Shipping department is a domain, inventory is another domain.</p><h2 id="Make-Ubiquitous-Languages"><a href="#Make-Ubiquitous-Languages" class="headerlink" title="Make Ubiquitous Languages"></a>Make Ubiquitous Languages</h2><p>Use a ubiquitous language to understand business or idea, to communicate between software developers and domain experts, ubiquitous language maybe different between different domains, because it is used to communicate inside a domain.</p><h2 id="Bounded-context"><a href="#Bounded-context" class="headerlink" title="Bounded context"></a>Bounded context</h2><p>When a domain is complex enough, it should be further break down to sub domain or a bounded context.</p><p>Now we can talk about how to identify domains, event storming is one way to achieve this, domain experts gather in front of a white board, listing out all events/activities that can happen in the system using using <em>subject-verb-object</em> notation.</p><h2 id="Domain-Activities"><a href="#Domain-Activities" class="headerlink" title="Domain Activities:"></a>Domain Activities:</h2><p>Some common types of domain activities include:</p><ul><li>Command: This means initiate a request, for example, register an account.</li><li>Query: This basically means the request to retrieving information and expecting to see the result.</li><li>Event: This means the fact that something happened in the past, for example, user account created.</li></ul><h2 id="Domain-Objects"><a href="#Domain-Objects" class="headerlink" title="Domain Objects:"></a>Domain Objects:</h2><ul><li>Value: A piece of information that is immutable</li><li>Entity(unique, mutable): This can be designed as a micro service.</li><li>Aggregation root:</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li>Domain abstraction: service(stateless), factory(create object), repository(delete object)</li><li>Anti corruption layer: to keep bounded context boundary clean and maintain purity</li></ul><h1 id="Visualizing-Software-Architecture"><a href="#Visualizing-Software-Architecture" class="headerlink" title="Visualizing Software Architecture"></a>Visualizing Software Architecture</h1><p>C4 model is one way to visualize the software architecture, it uses 4 level to guide you top-down visualization of a system.</p><ul><li>Context: The highest level of the architecture.</li><li>Container: A container is a deployable functional unit inside the context.</li><li>Component: A functional part of a container, it can be a module, lib etc.</li><li>Code: This is very similar to the data modelling</li></ul><h1 id="Other-topics-in-SOA"><a href="#Other-topics-in-SOA" class="headerlink" title="Other topics in SOA"></a>Other topics in SOA</h1><ul><li>Bulk heading: isolate failures to a specific area of the system, prevent cascading errors</li><li>Circuit breakers</li><li>Autonomy: less dependencies, self healing</li><li>Gateway services</li><li>CAP theory of distributed system</li><li>SAGAS: a long running transaction in distributed system, often using FSM(e.g akka persistence)</li><li>Delivery guarantee</li><li>CQRS: auditable system, split to command and query parts</li><li>Event sourcing: publish events as source of truth for every fact/state changes</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Reactive-Architecture&quot;&gt;&lt;a href=&quot;#Reactive-Architecture&quot; class=&quot;headerlink&quot; title=&quot;Reactive Architecture&quot;&gt;&lt;/a&gt;Reactive Architecture&lt;/
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
      <category term="Microservice" scheme="http://yqian1991.github.io/tags/Microservice/"/>
    
  </entry>
  
  <entry>
    <title>从中国到加拿大（二）-- 枫叶国体验</title>
    <link href="http://yqian1991.github.io/Life/%E4%BB%8E%E4%B8%AD%E5%9B%BD%E5%88%B0%E5%8A%A0%E6%8B%BF%E5%A4%A7%EF%BC%88%E4%BA%8C%EF%BC%89-%E6%9E%AB%E5%8F%B6%E5%9B%BD%E4%BD%93%E9%AA%8C/"/>
    <id>http://yqian1991.github.io/Life/从中国到加拿大（二）-枫叶国体验/</id>
    <published>2020-10-08T18:20:56.000Z</published>
    <updated>2020-10-08T18:29:20.942Z</updated>
    
    <content type="html"><![CDATA[<p>对于生长在国内一个普通小县城，上大学也不过在一个地级市，对北上广这种大城市根本没什么深刻体验的人来说，一下子要在一个陌生的国度开始生活，似乎跨越比较大，出国之前，想象了各种困难，但那个时候是兴奋和激动超越了真正对困难的思考。而也就是踏上加拿大土地的那一刻起，就慢慢决定了我为什么后来选择留下，一呆就是 7 年，一想到回国还开始纠结。</p><p>这一篇就讲一讲加拿大刷新我认知的地方，和国内不同的地方。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于生长在国内一个普通小县城，上大学也不过在一个地级市，对北上广这种大城市根本没什么深刻体验的人来说，一下子要在一个陌生的国度开始生活，似乎跨越比较大，出国之前，想象了各种困难，但那个时候是兴奋和激动超越了真正对困难的思考。而也就是踏上加拿大土地的那一刻起，就慢慢决定了我为
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="Canada" scheme="http://yqian1991.github.io/tags/Canada/"/>
    
      <category term="Study" scheme="http://yqian1991.github.io/tags/Study/"/>
    
      <category term="留学" scheme="http://yqian1991.github.io/tags/%E7%95%99%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>从中国到加拿大（一）-- 为什么出国 ？</title>
    <link href="http://yqian1991.github.io/Life/%E4%BB%8E%E4%B8%AD%E5%9B%BD%E5%88%B0%E5%8A%A0%E6%8B%BF%E5%A4%A7%EF%BC%88%E4%B8%80%EF%BC%89-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%87%BA%E5%9B%BD%EF%BC%9F/"/>
    <id>http://yqian1991.github.io/Life/从中国到加拿大（一）-为什么出国？/</id>
    <published>2020-10-08T14:13:33.000Z</published>
    <updated>2020-10-08T18:21:48.727Z</updated>
    
    <content type="html"><![CDATA[<p>2019、2020年的疫情让中国和其他国家形成了鲜明对比，中国经过数月的严格防控将疫情完美控制了，从开始的差生妥妥的成了优等生，反观其他国家例如欧洲诸国，以及我所在的加拿大，第二波疫情来势汹汹，而美国、印度似乎连第一波都还没有结束。伴之而来的不仅是日常生活的改变，对旅居海外的中国人来说，回国还是不回国也变成了一个热门话题。</p><p>然而对有些人，这个选择似乎就非常简单，可总有一部分人，比如说我，出于各种现实的考虑，充满纠结。哪怕没有疫情，这也是一个艰难的选择，就像大城床和小城房一样永远有讨论的空间，不禁回想，自己又是如何将自己处于这样一个艰难的处境呢？这又得从为什么出国说起。</p><h1 id="决定读研"><a href="#决定读研" class="headerlink" title="决定读研"></a>决定读研</h1><p>2012 年 – 本科毕业的前一年，通常是大家思考毕业出路的时候，当时的选择无非是：面试找工作、国内读研、出国读研。我当时主要是倾向于读研究生的，至于为什么，也无非是跟很多人一样，希望提升一下未来的竞争力（后来还一度尝试着申请过博士），内心深处可能也隐藏着本科毕业就面临职场的些许恐惧吧。这里我也不想再罗列数据证明本科就业到底有多残酷以至于吓到了我，但可以肯定的是当时我也没有深度剖析过到底有多残酷，可能只是想继续学习的思想，加上正好能暂时逃避找工作的心理，促使我们不断接受对这个选择有利的信息，进而又更巩固了读研的决心，乃至最后有底气向所有人宣告并解释自己的决定，似乎是为了赢得自己做出选择的合法性。</p><p>当然，读研并不是什么坏的决定，这里只是为了还原自己当时的决策过程。记得当时学校就有各种讲座关于出路的选择，辅导员、学长也通过各种方式帮助我们，鼓励我们思考什么是适合自己的。方法是正确的，思考也是重要的，但问题在于多少人真的知道什么是适合自己的，也许上大学之前，我们的人生就没需要我们自己做出什么重大的决定，小学，中学，高中，大学一切都是顺理成章在父母的指导下走来，即使中间出了岔子，也有父母帮我们提供方案。所以，其实对错不是主要的，而是我们面临决定时的心态，好像变得患得患失，优柔寡断。</p><h1 id="出国好像挺不错"><a href="#出国好像挺不错" class="headerlink" title="出国好像挺不错"></a>出国好像挺不错</h1><p>决定读研究生之后，又该继续选择在国内读研，还是出国读研了。在 2012 年之前，出国从来没出现在我的方案里，因为我觉得那是个昂贵的选择。然而，有关出国留学的信息却围绕在我们左右，学校每年都要展示一下出国留学率提升了多少，跟多少大学展开了国际合作，提供了多少互派交流生的机会，学院里又有多少新招的教师是从国外人才引进的，哪怕是我当时学院的老师，大部分都是有过海外学习或者工作经验的，上课的时候津津乐道的讲着在 IBM 工作的经历，讲着他以前的优秀的学生毕业后去哪里留学。尤其作为学习软件工程的学生，编程语言是国外发明的，当时用的大多数软件也是国外开发的，彼时中国似乎还是外包服务居多，因此我们专业还要求学习二外，而国内的互联网还在努力追赶世界的潮流，所以那个时候的学生都对国外软件行业有羡慕之情吧。人的内心似乎都有探索未知、渴望尝试的欲望，渐渐的，出国在我心里就演变成：不错的选择，只是经济负担太重而已。内心潜台词就是，只要条件允许，我还是想出国的（那个时候想的仅仅只是出国留学然后回国就业，出国后的变化后面会讲到）。</p><h1 id="下定决心"><a href="#下定决心" class="headerlink" title="下定决心"></a>下定决心</h1><p>在心里认可出国是个挺不错的选择之后，也去了解了下出国的条件，硬性指标里好像成绩也达标，托福或者雅思还有半年时间准备，但是经济问题怎么解决？既然自己出不了那么多钱（同时自己也是不想出太多钱的），那就只能找个便宜的地方，或者拿到全额奖学金了。果然，世界这么大，道路千万条，一次新加坡留学讲座为我打开了一扇窗，新加坡南洋理工大学居然就提供这种硕士项目，只要你愿意毕业后留新工作，政府可以负担学费，最后又有通过这个项目实现留学梦的学长现身说法，这无疑对我是有巨大吸引力的。从此，申请留学的准备工作开始了。</p><h1 id="初次艰难地尝试"><a href="#初次艰难地尝试" class="headerlink" title="初次艰难地尝试"></a>初次艰难地尝试</h1><p>2012 年上半年，基本都在准备雅思，为了考个高分，也投入了很多时间和精力，也不亚于复习考研了。2012 年下半年就得开始准备申请了，我的目标此时也从新加坡拓展到了香港（奔着申请直博去的，博士有奖学金）。期间趁香港大学一个教授去哈尔滨做讲座的时候还特意去面试过他的研究生名额，面试虽然被完虐（面试题现在看来跟 LeetCode上 的难度是 Hard 级别的动态规划题差不多），却没有打消我的积极性，开始尝试着读更多的学术论文加强自己的学术实力。有意思的是，后来我又去北京参加过香港大学的一场比较正式的研究生提前批招考，这种考试只在内地的合肥，北京，上海，广州，南京等地举行，我们这种在偏远地带的普通高校的学生根本就得不到通知，还好有以前的学长告诉我们要关注清北这些大学的学生论坛留学板块，后来我们真的从留学板块了解到了申请信息，并最终拿到了入场券。考试分两场，上午笔试，也都是算法、数学概率题；下午面试，港大会派出各个研究方向的老师来举行面试。这种招考的目的本来就是吸引国内最优秀的学生，所以结果可想而知，虽然很努力，但是仍落败。面试表现好的同学，可能当场就被教授发 Offer 了，更残酷的是，有些人在纠结要不要去，因为可能手上还攥着其他大学的 Offer， 比如香港科技大学，香港中文大学。</p><p>后来我还去广州参加过南洋理工大学同类型的考试，结果还是失败了，这类考试的要求确实高，因为很多人招过去都是当直博生培养的，所以如果本科学习期间没有比较强的研究经历或者学术成果，是很难脱颖而出的。虽然数次失败，但也增加了丰富的经历，也让我坚定了先申请研究生的目标，于是最终还是回到学校，继续正常申请之路了。</p><p>留学不仅仅是网上填个表那么简单，在全国已经是教育类的一类产业了，而在学校，申请留学也跟读研一样，有着自己的群体，每个在这个群体里的人，都把它当做自己这个阶段最主要的目标。背单词、刷论坛（寄托天下之类）、听讲座、套磁、了解签证等等。激情的投入让我们必须在这条路上有个满意的结果。多么羡慕有 plan B 的，而我们的 plan B 只是选个不同的国家，选个不同的学校，但是想让我们放弃留学就越来越难了。</p><h1 id="继续申请之路"><a href="#继续申请之路" class="headerlink" title="继续申请之路"></a>继续申请之路</h1><p>在把基本材料都准备好之后，就等学校开放申请了，可是新加坡南洋理工大学的申请开放得晚，需要等到 2013 年 2 月份，那时心里就稍微有点危机感了，如果 2 月份申请，结果到 6、7 月才能出来，万一没申请上，自己保研的机会也已经错过了，到时候再仓促的找工作，总觉得会很惨，更多的是不甘心。于是我开始考虑备选方案，也就是这个时候，加拿大进入我的视线，当时班里有其他几个同学早就开始关注加拿大了，我也是通过他们了解到的加拿大，美国在留学生心中肯定是不二之选，然而由于我们没有准备 SAT， 基本跟美国的学校无缘了，所以作为美国邻居而又经常被当做去美国的跳板的加拿大，就成了很多学生的选择。从那以后，我就开始更多的了解加拿大大学的信息，关注点也一直是选学校、选导师、准备材料，而这些也够我忙的了，一丁点也没有考虑真的去了加拿大会怎么样，满满的只是对未知世界的好奇和向往。毕竟当时只是想把加拿大作为保底方案，所以选了西安大略大学计算机系并且套磁了我后来的导师，由于教授对自己的学生有很大的自主权，在几轮面试之后，导师明确表达了接收并且愿意资助的意愿，只要我按正常流程继续申请，等申请到学院的时候，他会给出推荐意见。当时真是高兴坏了，连忙申请，并且在 2012 年的 12 月就收到了录取通知书，比其他申请的同学还快一点。</p><p>但是到底接受不接受呢，学校也算是加拿大 top 10，导师也很好，关键是有全额奖学金。纠结许久，我还是把 offer 接了，因为这个是 Conditional Offer，我想着到时候如果不想去，就以签证为理由拒绝掉就行。 2013 年，我就继续申请了南洋理工大学，并在7月份也拿到了 Offer。可是我就在这几个月期间发生了思想转变，最终还是决定去加拿大。这两个 offer，经济条件已经不是我需要考虑的因素了。其他方面，首先，加拿大的 Offer 是研究型硕士，更符合我对自己的学习规划（就是想学习一些比本科更深入的专业知识），对以后申请博士也比较有利；而新加坡是授课型硕士，从这一点上，我内心已经偏向加拿大的，其次听说加拿大拿绿卡很容易，离美国又近，以后不管是回国还是留在国外，选择非常自由。</p><p>作为一个快毕业的学生，能出国留学有什么不好 ？作为一个留学生，有机会选择留下或者回国，有什么不好， 我们不就是想给自己以后的人生提供更多的选择吗？从中国到加拿大，第一步就这样完成了，然而就像人生中所有的难以预料，你没有办法用上帝视角看清未来的每一步。这也就是后面纠结，忧虑，讨论产生的根源。生活本如此，而非因出国。</p><p>2013 年 8 月，我正式登陆加拿大开始留学生生活。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2019、2020年的疫情让中国和其他国家形成了鲜明对比，中国经过数月的严格防控将疫情完美控制了，从开始的差生妥妥的成了优等生，反观其他国家例如欧洲诸国，以及我所在的加拿大，第二波疫情来势汹汹，而美国、印度似乎连第一波都还没有结束。伴之而来的不仅是日常生活的改变，对旅居海外
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="Canada" scheme="http://yqian1991.github.io/tags/Canada/"/>
    
      <category term="留学" scheme="http://yqian1991.github.io/tags/%E7%95%99%E5%AD%A6/"/>
    
      <category term="China" scheme="http://yqian1991.github.io/tags/China/"/>
    
  </entry>
  
  <entry>
    <title>Enable JMX for Scala/Sbt Project</title>
    <link href="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/"/>
    <id>http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/</id>
    <published>2020-10-07T20:22:13.000Z</published>
    <updated>2020-10-07T21:34:01.191Z</updated>
    
    <content type="html"><![CDATA[<p>Java Management Extensions (JMX) is a Java technology that supplies tools for managing and monitoring applications, system objects, devices (such as printers) and service-oriented networks. Those resources are represented by objects called MBeans (for Managed Bean).</p><p>In this blog, I will show the steps to enable it for Scala/Sbt project and also show how to connect VisualVM to a remote server</p><h1 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h1><p>Follow the link below to install VisualVM: <a href="https://visualvm.github.io/" target="_blank" rel="noopener">https://visualvm.github.io/</a>.</p><p>If you don’t want to use VisualVM, Jconsole is an option too.</p><h1 id="Setup-sbt-project-with-JMX"><a href="#Setup-sbt-project-with-JMX" class="headerlink" title="Setup sbt project with JMX"></a>Setup sbt project with JMX</h1><p>Enable jmx settings in <code>build.sbt</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javaOptions <span class="keyword">in</span> Universal ++= Seq(</span><br><span class="line">  <span class="string">"-Djavax.management.builder.initial="</span>,</span><br><span class="line">  <span class="string">"-Djava.rmi.server.hostname=127.0.0.1"</span>,</span><br><span class="line">  <span class="string">"-Dcom.sun.management.jmxremote=true"</span>,</span><br><span class="line">  <span class="string">"-Dcom.sun.management.jmxremote.port=9186"</span>,</span><br><span class="line">  <span class="string">"-Dcom.sun.management.jmxremote.rmi.port=9186"</span>,</span><br><span class="line">  <span class="string">"-Dcom.sun.management.jmxremote.ssl=false"</span>,</span><br><span class="line">  <span class="string">"-Dcom.sun.management.jmxremote.authenticate=false"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Note: if you are adding JVM settings here, you need to prefix with <code>-J</code>, eg,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"-J--Xmx3000M"</span></span><br></pre></td></tr></table></figure><p>With this configured and if you run your program locally, jconsole and VisualVM can auto detect running processes already.</p><h1 id="Setup-for-remote-access"><a href="#Setup-for-remote-access" class="headerlink" title="Setup for remote access"></a>Setup for remote access</h1><p>If you want to to debug service running on a remote server. e.g staging environment. you will need to expose the jmx port at the same time.</p><p>Below instructions are based on a helm chart context, since all our services are deployed to EKS through helm chart.</p><h2 id="Add-ports-to-Service"><a href="#Add-ports-to-Service" class="headerlink" title="Add ports to Service"></a>Add ports to <code>Service</code></h2><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">apiVersion</span>: v1</span><br><span class="line"><span class="attribute">kind</span>: Service</span><br><span class="line"><span class="attribute">metadata</span>:</span><br><span class="line">  ...</span><br><span class="line"><span class="attribute">spec</span>:</span><br><span class="line">  <span class="attribute">type</span>: &#123;&#123; .Values.service.type &#125;&#125;</span><br><span class="line">  <span class="attribute">ports</span>:</span><br><span class="line">    - <span class="attribute">port</span>: &#123;&#123; .Values.service.jmx.port &#125;&#125;</span><br><span class="line">      <span class="attribute">targetPort</span>: jmx</span><br><span class="line">      <span class="attribute">protocol</span>: TCP</span><br><span class="line">      <span class="attribute">name</span>: jmx</span><br><span class="line">  <span class="attribute">selector</span>:</span><br><span class="line">    <span class="attribute">app</span>: &#123;&#123; include <span class="string">"application.name"</span> . &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="Add-ports-to-deployment-yaml"><a href="#Add-ports-to-deployment-yaml" class="headerlink" title="Add ports to deployment.yaml"></a>Add ports to deployment.yaml</h2><p>Under, Deployment.spec.template.spec.containers</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ports:</span><br><span class="line">  - name: jmx</span><br><span class="line">    containerPort: &#123;&#123; <span class="selector-class">.Values</span><span class="selector-class">.container</span><span class="selector-class">.jmx</span><span class="selector-class">.port</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>Finally, you can provide the port number for different envs in Values.yaml</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">container:</span></span><br><span class="line"><span class="symbol">  jmx:</span></span><br><span class="line"><span class="symbol">    port:</span> <span class="number">9186</span></span><br><span class="line"></span><br><span class="line"><span class="symbol">service:</span></span><br><span class="line"><span class="symbol">  type:</span> ClusterIP</span><br><span class="line"><span class="symbol">  jmx:</span></span><br><span class="line"><span class="symbol">    port:</span> <span class="number">9186</span></span><br></pre></td></tr></table></figure><p>Start port forwarding after service deployed to EKS</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kubectl</span> <span class="selector-tag">port-forward</span> <span class="selector-tag">-n</span> <span class="selector-attr">[namespace_name]</span> <span class="selector-tag">service</span>/<span class="selector-attr">[app_name]</span> <span class="selector-tag">9186</span><span class="selector-pseudo">:9186</span></span><br></pre></td></tr></table></figure><h1 id="Visualize-with-jconsole"><a href="#Visualize-with-jconsole" class="headerlink" title="Visualize with jconsole"></a>Visualize with jconsole</h1><p>Start jconsole in command line easily with</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">jconsole</span></span><br></pre></td></tr></table></figure><h2 id="Add-Connection"><a href="#Add-Connection" class="headerlink" title="Add Connection"></a>Add Connection</h2><p><img src="./1jconsole.png" alt="set jconsole connection"></p><h2 id="View-Metrics"><a href="#View-Metrics" class="headerlink" title="View Metrics"></a>View Metrics</h2><p><img src="./1jconsole_2.png" alt="jconsole stats"></p><h1 id="Visualize-with-VisualVM"><a href="#Visualize-with-VisualVM" class="headerlink" title="Visualize with VisualVM"></a>Visualize with VisualVM</h1><h2 id="Add-JMX-Connection"><a href="#Add-JMX-Connection" class="headerlink" title="Add JMX Connection"></a>Add JMX Connection</h2><p><img src="./1open_jmx_connection.png" alt="open jconsole connection"></p><h2 id="Input-JMX-Connection"><a href="#Input-JMX-Connection" class="headerlink" title="Input JMX Connection"></a>Input JMX Connection</h2><p><img src="./1set_jmx_connection.png" alt="set jmx connection"></p><h2 id="Start-exploring"><a href="#Start-exploring" class="headerlink" title="Start exploring"></a>Start exploring</h2><p>Click ok button, wait for a second, then you should see:</p><p><img src="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/1view_jmx.png" alt="JMX stat"></p><p>Start to exploring more data</p><p><img src="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/1view_jmx_2.png" alt="More JMX stats"></p><h1 id="Tuning-JVM-parameters"><a href="#Tuning-JVM-parameters" class="headerlink" title="Tuning JVM parameters"></a>Tuning JVM parameters</h1><p>If you want to test different JVM settings, this can be done in helm chart Values.yaml:</p><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- <span class="built_in">key</span>: <span class="string">"JAVA_OPTS"</span>    </span><br><span class="line">  <span class="built_in">value</span>: <span class="string">"-XX:+UnlockExperimentalVMOptions -Xms3000M -Xmx3000M -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ParallelGCThreads=4 -XX:+CMSClassUnloadingEnabled -XX:MaxRAMFraction=2 -XX:NewSize=3000M -XX:MaxNewSize=3000M -XX:+CMSParallelRemarkEnabled -XX:MaxMetaspaceSize=1000M -XX:+UseCGroupMemoryLimitForHeap"</span></span><br></pre></td></tr></table></figure><p>This can be added in <code>build.sbt</code> as well, but it loses the flexibility to tune them for different environments e.g staging, performance, production.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Java Management Extensions (JMX) is a Java technology that supplies tools for managing and monitoring applications, system objects, devic
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Design a web hook service</title>
    <link href="http://yqian1991.github.io/System-Design/Design-a-web-hook-service/"/>
    <id>http://yqian1991.github.io/System-Design/Design-a-web-hook-service/</id>
    <published>2020-10-05T20:18:20.000Z</published>
    <updated>2020-10-05T20:28:47.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Compared to traditional API services which is pull based, a push based webhook service stands out in many ways:</p><ul><li>Performance: In real time scenarios (for example, you want to receive real time update of information, metrics or alerts), polling(in an interval) is not efficient.<ul><li>Polling too often may waste resources, polling too late may not process the data in a timely manner.</li></ul></li></ul><ul><li>User experience: Instead of working around the API to pull data, it’s much easier to just handle the data pushed to you based on defined schema.<br>Only process when data is available.</li></ul><p>Many product provide web hooks to strengthen their features like SendGrid webhook, Sparkpost web hook.</p><h1 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h1><p>To have a web hook service working, 3 problems need to be engineered.</p><h2 id="Setup-subscription"><a href="#Setup-subscription" class="headerlink" title="Setup subscription"></a>Setup subscription</h2><p>Allow clients to subscribe to the web hook, an authentication method preferred. Use OAuth 2 or built your own</p><h2 id="Event-Categorization"><a href="#Event-Categorization" class="headerlink" title="Event Categorization"></a>Event Categorization</h2><p>Make it capable for clients to subscribe to different kinds of event categories.</p><p>At the minimum, clients need to provide the following informations to subscribe to the web hook:</p><ul><li>callback_url: String,    For where the web hook send events to</li><li>event categories: List,    For what events web hook should send, eg: unsubscribe events.</li><li>requester: String, For security check.</li><li>status:    String,    Status of the subscription, subscribe, suspend, unsubscribe</li></ul><h2 id="Sending-events"><a href="#Sending-events" class="headerlink" title="Sending events"></a>Sending events</h2><p>Once a subscription is validated, you should send events as soon as them arrive.</p><p>There are different triggering mechanisms for a web hook:</p><ul><li>realtime: Always on sending action.</li><li>scheduled: Useful if only hourly/daily/monthly data required</li></ul><h2 id="Event-schema"><a href="#Event-schema" class="headerlink" title="Event schema"></a>Event schema</h2><p>For events sending, we also needs to design a universal event schema.</p><h2 id="Events-Delivery-Guarantee"><a href="#Events-Delivery-Guarantee" class="headerlink" title="Events Delivery Guarantee"></a>Events Delivery Guarantee</h2><p>Client needs to send a response to acknowledge they received the events.</p><p>If web hook failed to send events or client replied back with a non success response, we should resent events.</p><p>Back pressure in place in case client’s response rate is lag behind the web hook publish rate</p><h2 id="Manage-Web-hook-status"><a href="#Manage-Web-hook-status" class="headerlink" title="Manage Web hook status"></a>Manage Web hook status</h2><p>Since web hook is push based, you should always keep your clients up-to-date the current health status of the web hook.</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>This implementation is based on the requirements of serving push campaign metrics, which will show up in push management dashboard</p><p>TODO</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://mandrill.zendesk.com/hc/en-us/articles/205583257-How-to-Authenticate-Webhook-Requests" target="_blank" rel="noopener">https://mandrill.zendesk.com/hc/en-us/articles/205583257-How-to-Authenticate-Webhook-Requests</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h1&gt;&lt;p&gt;Compared to traditional API servic
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>High Performance Spark Reading Notes</title>
    <link href="http://yqian1991.github.io/System-Design/High-Performance-Spark-I/"/>
    <id>http://yqian1991.github.io/System-Design/High-Performance-Spark-I/</id>
    <published>2020-07-03T16:02:54.000Z</published>
    <updated>2020-07-03T16:33:49.821Z</updated>
    
    <content type="html"><![CDATA[<p>As like many other programming languages or frameworks, writing a toy program is easy but it’s always difficult how to make a program high performant, especially with Spark, which is popular running in distributed environment.</p><p>Spark provides a rich lib of functions to meet various requirements, but you may only need some of them in your daily work. Here are some notes I took during reading the book “High Performance Spark”, with this notes, hope developers can have a better design of spark programs at upfront instead of learn from failures themselves. While a key takeaway is that, instead of only knowing how to use what, it’s more important why it is better.</p><p>Pyspark seems very popular in the market because of the popularity of big data, but since Spark relies heavily on inline function definitions and lambda expressions, which are much more naturally supported in Scala, also lazy evaluation within memory computations makes it particularly unique, this also makes programming in Scala has its own advantages over Python.</p><h1 id="General-RDDs"><a href="#General-RDDs" class="headerlink" title="General RDDs"></a>General RDDs</h1><p>Evert RDD has the following properties:</p><ul><li>A list of partition objects,</li><li>A function for computing an iterator of each partition,</li><li>A list of dependencies on other RDDs,</li><li>A partitioner,</li><li>A list of preferred locations</li></ul><p>Transformations and actions are two categories of operations that working on your data,</p><ul><li>A transformation is a function that return another RDD. e.g: <code>flatMap</code>, <code>map</code>, <code>reduceByKey</code>, <code>groupByKey</code> etc</li><li>An action is an operation that returns something other than an RDD,<br>Actions trigger the scheduler, which builds a directed acyclic graph (called the DAG), based on the dependencies between RDD transformations (I also mentioned this in a previous post).</li></ul><p>Note that not all transformations are 100% lazy. <code>sortByKey</code> needs to evaluate the RDD to determine the range of data, so it involves both a transformation and an action.</p><p>When persisting RDDs, the default implementation of RDDs evicts the least recently used partition (called LRU caching) if the space it takes is required to compute or to cache a new partition. However, you can change this behaviour and control Spark’s memory prioritization with the <code>persistencePriority()</code> function in the RDD class</p><h1 id="Knowing-your-Spark-Program"><a href="#Knowing-your-Spark-Program" class="headerlink" title="Knowing your Spark Program"></a>Knowing your Spark Program</h1><p>The Spark program itself runs in the driver node and sends some instructions to the executors</p><p>When the SparkContext is started, a driver and a series of executors are started on the worker nodes of the cluster. The SparkContext determines how many resources are allotted to each executor</p><p>With each action in a spark application, the Spark scheduler builds an execution graph and launches a Spark job, A job is then divided into stages and tasks. Stage boundary is determined by wide transformations which requires shuffle (narrow and wide transformations).</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J<span class="function"><span class="title">ob</span> -&gt;</span> <span class="function"><span class="title">stages</span> -&gt;</span> tasks</span><br></pre></td></tr></table></figure><p>The execution of job is taken care of by different kinds of schedulers:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dag<span class="built_in"> Scheduler </span>-&gt; Task Scheduler</span><br></pre></td></tr></table></figure><h1 id="DataSet-and-DataFrame"><a href="#DataSet-and-DataFrame" class="headerlink" title="DataSet and DataFrame"></a>DataSet and DataFrame</h1><p>DataFrames and Datasets can be read using the Spark SQL equivalent to a SparkContext, the SparkSession.</p><p>DataFrames are Datasets of a special Row object, which doesn’t provide any compile-time type checking, DataFrames allow Spark’s optimizer to better understand our code and our data, which allows for a new class of optimizations.</p><p>DataSet API is quite different when compared with general RDD APIs, it has its own optimizer and execution plans.<br>jobs are transformed to logical plan and then physical plan which has better performance.</p><h1 id="Spark-ML-and-Spark-MLlib"><a href="#Spark-ML-and-Spark-MLlib" class="headerlink" title="Spark ML and Spark MLlib"></a>Spark ML and Spark MLlib</h1><p>Spark ML provides a higher-level API than MLlib with the goal of allowing users to more easily create practical machine learning pipelines. Spark MLlib is primarily built on top of RDDs and uses functions from Spark Core, while ML is built on top of Spark SQL DataFrames</p><h1 id="Some-optimization-tips"><a href="#Some-optimization-tips" class="headerlink" title="Some optimization tips"></a>Some optimization tips</h1><ul><li><p>Sharing the same partitioner with RDDs are materialized by the same action, so thet will end up being co-located (which can even reduce network traffic).</p></li><li><p>Speed up joins by broad cast join, real world application is way more complicate than this, it depends on the property of the two datasets you want to join. If you are joining a big dataset with a small one, then you can broadcast the small one.</p></li><li><p>Since we can’t control the partitioner for DataFrames or Datasets, so we can’t manually avoid shuffles as you did with core Spark joins</p></li><li><p>Minimizing object creation in your RDD operations:</p><ul><li>Reusing existing objects: but note this may cause object mutable</li><li>Using smaller data structures, using primitive types instead of case classes, objects</li><li>Reduce setup overhead: db connection etc</li><li>Reusing RDD: persist, cache, checkpoint, shuffle files. but notice it is space intensive to store data in memory and will take time to serialize and deserialize</li></ul></li><li><p>Some common issues to avoid when doing key-value transformations:</p><ul><li>Memory error on Spark Driver: Avoid calling <code>collection</code>, <code>collectionAsMap</code> on large dataset.</li><li>OOM on executors: Spark parameter tuning, reduce data loading by filtering or aggregation first</li><li>Shuffle failures: Try to preserve partitioning across narrow transformations to avoid reshuffling data</li><li>Straggler tasks: Make partition size even by customizing partitioner</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;As like many other programming languages or frameworks, writing a toy program is easy but it’s always difficult how to make a program hig
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Spark" scheme="http://yqian1991.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Structured Streaming Internal</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/</id>
    <published>2020-05-31T02:58:25.000Z</published>
    <updated>2020-06-02T20:09:49.342Z</updated>
    
    <content type="html"><![CDATA[<p>Spark structured streaming is implemented in spark sql module.</p><h1 id="Spark-Session"><a href="#Spark-Session" class="headerlink" title="Spark Session:"></a>Spark Session:</h1><p>sparkSession is the entry to Spark sql and structured streaming.</p><ul><li>Spark Context: spark context is the entry point of spark, so it’s naturally the bedrock of spark session as well.</li><li>readStream: Read streaming data in as a DataFrame, this method returns an DataStreamReader object</li><li>StreamingQueryManager: Managing the execution of all streaming quires</li><li>createDataFrame: Generate Data frames from various sources,(DataFrame=Dataset[Row])</li><li>createDataset:</li><li>sql: execute sql quires and return data as DataFrame</li></ul><h1 id="DataStreamReader-and-DataStreamWriter"><a href="#DataStreamReader-and-DataStreamWriter" class="headerlink" title="DataStreamReader and DataStreamWriter"></a>DataStreamReader and DataStreamWriter</h1><ul><li>DataStreamReader: Load streaming data from external sources</li><li>DataStreamWriter: Output streaming data to external sources<br>DataStreamWriter has a start() method which calls <code>df.sparkSession.sessionState.StreamingQueryManager.startQuery()</code> to start streaming query.<br>As you can see, the query is started through StreamingQueryManager.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">streamSource = spark \</span><br><span class="line">  .readStream \</span><br><span class="line">  .format(<span class="string">"kafka"</span>) \</span><br><span class="line">  .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="type">KAFKA_BROKER</span>) \</span><br><span class="line">  .option(<span class="string">"subscribe"</span>, <span class="type">KAFKA_TOPIC</span>) \</span><br><span class="line">  .option(<span class="string">"startingOffsets"</span>, <span class="string">"earliest"</span>) \</span><br><span class="line">  .option(<span class="string">"group.id"</span>,  <span class="type">CONSUMER_ID</span>) \</span><br><span class="line">  .load()   <span class="comment">// DataStreamReader</span></span><br><span class="line"></span><br><span class="line">output = streamSource \</span><br><span class="line">  .writeStream \</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>) \</span><br><span class="line">  .trigger(processingTime = trigger_interval) \</span><br><span class="line">  .foreach(sink) \</span><br><span class="line">  .start() <span class="comment">// DataStreamWriter</span></span><br></pre></td></tr></table></figure><h1 id="Dataset-API"><a href="#Dataset-API" class="headerlink" title="Dataset API"></a>Dataset API</h1><p>Dataset is a strong typed data structure used to do transformations and actions in structured streaming.</p><ul><li>logicalPlan: After transformations and actions defined on a Dataset, it will be analyzed to logical plan, then optimized to physical plan for final execution.</li></ul><p>Some features can be applied are:</p><ul><li>withWatermark: A watermark defines a point in time before which we assume no more late data is going to arrive. This is useful for late data in streaming situation.</li><li>checkpoint: Apply dataset checkpointing, either eagerly or non eagerly.</li><li><p>cache: Cache dataset to memory.</p><p>A lot transformations can be applied:</p><ul><li>groupBy:</li><li>groupByKey: This will return a KeyValueGroupedDataset</li><li>agg:</li><li>repartition/coalesce: Returns a new dataset by specified partitioning.</li></ul></li></ul><h1 id="StreamingQueryManager"><a href="#StreamingQueryManager" class="headerlink" title="StreamingQueryManager"></a>StreamingQueryManager</h1><p>Manages all the StreamingQuerys active in a <code>SparkSession</code>.</p><ul><li><code>startQuery</code> method: create query with DataFrame and call StreamingExecution start()</li></ul><h1 id="StreamingExecution"><a href="#StreamingExecution" class="headerlink" title="StreamingExecution"></a>StreamingExecution</h1><p>StreamingExecution is a implementation of trait StreamingQuery, it can be:</p><ul><li>ContinousExecution: This mode doesn’t support <code>complete</code> outputMode and aggregation either.</li><li>MicrobatchExecution:</li></ul><p>Some methods in StreamingExecution:</p><ul><li>start(): start a thread of queryEexcutionThread</li><li>queryExecutionThread: This thread will run method runStream()</li><li>runStream(): The method to materialize the streaming query</li><li>runActivatedStream: All implementations need to implement this method. working on the logical plan</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spark structured streaming is implemented in spark sql module.&lt;/p&gt;
&lt;h1 id=&quot;Spark-Session&quot;&gt;&lt;a href=&quot;#Spark-Session&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Understanding</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/</id>
    <published>2020-05-19T20:12:02.000Z</published>
    <updated>2020-05-19T20:35:10.300Z</updated>
    
    <content type="html"><![CDATA[<p>When a streaming context starts, it will start a Job Scheduler:</p><ul><li>Start ReceiverTracker: receive data from source and use BlockManager to save it to memoryStore(BlockManager)</li><li>Start JobGenerator: Periodically check interval, create Jobs based on graph given timestamp and OutputStream</li><li>Other resources: like job listener etc</li></ul><p>Each scheduler is run in eventLoop</p><p>For a streaming application, it usually contains:</p><ul><li>An InputStream: Some examples are SocketInputStream, KafkaInputStream, FileInputStream etc</li><li>Transformations: functions that generate another stream, example functions can be filter, map etc</li><li>OutputStream: Output actions of the streaming application, this will trigger stream to be materialized</li></ul><p>Streaming Context includes a property <code>graph</code> which Input Stream and output stream are registered on.</p><p>How is data read from memoryStore?</p><ul><li>Every DStream has methods to generateJob, getOrCompute, then compute (load from memory)</li></ul><p>A simple class/entity relationship:</p><p>SparkStreamingContext:</p><ul><li>sparkContext: sparkContext can be run by calling runJob method which invoke DagScheduler</li><li>graph: A list of DStream which includes output stream and input stream.</li><li>JobScheduler: See below</li></ul><p>JobScheduler: eventLoop</p><ul><li>JobGenerator: generate jobs based on <code>ssc.graph</code> at <code>batchDuration</code> interval, then submit jobs to JobExecutor</li><li>JobExecutor: execute submitted jobs from JobGenerator, job executed by calling sparkContext runJob func.</li><li>receiverTracker:</li></ul><p>JobGenerator(jobScheduler): eventLoop</p><ul><li>RecurringTimer: interval is batchDuration, trigger <code>GenerateJob</code> action</li><li>generateJobs: generate jobs for each outputStream in <code>ssc.graph</code></li><li>Submit Jobs to jobScheduler</li></ul><p>DStream(ssc):</p><ul><li>foreachRDD: register as outputStream</li><li>register: add stream to outputStream that used to generateJobs()</li><li>generateJob,: call compute method to get RDDs and create Job object</li><li>getOrCompute/compute: get RDDs, either from parent or from memory if it’s InputDStream</li><li>generatedRDDs:</li></ul><p>SparkContext</p><ul><li>DagScheduler</li></ul><p>DagScheduler: eventLoop</p><ul><li>Create stages of a RDD based on RDD dependencies</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source: <span class="type">DStream</span></span><br><span class="line"><span class="keyword">val</span> transformed: <span class="type">DStream</span> = source.transform() <span class="comment">// transformed will store dependency source</span></span><br><span class="line"><span class="keyword">val</span> output = transformed.foreachRDD(println) <span class="comment">// outputStream</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;When a streaming context starts, it will start a Job Scheduler:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start ReceiverTracker: receive data from source and use Bloc
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>The way to async I/O</title>
    <link href="http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/"/>
    <id>http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/</id>
    <published>2020-04-15T22:02:39.000Z</published>
    <updated>2020-04-16T03:02:40.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Concurrency-vs-Parallelism"><a href="#Concurrency-vs-Parallelism" class="headerlink" title="Concurrency vs Parallelism"></a>Concurrency vs Parallelism</h1><ul><li><p>Concurrency: Do multiple tasks at the same time.</p></li><li><p>Parallelism: Do one task at a time, but the task can be splitted to multiple sub tasks which can be executed in parallel.</p></li></ul><h1 id="CPU-intensive-vs-I-O-intensive"><a href="#CPU-intensive-vs-I-O-intensive" class="headerlink" title="CPU intensive vs I/O intensive"></a>CPU intensive vs I/O intensive</h1><p>If your program is not interacting with disks, media, devices, network and peripheries, then it is CPU intensive, otherwise it is I/O intensive.</p><p>The mode really effects the performance of your program. For example, if you want to know how many RPS my program can handle:</p><ul><li><p>For CPU intensive: (Number of Cores) / time_to_complete_a_request_in_seconds</p></li><li><p>For I/O intensive: (RAM / worker memory) / time_to_complete_a_request_in_seconds</p><p>In I/O intensive scenarios, CPU is doing nothing, so the performance is limited by how many workers are running, thus memory related.</p></li></ul><h1 id="Blocking-vs-Non-blocking"><a href="#Blocking-vs-Non-blocking" class="headerlink" title="Blocking vs Non-blocking"></a>Blocking vs Non-blocking</h1><p>When a program spending most of the time dealing with I/O and not doing anything else, then it will be blocked by the I/O operations, thus CPU stay there and do nothing.</p><p>In order to reuse CPU during waiting, we need to make it non blocking.</p><p>Essentially, it means instead of waiting, it periodically checking the status of I/O operation, only back to handle it if it finishes, otherwise, allow the system to do other tasks.</p><h1 id="Synchronous-vs-asynchronous"><a href="#Synchronous-vs-asynchronous" class="headerlink" title="Synchronous vs asynchronous"></a>Synchronous vs asynchronous</h1><p>Blocking and synchronous are almost the same, a thread focuses on doing one task, no distractions.</p><p>But the difference between non-blocking and asynchronous sometimes is hard to understand. They can be the same in many ways, especially when you don’t deep dive into it.</p><p>If we understand it at thread level, asynchronous means task can be delegated to a different thread, responses can be communicated by other ways like event driven or an callback mechanism, whereas, non-blocking, the thread needs to periodically checking result until task finished.</p><h1 id="Different-models"><a href="#Different-models" class="headerlink" title="Different models"></a>Different models</h1><p>You can design a program as:</p><ul><li><p>synchronous, non-blocking I/O: Since we still need to periodically checking task status in the same thread, and concurrency can only be achieved by spawning more threads, thus more overhead doing context switch.</p></li><li><p>asynchronous, non-blocking I/O: This is preferred by modern web servers. E.g you can achieve this by one thread using event loop. Some real life examples are Python Twisted, Java Netty.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Concurrency-vs-Parallelism&quot;&gt;&lt;a href=&quot;#Concurrency-vs-Parallelism&quot; class=&quot;headerlink&quot; title=&quot;Concurrency vs Parallelism&quot;&gt;&lt;/a&gt;Concurre
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Async" scheme="http://yqian1991.github.io/tags/Async/"/>
    
  </entry>
  
  <entry>
    <title>Canary Deployment with k8s ingress-controller</title>
    <link href="http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/"/>
    <id>http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/</id>
    <published>2020-03-03T01:48:52.000Z</published>
    <updated>2020-03-03T02:02:00.156Z</updated>
    
    <content type="html"><![CDATA[<p>Canary deployment is useful in many scenarios, like A/B testing.<br>k8s ingress-controller makes canary deployment easy, I will introduce the steps of it using the example of a microservice we have.</p><p>This document will not cover how to set up ELB. It will focus only on how to create a canary deployment along side an existing deployment.</p><h2 id="Situation"><a href="#Situation" class="headerlink" title="Situation"></a>Situation</h2><p>We have a service A running in k8s cluster, there is an external host name setup to expose the API to public, Requests sent through this API will be route to this service,</p><p>Here is the public host name setup:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl <span class="builtin-name">get</span> ingress -n ingress</span><br><span class="line">NAME                   HOSTS              <span class="built_in"> ADDRESS </span>  PORTS   AGE</span><br><span class="line">service-a          public.domain.com             80      297d</span><br></pre></td></tr></table></figure></p><p>and it also has k8s service set up:</p><p>So far, this is running happily on production,<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n emaildelivery</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">5</span>          <span class="number">159</span>d</span><br></pre></td></tr></table></figure></p><h2 id="Canary-Deployment"><a href="#Canary-Deployment" class="headerlink" title="Canary Deployment"></a>Canary Deployment</h2><h3 id="Deploy-new-pods-with-correct-annotation"><a href="#Deploy-new-pods-with-correct-annotation" class="headerlink" title="Deploy new pods with correct annotation"></a>Deploy new pods with correct annotation</h3><p>You will need to deploy new pods for sure, without overwriting current pods, you can do this by deploying new pods with a new application name or using a different namespace.</p><p>I want to reuse the same namespace, so I changed the application name to <code>service-a-canary</code>.</p><p>Then comes to the fun part, Keep all the necessary labels as <code>service-a</code> in the resources. Resources you will need to change include <code>service.yaml</code>, <code>deployment.yaml</code>, <code>ingress.yaml</code>.</p><p>Key changes are as follows:</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  namespace:</span> service-a</span><br><span class="line"><span class="symbol">  name:</span> service-a-canary</span><br><span class="line"><span class="symbol">  labels:</span></span><br><span class="line"><span class="symbol">    name:</span> service-a</span><br><span class="line">...</span><br><span class="line"><span class="symbol">selector:</span></span><br><span class="line"><span class="symbol">    matchLabels:</span></span><br><span class="line"><span class="symbol">      app:</span> service-a</span><br></pre></td></tr></table></figure><p>If you release the new pods at this time point, new pods will running fine, but serving no traffic:</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n service-a</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">159</span>d</span><br><span class="line">service-a-canary<span class="number">-6</span>f7784fc7-wg7kz   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d20h</span><br></pre></td></tr></table></figure><h3 id="Enable-canary-with-ingress-controller"><a href="#Enable-canary-with-ingress-controller" class="headerlink" title="Enable canary with ingress-controller"></a>Enable canary with ingress-controller</h3><p>To make canary deployment serving real traffic, simply adding few lines in ingress.yaml</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary: <span class="string">"true"</span></span><br><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary-weight: <span class="string">"10"</span></span><br></pre></td></tr></table></figure><p>nginx.ingress.kubernetes.io/canary-weight is the percentage of traffic that will be routed to canary deployment.</p><p>NOW, you can check your logs to see if traffic routed to canary deployment.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Canary deployment is useful in many scenarios, like A/B testing.&lt;br&gt;k8s ingress-controller makes canary deployment easy, I will introduce
      
    
    </summary>
    
      <category term="SRE" scheme="http://yqian1991.github.io/categories/SRE/"/>
    
    
      <category term="k8s" scheme="http://yqian1991.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Distributed OLAP Datastore</title>
    <link href="http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/"/>
    <id>http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/</id>
    <published>2020-02-20T03:32:16.000Z</published>
    <updated>2020-03-28T14:30:29.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Apache-Pinot"><a href="#Apache-Pinot" class="headerlink" title="Apache Pinot"></a>Apache Pinot</h1><p>Pinot is a realtime distributed OLAP datastore for scalable real time analytics with low latency.</p><p>It’s using Apache Helix for cluster management, data stored as segment in Pinot.</p><h2 id="Components"><a href="#Components" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Manage brokers and servers, responsible for assigning segments to servers.</p><h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>Accepting queries from clients and return query results to clients.</p><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>Hosts one or more segments, and respond to queries from broker.<br>Data is stored as segment in servers. Segment is a columnar storage.</p><h2 id="Data-Ingestion"><a href="#Data-Ingestion" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Data can be ingested in real time or offline ingestion mode</p><h3 id="Real-time-ingestion"><a href="#Real-time-ingestion" class="headerlink" title="Real time ingestion"></a>Real time ingestion</h3><p>Data from Kafka or other streaming source can be ingested to Pinot servers directly, and can serve query right away.</p><h3 id="Offline-ingestion"><a href="#Offline-ingestion" class="headerlink" title="Offline ingestion"></a>Offline ingestion</h3><p>Data in storage can be ingested through Pinot controller, and pinot controller will assign segments to Pinot servers.</p><ul><li>Add Schema</li><li>Add Table</li><li>Create Segment</li><li>Upload Segment</li></ul><h2 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h2><p>Pinot query language is very similar to standard query language except that <code>JOIN</code> and <code>LIMIT</code> are not supported.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT &lt;outputColumn&gt; (, outputColumn, outputColumn,<span class="built_in">..</span>.)</span><br><span class="line">  <span class="keyword">FROM</span> &lt;tableName&gt;</span><br><span class="line">  (WHERE <span class="built_in">..</span>. |<span class="built_in"> GROUP </span>BY <span class="built_in">..</span>. | ORDER BY <span class="built_in">..</span>. | TOP <span class="built_in">..</span>. | LIMIT <span class="built_in">..</span>.)</span><br></pre></td></tr></table></figure><h3 id="Indexing-technology"><a href="#Indexing-technology" class="headerlink" title="Indexing technology"></a>Indexing technology</h3><h1 id="Apache-Druid"><a href="#Apache-Druid" class="headerlink" title="Apache Druid"></a>Apache Druid</h1><p>Druid is very similar to Pinot in many ways: both for real time queries, both support real time and offline ingestions. Instead of Helix, Druid uses Apache Zookeeper for coordination.</p><h2 id="Components-1"><a href="#Components-1" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Master-Server-Coordinator-and-overlord-processes"><a href="#Master-Server-Coordinator-and-overlord-processes" class="headerlink" title="Master Server(Coordinator and overlord processes)"></a>Master Server(Coordinator and overlord processes)</h3><p>Manages data availability and ingestion, similar to Pinot controller.</p><h3 id="Query-Server-Broker-and-Router"><a href="#Query-Server-Broker-and-Router" class="headerlink" title="Query Server(Broker and Router)"></a>Query Server(Broker and Router)</h3><p>Same as Pinot, accepting queries from external clients, routing queries to brokers, coordinators and overlords.</p><h3 id="Data-Server-Historical-and-Middle-Manager-processes"><a href="#Data-Server-Historical-and-Middle-Manager-processes" class="headerlink" title="Data Server(Historical and Middle Manager processes)"></a>Data Server(Historical and Middle Manager processes)</h3><p>This is similar to server in Pinot, handles ingestion workloads and stores all queryable data.<br>Druid also provides a Deep Storage component as backup of data.</p><p>Data is stored as segment in Druid as well, but Druid segment always comes with a timestamp.</p><p>Druid supports tiering which allows old data can be moved to clusters with more disk storage but less memory and CPU, This can improve query efficiency.</p><h2 id="Data-Ingestion-1"><a href="#Data-Ingestion-1" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Druid also support batch and real time ingestion.</p><h2 id="Query-1"><a href="#Query-1" class="headerlink" title="Query"></a>Query</h2><p>Druid’s native query language is JSON over HTTP, beside this, Druid also provides Druid SQL.</p><h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><h1 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h1><p>Presto was designed for OLAP to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. But unlike Pinot and Druid, Presto is used to connect and query from external data sources, varies from HDFS to Cassandra and traditional database like MySQL.</p><h2 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h2><p>Parsing statements, planning queries, and managing Presto worker nodes.</p><h2 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h2><p>Executing tasks and processing data, Worker nodes fetch data from connectors and exchange intermediate data with each other.</p><h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Since Presto has no its own data storage, it relies on different kind of connectors to get data.<br>Data is then modeled as Catalog, schema and table in Presto.</p><h2 id="Query-Execution"><a href="#Query-Execution" class="headerlink" title="Query Execution"></a>Query Execution</h2><p>Statement -&gt; Queries -&gt; Stages -&gt; Tasks</p><h1 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h1><h1 id="Extra-Reads"><a href="#Extra-Reads" class="headerlink" title="Extra Reads"></a>Extra Reads</h1><p><a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7" target="_blank" rel="noopener">https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7</a></p><p><a href="https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e" target="_blank" rel="noopener">https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Apache-Pinot&quot;&gt;&lt;a href=&quot;#Apache-Pinot&quot; class=&quot;headerlink&quot; title=&quot;Apache Pinot&quot;&gt;&lt;/a&gt;Apache Pinot&lt;/h1&gt;&lt;p&gt;Pinot is a realtime distribute
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>100 Questions About Cassandra</title>
    <link href="http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/"/>
    <id>http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/</id>
    <published>2020-01-11T17:41:47.000Z</published>
    <updated>2020-01-16T04:06:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fast-facts-about-Cassandra"><a href="#Fast-facts-about-Cassandra" class="headerlink" title="Fast facts about Cassandra"></a>Fast facts about Cassandra</h2><ul><li>A Columnar based fault tolerant NoSQL database</li><li>An AP system (Sacrifice Consistency for Available and Partition)</li><li>Easy to scale horizontally (No master)</li><li>No join or subquery for aggregation</li></ul><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h2><h3 id="What-is-Cassandra’s-Replication-Strategy"><a href="#What-is-Cassandra’s-Replication-Strategy" class="headerlink" title="What is Cassandra’s Replication Strategy?"></a>What is Cassandra’s Replication Strategy?</h3><p>Replication strategies define the technique how the replicas are placed in a cluster.<br>There are mainly two types of Replication Strategy:</p><ul><li>Simple strategy: For single data center</li><li>Network Topology Strategy: For multi-datacenter</li></ul><h3 id="What-is-Cassandra-Consistency-Level"><a href="#What-is-Cassandra-Consistency-Level" class="headerlink" title="What is Cassandra Consistency Level?"></a>What is Cassandra Consistency Level?</h3><p>The minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful</p><ul><li>Write Consistency: ALL, ANY, ONE, EACH_QUORUM, LOCAL_ONE, LOCAL_QUORUM</li><li>Read Consistency: ALL, ONE, TWO, THREE, QUORUM, LOCAL_ONE, LOCAL_QUORUM</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quorum = (sum_of_replication_factors / 2) + 1</span><br></pre></td></tr></table></figure><p><a href="https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html" target="_blank" rel="noopener">https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html</a></p><h3 id="What-is-Cassandra’s-compaction-strategy"><a href="#What-is-Cassandra’s-compaction-strategy" class="headerlink" title="What is Cassandra’s compaction strategy?"></a>What is Cassandra’s compaction strategy?</h3><p>To improve read performance as well as to utilize disk space, Cassandra periodically does compaction to create &amp; use new consolidated SSTable files instead of multiple old SSTables.</p><ul><li>SizeTieredCompactionStrategy: for write-intensive workloads</li><li>LeveledCompactionStrategy: read-intensive workloads</li></ul><h3 id="Partition-key-and-clustering-key"><a href="#Partition-key-and-clustering-key" class="headerlink" title="Partition key and clustering key"></a>Partition key and clustering key</h3><p>Partition key is similar to primary key in relational databases, it decides which node to store the record.</p><p>Clustering key is responsible for sorting data within a partition</p><h4 id="Compound-key"><a href="#Compound-key" class="headerlink" title="Compound key"></a>Compound key</h4><p>Compound key are partition keys with multiple columns, but only the first column is considered as partition key and the rest are clustering keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY (p1, c1, c2, c3)</span><br></pre></td></tr></table></figure><h4 id="Composite-key"><a href="#Composite-key" class="headerlink" title="Composite key"></a>Composite key</h4><p>Composite keys are partition keys that consist of multiple columns.<br>But when you do query, you will need to include all partition keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY ((p1, p2), c1, c2)</span><br></pre></td></tr></table></figure><h3 id="What-are-some-of-Cassandra’s-limitations"><a href="#What-are-some-of-Cassandra’s-limitations" class="headerlink" title="What are some of Cassandra’s limitations?"></a>What are some of Cassandra’s limitations?</h3><ul><li>A single column value is recommended to &lt;= 1 Mb (max is 2Gb)</li><li>Number of rows within a partition is better to below 100,000 items and disk size under 100 Mb</li></ul><h3 id="How-does-Cassandra-use-bloom-filters"><a href="#How-does-Cassandra-use-bloom-filters" class="headerlink" title="How does Cassandra use bloom filters?"></a>How does Cassandra use bloom filters?</h3><p>Cassandra uses bloom filters to check if a partition key exists in any of the SSTables or not, without actually having to read their contents.</p><p>Each SSTable has a bloom filter, bloom filter will be updated when a memtable is flushed to disk.</p><h3 id="What-are-seed-node-in-Cassandra-cluster-setup"><a href="#What-are-seed-node-in-Cassandra-cluster-setup" class="headerlink" title="What are seed node in Cassandra cluster setup?"></a>What are seed node in Cassandra cluster setup?</h3><p>Seeds are used during startup to discover the cluster. Seeds are also referred by new nodes on bootstrap to learn other nodes in ring. When you add a new node to ring, you need to specify at least one live seed to contact. Once a node join the ring, it learns about the other nodes, so it doesn’t need seed on subsequent boot.</p><h3 id="How-to-add-a-new-node-to-a-single-datacenter-cluster"><a href="#How-to-add-a-new-node-to-a-single-datacenter-cluster" class="headerlink" title="How to add a new node to a single datacenter cluster?"></a>How to add a new node to a single datacenter cluster?</h3><ul><li>Calculate tokens for the new node, below is the script to generate tokens with Murmur3Partitioner.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">"print [str(((2**64 / number_of_tokens) * i) - 2**63) for i in range(number_of_tokens)]"</span></span><br></pre></td></tr></table></figure><ul><li><p>Install Cassandra on the node with proper <code>cassandra.yaml</code></p></li><li><p>Use <code>nodetool move</code> to assign new token for it.</p></li><li><p>Use <code>nodetool cleanup</code> to remove keys that no longer belong to the previously existing nodes.</p></li></ul><p><a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html" target="_blank" rel="noopener">https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fast-facts-about-Cassandra&quot;&gt;&lt;a href=&quot;#Fast-facts-about-Cassandra&quot; class=&quot;headerlink&quot; title=&quot;Fast facts about Cassandra&quot;&gt;&lt;/a&gt;Fast fac
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Cassandra" scheme="http://yqian1991.github.io/tags/Cassandra/"/>
    
      <category term="Database" scheme="http://yqian1991.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>我们的2019</title>
    <link href="http://yqian1991.github.io/Life/%E6%88%91%E4%BB%AC%E7%9A%842019/"/>
    <id>http://yqian1991.github.io/Life/我们的2019/</id>
    <published>2020-01-04T18:28:28.000Z</published>
    <updated>2020-01-11T19:09:54.815Z</updated>
    
    <content type="html"><![CDATA[<p>Feb: We travelled to LA and Daphne watched Jay Chou concert,<br>     I then participated company hackathon in San Mateo, got first prize (received a GoPro)<br>     I passed my G license</p><p>March: I moved to Toronto from Ottawa and started remote working for 4 months till June.<br>       We had our first car.</p><p>April:</p><p>May: We shoot pre-wedding photograph in Toronto.</p><p>June 9: We hold a small wedding ceremony with friends (happy to receive a lot of gifts)</p><p>July: I started a new job in downtown Toronto</p><p>Aug: Daphne’s parents in Canada</p><p>Sep: We spent a long weekend in Algonquin lodge (fishing, boating, hiking etc)</p><p>Oct: We hold Wedding ceremony in China and spent one day in Guangzhou, meet Daphne’s friend</p><p>Oct - Dec: Daphne did a lot Yoga session,  we also went badminton every Friday night.</p><p>Dec: Daphne had a Mont Tremblant - Montreal travel with friends, which I should be there too,<br>     but I went back China because of family emergency.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Feb: We travelled to LA and Daphne watched Jay Chou concert,&lt;br&gt;     I then participated company hackathon in San Mateo, got first prize 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="We" scheme="http://yqian1991.github.io/tags/We/"/>
    
  </entry>
  
  <entry>
    <title>Best practices to use Apache Spark</title>
    <link href="http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/"/>
    <id>http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/</id>
    <published>2019-12-01T16:23:03.000Z</published>
    <updated>2019-12-01T17:51:32.939Z</updated>
    
    <content type="html"><![CDATA[<p>Learning notes from DataBricks talks</p><h2 id="Optimizing-File-Loading-And-Partition-Discovery"><a href="#Optimizing-File-Loading-And-Partition-Discovery" class="headerlink" title="Optimizing File Loading And Partition Discovery"></a>Optimizing File Loading And Partition Discovery</h2><p>Data loading is the first step of spark application, when dataset becomes large, data loading time becomes an issue.</p><ul><li>Use Datasource tables</li></ul><p>With tables, partition metadata and schema is managed by Hive, based on which partition pruning can be done at logical planning stage. You can also use Spark SQL API directly when loading from a table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.table(<span class="string">"../path"</span>)</span><br><span class="line"></span><br><span class="line">spark.write.partitionBy(<span class="string">"date"</span>).saveAsTable(<span class="string">"table"</span>)</span><br></pre></td></tr></table></figure><ul><li>Specify <code>basePath</code> if loading from external files directly(e.g CSV or Json files)</li></ul><p>When loading from a file, partition discovery is done for each DataFrame creation<br>, also spark needs to infer schema from files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"inferSchema"</span>, true).load(file)</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(knownSchema).load(file)</span><br></pre></td></tr></table></figure><h2 id="Optimizing-File-Storage-and-Layout"><a href="#Optimizing-File-Storage-and-Layout" class="headerlink" title="Optimizing File Storage and Layout"></a>Optimizing File Storage and Layout</h2><ul><li><p>Prefer columnar over text for analytical queries</p></li><li><p>Compression with splittable storage format</p></li><li><p>Avoid large GZIP files</p></li><li><p>Partitioning and bucketing</p></li></ul><p>Parquet + Snappy is a good candidate</p><h2 id="Optimizing-Queries"><a href="#Optimizing-Queries" class="headerlink" title="Optimizing Queries"></a>Optimizing Queries</h2><ul><li>Tuning spark sql shuffle partitions<br><code>spark.sql.shuffle.partitions</code> is used in shuffle operations like groupBy, repartition, join and window. the default value is <code>200</code>.</li></ul><p>This is hard to tune because the parameter is applied on the whole job, which many operations maybe taken, tuning for certain operations may do no good to others.</p><ul><li><p>Adaptive Execution(&gt; Spark 2.0)<br><code>spark.sql.adaptive.enabled</code><br><code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>: default is 64Mb</p></li><li><p>Understanding Unions</p></li><li><p>Data Skipping Index</p></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.youtube.com/watch?v=iwQel6JHMpA" target="_blank" rel="noopener">Youtube Talks</a><br><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/" target="_blank" rel="noopener">Understanding Spark Source Code</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Learning notes from DataBricks talks&lt;/p&gt;
&lt;h2 id=&quot;Optimizing-File-Loading-And-Partition-Discovery&quot;&gt;&lt;a href=&quot;#Optimizing-File-Loading-And-P
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
</feed>
