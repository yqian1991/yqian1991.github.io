<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu of Daphne</title>
  
  <subtitle>春秋笔法·丹枫嫩寒</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yqian1991.github.io/"/>
  <updated>2019-12-01T17:51:32.939Z</updated>
  <id>http://yqian1991.github.io/</id>
  
  <author>
    <name>Yu Qian</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Best practices to use Apache Spark</title>
    <link href="http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/"/>
    <id>http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/</id>
    <published>2019-12-01T16:23:03.000Z</published>
    <updated>2019-12-01T17:51:32.939Z</updated>
    
    <content type="html"><![CDATA[<p>Learning notes from DataBricks talks</p><h2 id="Optimizing-File-Loading-And-Partition-Discovery"><a href="#Optimizing-File-Loading-And-Partition-Discovery" class="headerlink" title="Optimizing File Loading And Partition Discovery"></a>Optimizing File Loading And Partition Discovery</h2><p>Data loading is the first step of spark application, when dataset becomes large, data loading time becomes an issue.</p><ul><li>Use Datasource tables</li></ul><p>With tables, partition metadata and schema is managed by Hive, based on which partition pruning can be done at logical planning stage. You can also use Spark SQL API directly when loading from a table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.table(<span class="string">"../path"</span>)</span><br><span class="line"></span><br><span class="line">spark.write.partitionBy(<span class="string">"date"</span>).saveAsTable(<span class="string">"table"</span>)</span><br></pre></td></tr></table></figure><ul><li>Specify <code>basePath</code> if loading from external files directly(e.g CSV or Json files)</li></ul><p>When loading from a file, partition discovery is done for each DataFrame creation<br>, also spark needs to infer schema from files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"inferSchema"</span>, true).load(file)</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(knownSchema).load(file)</span><br></pre></td></tr></table></figure><h2 id="Optimizing-File-Storage-and-Layout"><a href="#Optimizing-File-Storage-and-Layout" class="headerlink" title="Optimizing File Storage and Layout"></a>Optimizing File Storage and Layout</h2><ul><li><p>Prefer columnar over text for analytical queries</p></li><li><p>Compression with splittable storage format</p></li><li><p>Avoid large GZIP files</p></li><li><p>Partitioning and bucketing</p></li></ul><p>Parquet + Snappy is a good candidate</p><h2 id="Optimizing-Queries"><a href="#Optimizing-Queries" class="headerlink" title="Optimizing Queries"></a>Optimizing Queries</h2><ul><li>Tuning spark sql shuffle partitions<br><code>spark.sql.shuffle.partitions</code> is used in shuffle operations like groupBy, repartition, join and window. the default value is <code>200</code>.</li></ul><p>This is hard to tune because the parameter is applied on the whole job, which many operations maybe taken, tuning for certain operations may do no good to others.</p><ul><li><p>Adaptive Execution(&gt; Spark 2.0)<br><code>spark.sql.adaptive.enabled</code><br><code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>: default is 64Mb</p></li><li><p>Understanding Unions</p></li><li><p>Data Skipping Index</p></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.youtube.com/watch?v=iwQel6JHMpA" target="_blank" rel="noopener">Youtube Talks</a><br><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/" target="_blank" rel="noopener">Understanding Spark Source Code</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Learning notes from DataBricks talks&lt;/p&gt;
&lt;h2 id=&quot;Optimizing-File-Loading-And-Partition-Discovery&quot;&gt;&lt;a href=&quot;#Optimizing-File-Loading-And-P
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Starting a new journey</title>
    <link href="http://yqian1991.github.io/Life/Starting-a-new-journey/"/>
    <id>http://yqian1991.github.io/Life/Starting-a-new-journey/</id>
    <published>2019-07-01T18:36:56.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to say goodbye.</p><p>Many colleagues were wondering why I leave when it’s only half year to my 4 years anniversary. This is one of the reasons that makes Survey Monkey great. You will get extra 4 weeks vacation every 4 years. The company cares about employees life a lot and try to balance our work and life. Besides the generous take 4 policy, they provide free breakfast and lunch, support you to attend conference every year, allow working from home if you want, monthly bonus points(can redeem Amazon gift cards) etc. just name a few.</p><p>It was not easy to say goodbye to such amazing benefits, leaving an amazing team is also risky. Although all my team members except me located in Bay area, I never feel I am alone, daily stand up, biweekly 1:1 with manager, sprint planning meetings every two weeks and a lot slack calls for discuss or pair programming, we have established a solid way to do remote communication. because of 3 hours time difference, they try their best to no bother me after 6:00pm (EST), and I try my best to cover incidents or issues happen in the morning (before 12:00pm PST), which proved to work very well.</p><p>I was impressed the warm words the team delivered to me upon hearing my news, they even prepared a thank you card and gift to me which I was really touched. I was even questioning myself: is leaving  good?.</p><p>Nobody know a decision is good or not until it proved to be. we experienced a lot good things in the past, nice people, wonderful place, but as life moving on and us growing up, everything has an end. Leaving is exactly a graduation for me. SurveyMonkey is indeed a university for me, where I focused on technology but also took a lot ‘courses’ in cooperation, leading, communication, understanding. And just like 6 years ago, and almost the same time when I graduated from University. maybe this is also a time for my graduation as an intermediate software engineer.</p><p>Graduation marks the starting of a new journey, a new school, in the new journey, I will be a curious student to learn, be a good team player to contribute, be a better man.</p><p>Last but not least, I will be a good husband to support my family. Thanks to my wife for accompanying me since 2016, the journey is meaningless without you.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="Growth" scheme="http://yqian1991.github.io/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>写给Daphne的诗</title>
    <link href="http://yqian1991.github.io/Life/%E5%86%99%E7%BB%99Daphne%E7%9A%84%E8%AF%97/"/>
    <id>http://yqian1991.github.io/Life/写给Daphne的诗/</id>
    <published>2019-05-18T12:36:20.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<pre><code>第一章： 萌芽</code></pre><p>你要问我，我们的故事从哪儿开始，</p><p>走出考场的那一刻，我以为将是故事的结局</p><p>而微信上的只言片语，难道只是我一如既往的淡定？</p><p>也许大家都羡慕一见钟情，</p><p>可比一见钟情更浪漫的，是一聊倾心</p><pre><code>第二章：启 城</code></pre><p>即便我有一双翅膀，我也会将它折断</p><p>因为唾手可得的，到头来也可能只是冷面</p><p>而纵览八百里路云和月，我却恋上你的眼</p><p>如果那双翅膀还在，我必将它修复，</p><p>因为与你的日夜，多想跨过无尽的高山大海</p><p>未来的春夏秋冬，我们可以走得更远</p><p>夜阑人静，午夜梦回，</p><p>你就是我的那双翅膀</p><p>尽管摇摇晃晃， 跌跌撞撞</p><p>但是回头是喜悦，前路是希望</p><pre><code>第三章：誓言</code></pre><p>我记得你每一滴掉下的眼泪，每一次撅起的小嘴</p><p>我记得你激动地辩驳，又原谅的叹息</p><p>相知诚不如相爱那么容易，但是多难都要在一起。</p><p>我记得，你所有有味道的陪伴，邪性的范儿</p><p>我记得，你魔舞的腰枝，磅礴的胃气</p><p>从此，我也会记得，稀饭要稠，汤要浓</p><p>誓言无声，贵在默默坚守</p><p>我心永恒，坚信细水长流</p><p>从此，让我们的爱情澎湃，人生隽永</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;第一章： 萌芽
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;你要问我，我们的故事从哪儿开始，&lt;/p&gt;
&lt;p&gt;走出考场的那一刻，我以为将是故事的结局&lt;/p&gt;
&lt;p&gt;而微信上的只言片语，难道只是我一如既往的淡定？&lt;/p&gt;
&lt;p&gt;也许大家都羡慕一见钟情，&lt;/p&gt;
&lt;p&gt;可比一
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="She" scheme="http://yqian1991.github.io/tags/She/"/>
    
  </entry>
  
  <entry>
    <title>Fun topics in distributed system</title>
    <link href="http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/"/>
    <id>http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/</id>
    <published>2019-04-26T19:54:05.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one after one.</p><ul><li>Micro services architecture</li><li>CDN, Caching, load balancer</li><li>Event Sourcing</li><li>Messaging</li><li>CAP<br>…</li></ul><p>Needless to say, each of these topics are complex enough and takes years to learn. What we expect is: when we are tasked with a system design problem, we will be able to have a robust architecture overall.</p><p>Well, this is important, but when after you see tons of architectures in different use cases, still there is no one fit all solution. There are always tough challenges come up. and usually, the issue that stops you is not the high level design, it’s some small problems instead.</p><ul><li>Sticky Session</li></ul><p>Now we have a lot machines behind the load balancer can serve the traffic, that means requests from a same user can be served by different nodes, hmm, do I need to sign in again every time if the node doesn’t have my session info?</p><p>Absolutely not necessary, now you may think of sticky session which routes the requests for a particular session to the same physical machine that serviced the first request for that session. But problem is not totally solved at this point yet, this method may still cause uneven loads to service which actually we want to solve by distributed system.</p><p><a href="http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/" target="_blank" rel="noopener">http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/</a></p><ul><li>Distributed transactions</li></ul><p>When we throw out a nice distributed architecture, we are confident that it supports high throughputs with decent performance, and you also adds on that, it can handle traffic bursts by adding more machines since it’s a distributed oriented. This looks nice, <em>but how do you handle failures in payment transactions?</em></p><p>I will stuck here for a while, yes, this is not easy, in one machine environment, we can locking and waiting, but if the system is distributed, how can we make sure the transaction ACID?.</p><p>I am not a innovator, but curiosity drives me to find answers with two phase commits and PAXOS. There are tons of materials about those, just giving some I read:<br><a href="https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/" target="_blank" rel="noopener">https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/</a></p><ul><li>Distributed Tracing.<br>It’s common that a user request may cause cascading services calls in the backend, in case of data analysis or debugging, how can we stitch requests to the same origin?</li></ul><p>Using same RequestID is a common strategy, but usually, you will need another service/lib dedicated for distributed tracing. There are some popular frameworks for it, like opentracing.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Distributed system is good, but also introduces more problems you need to solve.<br>Generate a fancy system architecture is easy, what’s more important is how to solve the problems it brings.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Companies (Toronto)</title>
    <link href="http://yqian1991.github.io/Careers/Hidden-Companies/"/>
    <id>http://yqian1991.github.io/Careers/Hidden-Companies/</id>
    <published>2019-04-20T15:32:04.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.</p><p>But there is still a ton of jobs outside those popular sites. And also, most of time, we are interested in opportunities in our local area.</p><p>I did find a lot hidden companies that not easy to be aware of(based in Toronto). Usually due to the following reasons:</p><ul><li><p>They don’t have physical offices in Toronto, but support remote working:<br>Stripe<br>Github<br>DataDog<br>Zapier</p></li><li><p>They are actually good ones/Unicorns, but they only run a tiny team in Toronto, thus less advertised and less presence:<br>Etsy<br>Snap: Through acquisition of Bitmoji<br>Okta<br>iHerb<br>Pivotal (CloudFoundry)<br>Instacart: This one is getting bigger and bigger in Toronto.</p></li></ul><p>Many companies entered Toronto tech scene through acquiring local startups</p><ul><li>They are emerging startups, but some of them will become popular.<br>Dessa<br>Element AI</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.&lt;/p&gt;
&lt;p&gt;But there is still a ton of jobs out
      
    
    </summary>
    
      <category term="Careers" scheme="http://yqian1991.github.io/categories/Careers/"/>
    
    
      <category term="Job seeking" scheme="http://yqian1991.github.io/tags/Job-seeking/"/>
    
  </entry>
  
  <entry>
    <title>NLP in big companies</title>
    <link href="http://yqian1991.github.io/System-Design/NLP-in-big-companies/"/>
    <id>http://yqian1991.github.io/System-Design/NLP-in-big-companies/</id>
    <published>2019-04-19T17:06:13.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out how some other companies build their platforms.</p><h1 id="Uber"><a href="#Uber" class="headerlink" title="Uber"></a>Uber</h1><h2 id="NLP-platform-to-process-customer-support-tickets"><a href="#NLP-platform-to-process-customer-support-tickets" class="headerlink" title="NLP platform to process customer support tickets"></a>NLP platform to process customer support tickets</h2><p>In practice, we may not need very fancy algorithms and some simple ones just work, at least it’s a good starting point of building an engineering product.<br>Uber tried to use NLP to process customer support tickets with a classification model(logistic regression), Data processing is always the key task before machine learning, like how to encoding ticket and transform text, category to numerical vectors, apparently word2vec can be used here.</p><p>In the future, more complex and high performance algorithms an deep learning frameworks can be adopted like WordCNN.</p><p>In terms of the foundation platform, Uber utilizes Spark + hive for big data processing and scaled prediction.</p><p><a href="https://eng.uber.com/nlp-deep-learning-uber-maps/" target="_blank" rel="noopener">https://eng.uber.com/nlp-deep-learning-uber-maps/</a></p><h2 id="Uber-one-click-chat"><a href="#Uber-one-click-chat" class="headerlink" title="Uber one click chat"></a>Uber one click chat</h2><p>This is a smart reply system that auto-reply to user messages.</p><p>From this system, we can have a sense that a typical machine learning platform usually has two components:</p><ul><li><p>Offline training:<br>Using NLP and ML pipelines to do intent detection. Here is where NLP models got applied like Doc2Vec model</p></li><li><p>Online serving:<br>A message will be encoded as fixed-length vector representation via the pre-trained Doc2vec model, after which the vector and the intent detection classifier will be used to predict the message’s possible intent.</p></li></ul><p>The system then retrieve the most relevant replies based on the detected intent and surface them to the driver-partner receiving the message</p><p><a href="https://eng.uber.com/one-click-chat/" target="_blank" rel="noopener">https://eng.uber.com/one-click-chat/</a></p><h1 id="Airbnb"><a href="#Airbnb" class="headerlink" title="Airbnb"></a>Airbnb</h1><p>Airbnb built an online risk mitigation system which it mentioned some requirements of a machine learning platform:</p><ul><li>Fast</li><li>Robust</li><li>Scale</li></ul><p>Although it used an open source framework(OpenScoring) for it, we should know the typical pipeline is the same most of the times like what we saw in Airbnb platforms above. Feel free to check out some characteristics of OpenScoring.</p><p><a href="https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60</a><br><a href="https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d</a></p><h1 id="Zendesk"><a href="#Zendesk" class="headerlink" title="Zendesk"></a>Zendesk</h1><p>Zendesk summarizes customer support tickets to topics.</p><p>A big take-way is how they support 50k models on a daily basis with AWS Batch,<br>AWS batch supports auto scaling and job management, it also provides GPU support.<br>In terms of job management, this is a hot topic and demand need for building large scale platforms. A lot products emerges in this regards, like Airflow.</p><p><a href="https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9" target="_blank" rel="noopener">https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9</a></p><h1 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h1><p>Cortex</p><h1 id="LinkedIn"><a href="#LinkedIn" class="headerlink" title="LinkedIn"></a>LinkedIn</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out ho
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Natural Language Processing 101</title>
    <link href="http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/"/>
    <id>http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/</id>
    <published>2019-04-17T14:41:53.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.</p><h1 id="What-is-Natural-Language-Processing"><a href="#What-is-Natural-Language-Processing" class="headerlink" title="What is Natural Language Processing?"></a>What is Natural Language Processing?</h1><p>Natural Language Processing (NLP) is an important sub category in Artificial Intelligence that enabling computers to understand and process human languages, it tries to get computers closer to a human-level understanding of language.</p><h2 id="Some-research-topics-in-NLP"><a href="#Some-research-topics-in-NLP" class="headerlink" title="Some research topics in NLP"></a>Some research topics in NLP</h2><ul><li>Information Retrieval/Extraction/Filtering</li><li>Machine Translation</li><li>Document/Topic Classification/Summarization</li><li>Question Answering</li><li>Text Mining</li><li>Sentiment Analysis</li><li>Speech Recognition</li><li>Machine Writing/Content Genetation</li></ul><h1 id="Statistical-Language-Models"><a href="#Statistical-Language-Models" class="headerlink" title="Statistical Language Models"></a>Statistical Language Models</h1><p>This is to compute the probability of a sentence or sequence of words.</p><h2 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h2><p>N-gram is a popular statistical language model.<br>After building a model, we usually use cross-entropy and perplexity to evaluate the model.<br>Lower perplexities correspond to higher likelihoods, so lower scores are better on this<br>metric.</p><p>A major concern in language modeling is to avoid the situation <code>p(w) = 0</code>, which could arise as a result of a single unseen n-gram, the solution is using smoothing methods, some smoothing methods includes:</p><ul><li>Add-One(Laplace) smoothing</li><li>Good-Turing smoothing</li><li>Kneser-Ney smoothing</li><li>Witten-Bell smoothing</li></ul><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>A sentence/document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, POS tag, semantics and syntax etc, are all discarded.</p><h2 id="Probabilistic-Graphical-Models"><a href="#Probabilistic-Graphical-Models" class="headerlink" title="Probabilistic Graphical Models"></a>Probabilistic Graphical Models</h2><p>This is an important math theory/algorithm used in NLP tasks.</p><ul><li>Bayesian Network</li><li>Markov Network</li><li>Condition Random Fields</li><li>Hidden Markov Models</li><li>Estimation Maximization</li><li>Max Entropy</li></ul><h2 id="Topic-Model"><a href="#Topic-Model" class="headerlink" title="Topic Model"></a>Topic Model</h2><ul><li>Latent Dirichlet Allocation (LDA): Based on probabilistic graphical models</li><li>LSA: Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra</li><li>NMF: Non-Negative Matrix Factorization – Based on Linear Algebra</li></ul><h1 id="Some-popular-tasks-in-NLP"><a href="#Some-popular-tasks-in-NLP" class="headerlink" title="Some popular tasks in NLP"></a>Some popular tasks in NLP</h1><p>These are some tasks that may not be the solution to any particular NLP problem but are done as pre-requisites to simplify a lot of different problems in NLP. These are pretty much like reading comprehension we learn in school.</p><h2 id="Parts-of-Speech-Tagging"><a href="#Parts-of-Speech-Tagging" class="headerlink" title="Parts of Speech Tagging"></a>Parts of Speech Tagging</h2><p>Identify Proper nouns, Common nouns, Verbs, Adjectives, Preposition etc.</p><h2 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h2><p>Identify name of people, location etc.</p><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><h2 id="Morphosyntactic-Attributes"><a href="#Morphosyntactic-Attributes" class="headerlink" title="Morphosyntactic Attributes"></a>Morphosyntactic Attributes</h2><h1 id="Deep-Learning-in-NLP"><a href="#Deep-Learning-in-NLP" class="headerlink" title="Deep Learning in NLP"></a>Deep Learning in NLP</h1><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Previously, there are some other popular distributed representation of word as vectors, like Tf-Idf.<br> But they are sparse and long which is not computing efficient. Word2Vec instead is a dense vector representation of words(commonly 100-500 dimensions). and it models the meaning of a word as an embedding.</p><p>But how to get the dense vectors? Singular value    decomposition(Latent Semantic    Analysis) can be used, but a more successful way is through neural network inspired learning strategy.</p><ul><li>CBOW: Predict center/target word based on context words</li><li>Skip-grams: Predict context words based on center/target word.</li></ul><p>Other vector based models include: fastText, Doc2Vec, GloVe etc.</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>Stay tuned…</p><p>Highly recommend <a href="https://people.cs.umass.edu/~miyyer/cs585/" target="_blank" rel="noopener">https://people.cs.umass.edu/~miyyer/cs585/</a> as 101 course for NLP.<br>More advanced courses:<br><a href="https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6" target="_blank" rel="noopener">https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.&lt;/p&gt;
&lt;h1 
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Searching with bloom filter</title>
    <link href="http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/"/>
    <id>http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/</id>
    <published>2018-10-16T21:09:45.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>Our platform is sending 4 million emails per day, and many of them contains a lot user generated content which has potential risk of spam. A very important action we need to take is: if we know the sender is already marked as a bad email, we should stop sending emails for it right away.</p><p>We now have more than ten thousands of blacklisted emails stored in database, when sending an email, we need to check against this big list to see if we can send the current email out. retrieving all emails from db is not realistic since it has significant latency.</p><h1 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h1><h2 id="Let-DB-do-the-search"><a href="#Let-DB-do-the-search" class="headerlink" title="Let DB do the search"></a>Let DB do the search</h2><p>This is simple to implement and fit the current situation well since all emails are stored in database, we just need a stored procedure to take an email as input and return True if the email is in the table, otherwise false.</p><h2 id="Bloom-filter"><a href="#Bloom-filter" class="headerlink" title="Bloom filter"></a>Bloom filter</h2><p>Thinking out of the box, store with database itself may not be a good solution considering latency of DB read and write. Bloom filter is an excellent algorithm for this kind of large scale search.</p><p>The algorithm is very ideal, but when comes to engineering, we still need to consider the trade-offs based on the current service architecture.</p><ul><li>The current black listed emails are stored in database, and the write operation is done by a different service, we need to consider do we need to change the storage for bloom filter</li><li><p>bloom filter needs tuning to get low false positive rates. considering the growth rate of black listed emails, how do we adjust the size of hash functions and bit array lengths</p><p>Here is a useful tool to do the math: <a href="https://hur.st/bloomfilter/" target="_blank" rel="noopener">https://hur.st/bloomfilter/</a></p></li></ul><h1 id="My-take-aways"><a href="#My-take-aways" class="headerlink" title="My take aways"></a>My take aways</h1><ul><li>Thinking out of the box can bring you more brilliant ideas</li><li>Sometimes, excellent algorithm is not always fit</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Problem-statement&quot;&gt;&lt;a href=&quot;#Problem-statement&quot; class=&quot;headerlink&quot; title=&quot;Problem statement&quot;&gt;&lt;/a&gt;Problem statement&lt;/h1&gt;&lt;p&gt;Our platfo
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Algorithm" scheme="http://yqian1991.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Compare streaming frameworks</title>
    <link href="http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/"/>
    <id>http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/</id>
    <published>2018-10-16T20:06:49.000Z</published>
    <updated>2019-12-03T02:25:50.445Z</updated>
    
    <content type="html"><![CDATA[<p>The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is said that master/slave is not good words now). We use spark streaming to read messages from Kafka and rollup metrics, then send to RabbitMQ which will be used to send out notifications.</p><p>There are 20 million questions answered each day on our platform, to better keep customers engaged and provide them better insights, it’s nice to send notifications to the survey owners about the responses they collected, this needs to be happen in a timely manner, so Apache Spark seems to be a good fit.</p><p>Spark is so popular at that time, it is the only streaming framework I know, and I even thought that is the only streaming framework ever exists. Later on, I heard more and more voices that spark is not a real streaming tool, instead a batching process. Now I understand that Apache spark is for batch processing and spark streaming is for mini-batch processing. With the arising of LAMBDA framework, spark becomes popular since it can do both batch and “streaming” process.</p><p>There are some other streaming frameworks in the market, Apache Storm, Apache Samza, Apache Flink etc. It is easy to get lost when facing too many choices, it is also true that each of them has suitable use cases. what’s important is to understand the problem we are going to solve.</p><p>Here are some facts of those frameworks:</p><h1 id="Apache-Storm"><a href="#Apache-Storm" class="headerlink" title="Apache Storm"></a>Apache Storm</h1><p>Open sourced by Twitter and was initially released in 2011.</p><p>Key concepts:</p><ul><li>Spouts: (data source)</li><li>Bolts: functions</li><li>Tuple: stream</li></ul><p>Storm is able to process +1M msgs/second/node, A cluster can be configured to have &gt; 1k workers, Storm provides at least once delivery</p><p>On top of its core, Storm also provides Trident API for micro batch processing,<br>it assures exactly once guarantee.</p><p>With Trident API, you can do aggregation, merge, join, grouping, functions, filters etc.</p><p>Storm supports data transfer protocol: Avro, thrift.</p><h1 id="Apache-Flink"><a href="#Apache-Flink" class="headerlink" title="Apache Flink"></a>Apache Flink</h1><p>Flink is originated from an academic project called <code>Stratosphere</code>, it had the first release after transferring to Apache incubator at 2014.</p><p>Flink unified stream and batch processing, streaming is the nature of flink, and batch is regarded as a special case of streaming which in contrast of Spark.</p><p>Flink provides exactly once processing.</p><p>It has rich higher level API compared to Storm, also support out of order process</p><p>It is able to process 1.5M msgs/second/node</p><h1 id="Apache-Spark"><a href="#Apache-Spark" class="headerlink" title="Apache Spark"></a>Apache Spark</h1><p>Originally developed at the University of California, Berkeley’s AMPLab in 2009 and open sourced in 2010.</p><p>Spark can do exactly once delivery as well, it is working with RDD, but now suggested to use DataFrame/DataSet.</p><p>Spark is also known as hard to tune those complicate parameters for better performance.</p><h1 id="Apache-Samza"><a href="#Apache-Samza" class="headerlink" title="Apache Samza"></a>Apache Samza</h1><p>The project entered Apache Incubator in 2013 and was originally created at LinkedIn.</p><p>Samza achieves at least once delivery.</p><p>Samza is famous for its state management based on RockDB, but it’s tightly coupled with Kafka and Yarn.</p><p>It lacks some advanced features like watermarks, sessions.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is 
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Notes on data science self learning</title>
    <link href="http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/"/>
    <id>http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/</id>
    <published>2018-08-15T14:54:58.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from two people:</p><ul><li><a href="https://www.coxy1989.com/curriculum.html" target="_blank" rel="noopener">https://www.coxy1989.com/curriculum.html</a></li><li><a href="http://karlrosaen.com/ml/" target="_blank" rel="noopener">http://karlrosaen.com/ml/</a>.</li></ul><p>Thanks a lot to them for sharing their own experiences.</p><p>This blog is a guideline that I will continue update. Side by side, I may post learning logs with details on what I learned.</p><h1 id="Stage-1-Foundations"><a href="#Stage-1-Foundations" class="headerlink" title="Stage 1 - Foundations"></a>Stage 1 - Foundations</h1><p>This is to lay a good foundation for later machine learning, which includes:</p><h2 id="Probability-and-Statistics"><a href="#Probability-and-Statistics" class="headerlink" title="Probability and Statistics"></a>Probability and Statistics</h2><p>Bloomberg ML EDU course which I am following at this moment: <a href="https://bloomberg.github.io/foml/#about" target="_blank" rel="noopener">https://bloomberg.github.io/foml/#about</a><br>More resources on statistics: <a href="https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html" target="_blank" rel="noopener">https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html</a><br>Problems and solutions: <a href="http://karlrosaen.com/ml/hw/" target="_blank" rel="noopener">http://karlrosaen.com/ml/hw/</a></p><p>Some supporting materials:</p><ul><li>Mathematics background check: <a href="https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf" target="_blank" rel="noopener">https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf</a></li><li>Simple statistics crib-sheet: <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf" target="_blank" rel="noopener">http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf</a></li></ul><h2 id="Machine-learning-101"><a href="#Machine-learning-101" class="headerlink" title="Machine learning 101"></a>Machine learning 101</h2><p>Now you can have a glance of what machine learning is, and use your mathematics learned to understand concepts and practice on easy tasks.</p><p>Machine learning by Andrew Ng: <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a><br>Python Machine Learning: <a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition" target="_blank" rel="noopener">https://github.com/rasbt/python-machine-learning-book-2nd-edition</a></p><p>There are some good podcasts:</p><ul><li>Talking machine podcasts:<a href="https://www.thetalkingmachines.com/" target="_blank" rel="noopener">https://www.thetalkingmachines.com/</a></li><li>Becoming a data scientist podcasts: <a href="https://www.becomingadatascientist.com/category/podcast/" target="_blank" rel="noopener">https://www.becomingadatascientist.com/category/podcast/</a></li><li>The Master Algorithm: <a href="https://player.fm/series/data-skeptic/the-master-algorithm" target="_blank" rel="noopener">https://player.fm/series/data-skeptic/the-master-algorithm</a></li></ul><p>To complete this stage, 5 months are recommended.</p><h1 id="Stage-2-Applied"><a href="#Stage-2-Applied" class="headerlink" title="Stage 2 - Applied"></a>Stage 2 - Applied</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>[译] Airflow: 一个工作流程管理平台</title>
    <link href="http://yqian1991.github.io/System-Design/%E8%AF%91-Airflow-%E4%B8%80%E4%B8%AA%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0/"/>
    <id>http://yqian1991.github.io/System-Design/译-Airflow-一个工作流程管理平台/</id>
    <published>2018-07-28T02:28:02.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>原文地址：<a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" target="_blank" rel="noopener">Airflow: a workflow management platform</a></li><li>原文作者：<a href="https://medium.com/@maximebeauchemin" target="_blank" rel="noopener">Maxime Beauchemin</a></li><li>译文出自：<a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a></li><li>本文永久链接：<a href="https://github.com/xitu/gold-miner/blob/master/TODO1/airflow-a-workflow-management-platform.md" target="_blank" rel="noopener">https://github.com/xitu/gold-miner/blob/master/TODO1/airflow-a-workflow-management-platform.md</a></li><li>译者：<a href="https://github.com/yqian1991" target="_blank" rel="noopener">yqian1991</a></li><li>校对者：<a href="https://github.com/Park-ma" target="_blank" rel="noopener">Park-ma</a> <a href="https://github.com/DerekDick" target="_blank" rel="noopener">DerekDick</a></li></ul></blockquote><h1 id="Airflow-一个工作流程管理平台"><a href="#Airflow-一个工作流程管理平台" class="headerlink" title="Airflow: 一个工作流程管理平台"></a>Airflow: 一个工作流程管理平台</h1><p>出自 <a href="https://medium.com/@maximebeauchemin" target="_blank" rel="noopener">Maxime Beauchemin</a></p><p><img src="https://cdn-images-1.medium.com/max/800/0*277Imf2r7ouTXOVy.png" alt></p><p><strong>Airbnb</strong> 是一个快速增长的、数据启示型的公司。我们的数据团队和数据量都在快速地增长，同时我们所面临的挑战的复杂性也在同步增长。我们正在扩张的数据工程师、数据科学家和分析师团队在使用 <strong>Airflow</strong>，它是我们搭建的一个可以快速推进工作，保持发展优势的平台，因为我们可以自己编辑、监控和改写 <strong>数据管道</strong>。</p><p>今天，我们非常自豪地宣布我们要 <strong>开源</strong> 和 <strong>共享</strong> 我们的工作流程管理平台：<strong>Airflow</strong>。</p><p><a href="https://github.com/apache/incubator-airflow" target="_blank" rel="noopener">https://github.com/airbnb/airflow</a></p><hr><h3 id="有向无环图（DAGs）呈绽放之势"><a href="#有向无环图（DAGs）呈绽放之势" class="headerlink" title="有向无环图（DAGs）呈绽放之势"></a>有向无环图（DAGs）呈绽放之势</h3><p>当与数据打交道的工作人员开始将他们的流程自动化，那么写批处理作业是不可避免的。这些作业必须按照一个给定的时间安排执行，它们通常依赖于一组已有的数据集，并且其它的作业也会依赖于它们。即使你让好几个数据工作节点在一起工作很短的一段时间，用于计算的批处理作业也会很快地扩大成一个复杂的图。现在，如果有一个工作节奏快、中型规模的数据团队，而且他们在几年之内要面临不断改进的数据基础设施，并且手头上还有大量复杂的计算作业网络。那这个复杂性就成为数据团队需要处理，甚至深入了解的一个重要负担。</p><p>这些作业网络通常就是 <strong>有向无环图</strong>（<strong>DAGs</strong>），它们具有以下属性：</p><ul><li><strong>已排程：</strong> 每个作业应该按计划好的时间间隔运行</li><li><strong>关键任务：</strong> 如果一些作业没有运行，那我们就有麻烦了</li><li><strong>演进：</strong> 随着公司和数据团队的成熟，数据处理也会变得成熟</li><li><strong>异质性：</strong> 现代化的分析技术栈正在快速发生着改变，而且大多数公司都运行着好几个需要被粘合在一起的系统</li></ul><h3 id="每个公司都有一个（或者多个）"><a href="#每个公司都有一个（或者多个）" class="headerlink" title="每个公司都有一个（或者多个）"></a>每个公司都有一个（或者多个）</h3><p><strong>工作流程管理</strong> 已经成为一个常见的需求，因为大多数公司内部有多种创建和调度作业的方式。你总是可以从古老的 cron 调度器开始，并且很多供应商的开发包都自带调度功能。下一步就是创建脚本来调用其它的脚本，这在短期时间内是可以工作的。最终，一些为了解决作业状态存储和依赖的简单框架就涌现了。</p><p>通常，这些解决方案都是 <strong>被动增长</strong> 的，它们都是为了响应特定作业调度需求的增长，而这通常也是因为现有的这种系统的变种连简单的扩展都做不到。同时也请注意，那些编写数据管道的人通常不是软件工程师，并且他们的任务和竞争力都是围绕着处理和分析数据的，而不是搭建工作流程管理系统。</p><p>鉴于公司内部工作流程管理系统的成长总是比公司的需求落后至少一代，作业的编辑、调度和错误排查之间的 <strong>摩擦</strong> 制造了大量低效且令人沮丧的事情，这使得数据工作者和他们的高产出路线背道而驰。</p><h3 id="Airflow"><a href="#Airflow" class="headerlink" title="Airflow"></a>Airflow</h3><p>在评审完开源解决方案，同时听取 Airbnb 的员工对他们过去使用的系统的见解后，我们得出的结论是市场上没有任何可以满足我们当前和未来需求的方案。我们决定搭建一个崭新的系统来正确地解决这个问题。随着这个项目的开发进展，我们意识到我们有一个极好的机会去回馈我们也极度依赖的开源社区。因此，我们决定依照 Apache 的许可开源这个项目。</p><p>这里是 Airbnb 的一些靠 Airflow 推动的处理工序：</p><ul><li><strong>数据仓储：</strong> 清洗、组织规划、数据质量检测并且将数据发布到我们持续增长的数据仓库中去</li><li><strong>增长分析：</strong> 计算关于住客和房主参与度的指标以及增长审计</li><li><strong>试验：</strong> 计算我们 A/B 测试试验框架的逻辑并进行合计</li><li><strong>定向电子邮件：</strong> 对目标使用规则并且通过群发邮件来吸引用户</li><li><strong>会话（Sessionization）：</strong> 计算点击流和停留时间的数据集</li><li><strong>搜索：</strong> 计算搜索排名相关的指标</li><li><strong>数据基础架构维护：</strong> 数据库抓取、文件夹清理以及应用数据留存策略…</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>就像英语是商务活动经常使用的语言一样，Python 已经稳固地将自己树立为数据工作的语言。Airflow 从创建之初就是用 Python 编写的。代码库可扩展、文档齐全、风格一致、语法过检并且有很高的单元测试覆盖率。</p><p>管道的编写也是用 Python 完成的，这意味着通过配置文件或者其他元数据进行动态管道生成是与生俱来的。“<strong>配置即代码</strong>” 是我们为了达到这个目的而坚守的准则。虽然基于 yaml 或者 json 的作业配置方式可以让我们用任何语言来生成 Airflow 数据管道，但是我们感觉到转化过程中的一些流动性丧失了。能够内省代码（ipython！和集成开发工具）子类和元程序并且使用导入的库来帮助编写数据管道为 Airflow 增加了巨大的价值。注意，只要你能写 Python 代码来解释配置，你还是可以用任何编程语言或者标记语言来编辑作业。</p><p>你仅需几行命令就可以让 Airflow 运行起来，但是它的完整架构包含有下面这么多组件：</p><ul><li><strong>作业定义</strong>，包含在源代码控制中。</li><li>一个丰富的 <strong>命令行工具</strong> (命令行接口) 用来测试、运行、回填、描述和清理你的有向无环图的组成部件。</li><li>一个 <strong>web 应用程序</strong>，用来浏览有向无环图的定义、依赖项、进度、元数据和日志。web 服务器打包在 Airflow 里面并且是基于 Python web 框架 Flask 构建的。</li><li>一个 <strong>元数据仓库</strong>，通常是一个 MySQL 或者 Postgres 数据库，Airflow 可以用它来记录任务作业状态和其它持久化的信息。</li><li>一组 <strong>工作节点</strong>，以分布式的方式运行作业的任务实例。</li><li><strong>调度</strong> 程序，触发准备运行的任务实例。</li></ul><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>Airflow 自带各种与 Hive、Presto、MySQL、HDFS、Postgres 和 S3 这些常用系统交互的方法，并且允许你触发任意的脚本，基础模块也被设计得非常容易进行扩展。</p><p><strong>Hooks</strong> 被定义成外部系统的抽象并且共享同样的接口。Hooks 使用中心化的 vault 数据库将主机/端口/登录名/密码信息进行抽象并且提供了可供调用的方法来跟这些系统进行交互。</p><p><strong>操作符</strong> 利用 hooks 生成特定的任务，这些任务在实例化后就变成了数据流程中的节点。所有的操作符都派生自 BaseOperator 并且继承了一组丰富的属性和方法。三种主流的操作符分别是：</p><ul><li>执行 <strong>动作</strong> 的操作符, 或者通知其它系统去执行一个动作</li><li><strong>转移</strong> 操作符将数据从一个系统移动到另一个系统</li><li><strong>传感器</strong> 是一类特定的操作符，它们会一直运行直到满足了特定的条件</li></ul><p><strong>执行器（Executors）</strong> 实现了一个接口，它可以让 Airflow 组件（命令行接口、调度器和 web 服务器）可以远程执行作业。目前，Airflow 自带一个 SequentialExecutor（用来做测试）、一个多线程的 LocalExecutor、一个使用了 <a href="http://www.celeryproject.org/" target="_blank" rel="noopener">Celery</a> 的 CeleryExecutor 和一个超棒的基于分布式消息传递的异步任务队列。我们也计划在不久后开源 YarnExecutor。</p><h3 id="一个绚丽的用户界面"><a href="#一个绚丽的用户界面" class="headerlink" title="一个绚丽的用户界面"></a>一个绚丽的用户界面</h3><p>虽然 Airflow 提供了一个丰富的<a href="https://airflow.apache.org/cli.html" target="_blank" rel="noopener">命令行接口</a>，但是最好的工作流监控和交互办法还是使用 web 用户接口。你可以容易地图形化显示管道依赖项、查看进度、轻松获取日志、查阅相关代码、触发任务、修正 false positives/negatives 以及分析任务消耗的时间，同时你也能得到一个任务通常在每天什么时候结束的全面视图。用户界面也提供了一些管理功能：管理连接、池和暂停有向无环图的进程。</p><p><img src="https://cdn-images-1.medium.com/max/400/1*nbwR8O-CDH67fkHrXVDvYw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*0Mask8UZw_aCsd_7JM2Rjw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*JNOJotSnC3t0TIQC8gYcsg.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/600/1*qqOg_8bMS_MzDgWSbgdtOw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*rNaZuJ2168jvUYiEkdu1ww.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*ojItdtSC6etsUWOZIK8trw.png" alt></p><p>锦上添花的是，用户界面有一个 <a href="https://airflow.apache.org/profiling.html" target="_blank" rel="noopener">Data Profiling</a> 区，可以让用户在注册好的连接上进行 SQL 查询、浏览结果集，同时也提供了创建和分享一些简单图表的方法。这个制图应用是由 <a href="http://www.highcharts.com/" target="_blank" rel="noopener">Highcharts</a>、<a href="https://flask-admin.readthedocs.org/en/v1.0.9/" target="_blank" rel="noopener">Flask Admin</a> 的增删改查接口以及 Airflow 的 <a href="https://airflow.apache.org/code.html#hooks" target="_blank" rel="noopener">hooks</a> 和 <a href="https://airflow.apache.org/code.html#macros" target="_blank" rel="noopener">宏</a>库混搭而成的。URL 参数可以传递给你图表中使用的 SQL，Airflow 的宏是通过 <a href="http://jinja.pocoo.org/" target="_blank" rel="noopener">Jinja templating</a> 的方式工作的。有了这些特性和查询功能，Airflow 用户可以很容易的创建和分享结果集和图表。</p><p><img src="https://cdn-images-1.medium.com/max/400/1*8SD5x-62kLVzZ9SSfAXKCg.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*2L-uvEnYDvf5FG3eMuknuQ.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*EbUXRyeS65GZTXbCPWrF7w.png" alt></p><h3 id="一种催化剂"><a href="#一种催化剂" class="headerlink" title="一种催化剂"></a>一种催化剂</h3><p>使用 Airflow 之后，Airbnb 的员工进行数据工作的生产率和热情提高了好g几倍。管道的编写也加速了，监控和错误排查所花费的时间也显著减少了。更重要的是，这个平台允许人们从一个更高级别的抽象中去创建可重用的模块、计算框架以及服务。</p><h3 id="说得够多的了！"><a href="#说得够多的了！" class="headerlink" title="说得够多的了！"></a>说得够多的了！</h3><p>我们已经通过一个启发式的教程把试用 Airflow 变得极其简单。想看到示例结果也只需要执行几个 shell 命令。看一看 <a href="https://airflow.apache.org/" target="_blank" rel="noopener">Airflow 文档</a> 的<a href="https://airflow.apache.org/start.html" target="_blank" rel="noopener">快速上手</a>和<a href="https://airflow.apache.org/tutorial.html" target="_blank" rel="noopener">教程</a>部分，你可以在几分钟之内就让你的 Airflow web 程序以及它自带的交互式实例跑起来！</p><p><a href="https://github.com/apache/incubator-airflow" target="_blank" rel="noopener">https://github.com/airbnb/airflow</a></p><p><img src="https://cdn-images-1.medium.com/max/800/1*YsUOrWx3mRxZZljtc9xZyw.png" alt></p><h4 id="在-airbnb-io-上查看我们所有的开源项目并-在-Twitter-上关注我们：-AirbnbEng-AirbnbData"><a href="#在-airbnb-io-上查看我们所有的开源项目并-在-Twitter-上关注我们：-AirbnbEng-AirbnbData" class="headerlink" title="在 airbnb.io 上查看我们所有的开源项目并 在 Twitter 上关注我们：@AirbnbEng + @AirbnbData"></a>在 <a href="http://airbnb.io" target="_blank" rel="noopener">airbnb.io</a> 上查看我们所有的开源项目并 在 Twitter 上关注我们：<a href="https://twitter.com/AirbnbEng" target="_blank" rel="noopener">@AirbnbEng</a> + <a href="https://twitter.com/AirbnbData" target="_blank" rel="noopener">@AirbnbData</a></h4><blockquote><p>如果发现译文存在错误或其他需要改进的地方，欢迎到 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 <strong>本文永久链接</strong> 即为本文在 GitHub 上的 MarkDown 链接。</p></blockquote><hr><blockquote><p><a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im" target="_blank" rel="noopener">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android" target="_blank" rel="noopener">Android</a>、<a href="https://github.com/xitu/gold-miner#ios" target="_blank" rel="noopener">iOS</a>、<a href="https://github.com/xitu/gold-miner#前端" target="_blank" rel="noopener">前端</a>、<a href="https://github.com/xitu/gold-miner#后端" target="_blank" rel="noopener">后端</a>、<a href="https://github.com/xitu/gold-miner#区块链" target="_blank" rel="noopener">区块链</a>、<a href="https://github.com/xitu/gold-miner#产品" target="_blank" rel="noopener">产品</a>、<a href="https://github.com/xitu/gold-miner#设计" target="_blank" rel="noopener">设计</a>、<a href="https://github.com/xitu/gold-miner#人工智能" target="_blank" rel="noopener">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi" target="_blank" rel="noopener">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi" target="_blank" rel="noopener">知乎专栏</a>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原文地址：&lt;a href=&quot;https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8&quot; target=&quot;_blan
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Airflow" scheme="http://yqian1991.github.io/tags/Airflow/"/>
    
  </entry>
  
  <entry>
    <title>[译] 我们是如何高效实现一致性哈希的</title>
    <link href="http://yqian1991.github.io/System-Design/How-we-implemented-consistent-hashing-efficiently/"/>
    <id>http://yqian1991.github.io/System-Design/How-we-implemented-consistent-hashing-efficiently/</id>
    <published>2018-07-22T14:01:32.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>原文地址：<a href="https://blog.ably.io/how-to-implement-consistent-hashing-efficiently-fe038d59fff2" target="_blank" rel="noopener">How we implemented consistent hashing efficiently</a></li><li>原文作者：<a href="https://blog.ably.io/@n.srushtika?source=post_header_lockup" target="_blank" rel="noopener">Srushtika Neelakantam</a></li><li>译文出自：<a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a></li><li>本文永久链接：<a href="https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-implement-consistent-hashing-efficiently.md" target="_blank" rel="noopener">https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-implement-consistent-hashing-efficiently.md</a></li><li>译者：<a href="https://github.com/yqian1991" target="_blank" rel="noopener">yqian1991</a></li><li>校对者：<a href="https://github.com/Starrier" target="_blank" rel="noopener">Starrier</a></li></ul></blockquote><h1 id="我们是如何高效实现一致性哈希的"><a href="#我们是如何高效实现一致性哈希的" class="headerlink" title="我们是如何高效实现一致性哈希的"></a>我们是如何高效实现一致性哈希的</h1><h2 id="Ably-的实时平台分布在超过-14-个物理数据中心和-100-多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。"><a href="#Ably-的实时平台分布在超过-14-个物理数据中心和-100-多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。" class="headerlink" title="Ably 的实时平台分布在超过 14 个物理数据中心和 100 多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。"></a>Ably 的实时平台分布在超过 14 个物理数据中心和 100 多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。</h2><p>在这篇文章中，我们将会理解一致性哈希到底是怎么回事，为什么它是可伸缩的分布式系统架构中的一个重要工具。然后更进一步，我们会介绍可以用来高效率规模化实现一致性哈希算法的数据结构。最后，我们也会带大家看一看用这个算法实现的一个可工作实例。</p><h3 id="再谈哈希"><a href="#再谈哈希" class="headerlink" title="再谈哈希"></a>再谈哈希</h3><p>还记得大学里学的那个古老而原始的哈希方法吗？通过使用哈希函数，我们确保了计算机程序所需要的资源可以通过一种高效的方式存储在内存中，也确保了内存数据结构能被均匀加载。我们也确保了这种资源存储策略使信息检索变得更高效，从而让程序运行得更快。</p><p>经典的哈希方法用一个哈希函数来生成一个伪随机数，然后这个伪随机数被内存空间大小整除，从而将一个随机的数值标识转换成可用内存空间里的一个位置。就如同下面这个函数所示：</p><p><code>location = hash(key) mod size</code></p><p><img src="https://cdn-images-1.medium.com/max/800/1*ojknKxQ7uxGaJEam2nQYWQ.png" alt></p><h3 id="既然如此，我们为什么不能用同样的方法来处理网络请求呢？"><a href="#既然如此，我们为什么不能用同样的方法来处理网络请求呢？" class="headerlink" title="既然如此，我们为什么不能用同样的方法来处理网络请求呢？"></a>既然如此，我们为什么不能用同样的方法来处理网络请求呢？</h3><p>在各种不同的程序、计算机或者用户从多个服务器请求资源的场景里，我们需要一种机制来将请求均匀地分布到可用的服务器上，从而保证负载均衡，并且保持稳定一致的性能。我们可以将这些服务器节点看做是一个或多个请求可以被映射到的位置。</p><p>现在让我们先退一步。在传统的哈希方法中，我们总是假设：</p><ul><li>内存位置的数量是已知的，并且</li><li>这个数量从不改变</li></ul><p>例如，在 Ably，我们一整天里通常需要扩大或者缩减集群的大小，而且我们也要处理一些意外的故障。但是，如果我们考虑前面提到的这些场景的话，我们就不能保证服务器数量是不变的。如果其中一个服务器发生意外故障了怎么办？如果继续使用最简单的哈希方法，结果就是我们需要对每个哈希键重新计算哈希值，因为新的映射完全决定于服务器节点或者内存地址的数量，如下图所示：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*ojknKxQ7uxGaJEam2nQYWQ.png" alt></p><p>节点变化之前</p><p><img src="https://cdn-images-1.medium.com/max/800/1*8wnQ4y-9waQPC6sHdmZgdg.png" alt></p><p>节点变化之后</p><p>在分布式系统中使用简单再哈希存在的问题 — 每个哈希键的存放位置都会变化 — 就是因为每个节点都存放了一个状态；哪怕只是集群数目的一个非常小的变化，都可能导致需要重新排列集群上的所有数据，从而产生巨大的工作量。随着集群的增长，重新哈希的方法是没法持续使用的，因为重新哈希所需要的工作量会随着集群的大小而线性地增长。这就是一致性哈希的概念被引入的场景。</p><h3 id="一致性哈希-—-它到底是什么？"><a href="#一致性哈希-—-它到底是什么？" class="headerlink" title="一致性哈希 — 它到底是什么？"></a>一致性哈希 — 它到底是什么？</h3><p>一致性哈希可以用下面的方式描述：</p><ul><li>它用虚拟环形的结构来表示资源请求者（为了叙述方便，后文将称之为“请求”）和服务器节点，这个环通常被称作一个 <strong>hashring</strong>。</li><li>存储位置的数量不再是确定的，但是我们认为这个环上有无穷多个点并且服务器节点可以被放置到环上的任意位置。当然，我们仍然可以使用哈希函数来选择这个随机数，但是之前的第二个步骤，也就是除以存储位置数量的那一步被省略了，因为存储位置的数量不再是一个有限的数值。</li><li>请求，例如用户，计算机或者无服务（serverless）程序，这些就等同于传统哈希方法中的键，也使用同样的哈希函数被放置到同样的环上。</li></ul><p><img src="https://cdn-images-1.medium.com/max/800/1*002BDjvoadVRbPyo0lkuiQ.png" alt></p><p>那么它到底是如何决定请求被哪个服务器所服务呢？如果我们假设这个环是有序的，而且在环上进行顺时针遍历就对应着存储地址的增长顺序，每个请求可以被顺时针遍历过程中所遇到的第一个节点所服务；也就是说，第一个在环上的地址比请求的地址大的服务器会服务这个请求。如果请求的地址比节点中的最大地址还大，那它会反过来被拥有最小地址的那个服务器服务，因为在这个环上的遍历是以循环的方式进行的。方法用下图进行了阐明：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*yhBejrSaatHa4b0gr96tvQ.png" alt></p><p>理论上，每个服务器‘拥有’哈希环（hashring）上的一段区间范围，任何映射到这个范围里的请求都将被同一个服务器服务。现在好了，如果其中一个服务器出现故障了怎么办，就以节点 3 为例吧，这个时候下一个服务器节点在环上的地址范围就会扩大，并且映射到这个范围的任何请求会被分派给新的服务器。仅此而已。只有对应到故障节点的区间范围内的哈希需要被重新分配，而哈希环上其余的部分和请求 - 服务器的分配仍然不会受到影响。这跟传统的哈希技术正好是相反的，在传统的哈希中，哈希表大小的变化会影响 <em>全部</em> 的映射。因为有了 <strong>一致性哈希</strong>，只有一部分（这跟环的分布因子有关）请求会受已知的哈希环变化的影响。（节点增加或者删除会导致环的变化，从而引起一些请求 - 服务器之间的映射发生改变。）</p><p><img src="https://cdn-images-1.medium.com/max/800/1*59Mn6sT0Wu7qQJmX1FOhtw.png" alt></p><h3 id="一种高效的实现方法"><a href="#一种高效的实现方法" class="headerlink" title="一种高效的实现方法"></a>一种高效的实现方法</h3><p>现在我们对什么是哈希环已经熟悉了…</p><p>我们需要实现以下内容来让它工作：</p><ol><li>一个从哈希空间到集群上所有服务器节点之间的映射，让我们能找到可以服务指定请求的节点。</li><li>一个集群上每个节点所服务的请求的集合。在后面，这个集合可以让我们找到哪些哈希因为节点的增加或者删除而受到了影响。</li></ol><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><p>要完成上述的第一个部分，我们需要以下内容：</p><ul><li>一个哈希函数，用来计算已知请求的标识（ID）在环上对应的位置。</li><li>一种方法，用来寻找转换为哈希值的请求标识所对应的节点。</li></ul><p>为了找到与特定请求相对应的节点，我们可以用一种简单的数据结构来阐释，它由以下内容组成：</p><ul><li>一个与环上的节点一一对应的哈希数组。</li><li>一张图（哈希表），用来寻找与已知请求相对应的服务器节点。</li></ul><p>这实际上就是一个有序图的原始表示。</p><p>为了能在以上数据结构中找到可以服务于已知哈希值的节点，我们需要：</p><ul><li>执行修改过的二分搜索，在数组中查找到第一个等于或者大于（≥）你要查询的哈希值所对应的节点 — 哈希映射。</li><li>查找在图中发现的节点 — 哈希映射所对应的那个节点。</li></ul><h4 id="节点的增加或者删除"><a href="#节点的增加或者删除" class="headerlink" title="节点的增加或者删除"></a>节点的增加或者删除</h4><p>在这篇文章的开头我们已经看到了，当一个节点被添加，哈希环上的一部分区间范围，以及它所包括的各种请求，都必须被分配到这个新节点。反过来，当一个节点被删除，过去被分配到这个节点的请求都将需要被其他节点处理。</p><h4 id="如何寻找到被哈希环的改变所影响的那些请求？"><a href="#如何寻找到被哈希环的改变所影响的那些请求？" class="headerlink" title="如何寻找到被哈希环的改变所影响的那些请求？"></a>如何寻找到被哈希环的改变所影响的那些请求？</h4><p>一种解决方法就是遍历分配到一个节点的所有请求。对每个请求，我们判断它是否处在环发生变化的区间范围内，如果有需要的话，把它转移到其他地方。</p><p>然而，这么做所需要的工作量会随着节点上请求数量的增加而增加。让情况变得更糟糕的是，随着节点数量的增加，环上发生变化的数量也可能会增加。最坏的情况是，由于环的变化通常与局部故障有关，与环变化相关联的瞬时负载也可能增加其他受影响节点发生故障的可能性，有可能导致整个系统发生级联故障。</p><p>考虑到这个因素，我们希望请求的重定位做到尽可能高效。最理想的情况是，我们可以将所有请求保存在一种数据结构里，这样我们能找到环上任何地方发生哈希变化时受到影响的请求。</p><h4 id="高效查找受影响的哈希值"><a href="#高效查找受影响的哈希值" class="headerlink" title="高效查找受影响的哈希值"></a>高效查找受影响的哈希值</h4><p>在集群上增加或者删除一个节点将改变环上一部分请求的分配，我们称之为 <strong>受影响范围</strong>（<strong>affected range</strong>）。如果我们知道受影响范围的边界，我们就可以把请求转移到正确的位置。</p><p>为了寻找受影响范围的边界，我们从增加或者删除掉的一个节点的哈希值 H 开始，从 H 开始绕着环向后移动（图中的逆时针方向），直到找到另外一个节点。让我们将这个节点的哈希值定义为 S（作为开始）。从这个节点开始逆时针方向上的请求会被指定给它（S），因此它们不会受到影响。</p><p><strong>注意：这只是实际将发生的情况的一个简化描述；在实践中，数据结构和算法都更加复杂，因为我们使用的复制因子（replication factors）数目大于 1，并且当任意给定的请求都只有一部分节点可用的情况下，我们还会使用专门的复制策略。</strong></p><p>那些哈希值在被找到的节点和增加（或者删除）的节点范围之间的请求就是需要被移动的。</p><h4 id="高效查找受影响范围内的请求"><a href="#高效查找受影响范围内的请求" class="headerlink" title="高效查找受影响范围内的请求"></a>高效查找受影响范围内的请求</h4><p>一种解决方法就是简单的遍历对应于一个节点的所有请求，并且更新那些哈希值映射到此范围内的请求。</p><p>在 JavaScript 中类似这样：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> request <span class="keyword">of</span> requests) &#123;</span><br><span class="line">  <span class="keyword">if</span> (contains(S, H, request.hash)) &#123;</span><br><span class="line">    <span class="comment">/* 这个请求受环变化的影响 */</span></span><br><span class="line">    request.relocate();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">contains</span>(<span class="params">lowerBound, upperBound, hash</span>) </span>&#123;</span><br><span class="line">   <span class="keyword">const</span> wrapsOver = upperBound &lt; lowerBound;</span><br><span class="line">   <span class="keyword">const</span> aboveLower = hash &gt;= lowerBound;</span><br><span class="line">   <span class="keyword">const</span> belowUpper = upperBound &gt;= hash;</span><br><span class="line">   <span class="keyword">if</span> (wrapsOver) &#123;</span><br><span class="line">     <span class="keyword">return</span> aboveLower || belowUpper;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">return</span> aboveLower &amp;&amp; belowUpper;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于哈希环是环状的，仅仅查找 S &lt;= r &lt; H 之间的请求是不够的，因为 S 可能比 H 大（表明这个区间范围包含了哈希环的最顶端的开始部分）。函数 <code>contains()</code> 可以处理这种情况。</p><p>只要请求数量相对较少，或者节点的增加或者删除的情况也相对较少出现，遍历一个给定节点的所有请求还是可行的。</p><p>然而，随着节点上的请求数量的增加，所需的工作量也随之增加，更糟糕的是，随着节点的增加，环变化也可能发生得更频繁，无论是因为在自动节点伸缩（automated scaling）或者是故障转换（failover）的情况下为了重新均衡访问请求而触发的整个系统上的并发负载。</p><p>最糟的情况是，与这些变化相关的负载可能增加其它节点发生故障的可能性，有可能导致整个系统范围的级联故障。</p><p>为了减轻这种影响，我们也可以将请求存储到类似于之前讨论过的一个单独的环状数据结构中，在这个环里，一个哈希值直接映射到这个哈希对应的请求。</p><p>这样我们就能通过以下步骤来定位受影响范围内的所有请求：</p><ul><li>定位从 S 开始的第一个请求。</li><li>顺时针遍历直到你找到了这个范围以外的一个哈希值。</li><li>重新定位落在这个范围之内的请求。</li></ul><p>当一个哈希更新时所需要遍历的请求数量平均是 R/N，R 是定位到这个节点范围内的请求数量，N 是环上哈希值的数量，这里我们假设请求是均匀分布的。</p><hr><p>让我们通过一个可工作的例子将以上解释付诸实践：</p><p>假设我们有一个包含节点 A 和 B 的集群。</p><p>让我们随机的产生每个节点的 ‘哈希分配’：（假设是32位的哈希），因此我们得到了</p><p><code>A:0x5e6058e5</code></p><p><code>B:0xa2d65c0</code></p><p>在此我们将节点放到一个虚拟的环上，数值 <code>0x0</code>、<code>0x1</code> 和 <code>0x2</code>… 是被连续放置到环上的直到 <code>0xffffffff</code>，就这样在环上绕一个圈后 <code>0xffffffff</code> 的后面正好跟着的就是 <code>0x0</code>。</p><p>由于节点 A 的哈希是 <code>0x5e6058e5</code>，它负责的就是从 <code>0xa2d65c0+1</code> 到 <code>0xffffffff</code>，以及从 <code>0x0</code> 到 <code>0x5e6058e5</code> 范围里的任何请求，如下图所示：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*inKL8q-CTZ6Asl_uSpDYew.png" alt></p><p>另一方面，B 负责的是从 <code>0x5e6058e5+1</code> 到 <code>0xa2d65c0</code> 的范围。如此，整个哈希空间都被划分了。</p><p>从节点到它们的哈希之间的映射在整个集群上是共享的，这样保证了每次环计算的结果总是一致的。因此，任何节点在需要服务请求的时候都可以判断请求放在哪里。</p><p>比如我们需要寻找 （或者创建）一个新的请求，这个请求的标识符是 ‘<a href="mailto:bobs.blog@example.com" target="_blank" rel="noopener">bobs.blog@example.com</a>’。</p><ol><li>我们计算这个标识的哈希 H ，比如得到的是 <code>0x89e04a0a</code></li><li>我们在环上寻找拥有比 H 大的哈希值的第一个节点。这里我们找到了 B。</li></ol><p>因此 B 是负责这个请求的节点。如果我们再次需要这个请求，我们将重复以上步骤并且又会得到同样的节点，它会包含我们需要的的状态。</p><p>这个例子是过于简单了。在实际情况中，只给每个节点一个哈希可能导致负载非常不均匀的分布。你可能已经注意到了，在这个例子中，B 负责环的 <code>(0xa2d656c0-0x5e6058e5)/232 = 26.7%</code>，同时 A 负责剩下的部分。理想的情况是，每个节点可以负责环上同等大小的一部分。</p><p>让分布更均衡合理的一种方法是为每个节点产生多个随机哈希，像下面这样：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*7qNhuMpoIWhatDWSOJaZVA.png" alt></p><p>事实上，我们发现这样做的结果照样令人不满意，因此我们将环分成 64 个同样大小的片段并且确保每个节点都会被放到每个片段中的某个位置；这个的细节就不是那么重要了。反正目的就是确保每个节点能负责环上同等大小的一部分，因此保证负载是均匀分布的。（为每个节点产生多个哈希的另一个优势就是哈希可以在环上逐渐的被增加或者删除，这样就避免了负载的突然间的变化。）</p><p>假设我们现在在环上增加一个新节点叫做 C，我们为 C 产生一个随机哈希值。</p><p><code>A:0x5e6058e5</code></p><p><code>B:0xa2d65c0</code></p><p><code>C:0xe12f751c</code></p><p>现在，<code>0xa2d65c0 + 1</code> 和 <code>0xe12f751c</code> （以前是属于A的部分）之间的环空间被分配给了 C。所有其他的请求像以前一样继续被哈希到同样的节点。为了处理节点职责的变化，这个范围内的已经分配给 A 的所有请求需要将它们的所有状态转移给 C。</p><p><img src="https://cdn-images-1.medium.com/max/800/1*9EH-yVTX8U9dxRdQ7pjK1Q.png" alt></p><p>现在你理解了为什么在分布式系统中均衡负载是需要哈希的。然而我们需要一致性哈希来确保在环发生任何变化的时候最小化集群上所需要的工作量。</p><p>另外，节点需要存在于环上的多个地方，这样可以从统计学的角度保证负载被均匀分布。每次环发生变化都遍历整个哈希环的效率是不高的，随着你的分布式系统的伸缩，有一种更高效的方法来决定什么发生了变化是很必要的，它能帮助你尽可能的最小化环变化带来的性能上的影响。我们需要新的索引和数据类型来解决这个问题。</p><hr><p>构建分布式系统是很难的事情。但是我们热爱它并且我们喜欢谈论它。如果你需要依靠一种分布式系统的话，选择 Ably。如果你想跟我们谈一谈的话，联系我们！</p><p>在此特别感谢 Ably 的分布式系统工程师 <a href="https://github.com/jdmnd" target="_blank" rel="noopener">John Diamond</a> 对本文的贡献。</p><hr><p><img src="https://cdn-images-1.medium.com/max/800/1*L6S-jVuznYcx4W1o4i9i9w.jpeg" alt></p><p>Srushtika 是 <a href="http://ably.io" target="_blank" rel="noopener">Ably Realtime</a>的软件开发顾问</p><p><img src="https://cdn-images-1.medium.com/max/800/1*g_I_lIRmw4_IODWKLJweqw.png" alt></p><p>感谢 <a href="https://medium.com/@john_91129?source=post_page" target="_blank" rel="noopener">John Diamond</a> 和 <a href="https://medium.com/@matt.at.ably?source=post_page" target="_blank" rel="noopener">Matthew O’Riordan</a>。</p><blockquote><p>如果发现译文存在错误或其他需要改进的地方，欢迎到 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 <strong>本文永久链接</strong> 即为本文在 GitHub 上的 MarkDown 链接。</p></blockquote><hr><blockquote><p><a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im" target="_blank" rel="noopener">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android" target="_blank" rel="noopener">Android</a>、<a href="https://github.com/xitu/gold-miner#ios" target="_blank" rel="noopener">iOS</a>、<a href="https://github.com/xitu/gold-miner#前端" target="_blank" rel="noopener">前端</a>、<a href="https://github.com/xitu/gold-miner#后端" target="_blank" rel="noopener">后端</a>、<a href="https://github.com/xitu/gold-miner#区块链" target="_blank" rel="noopener">区块链</a>、<a href="https://github.com/xitu/gold-miner#产品" target="_blank" rel="noopener">产品</a>、<a href="https://github.com/xitu/gold-miner#设计" target="_blank" rel="noopener">设计</a>、<a href="https://github.com/xitu/gold-miner#人工智能" target="_blank" rel="noopener">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi" target="_blank" rel="noopener">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi" target="_blank" rel="noopener">知乎专栏</a>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原文地址：&lt;a href=&quot;https://blog.ably.io/how-to-implement-consistent-hashing-efficiently-fe038d59fff2&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
      <category term="Algorithm" scheme="http://yqian1991.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Things I learnt from colleagues</title>
    <link href="http://yqian1991.github.io/Software-Development/Things-I-learnt-from-colleagues/"/>
    <id>http://yqian1991.github.io/Software-Development/Things-I-learnt-from-colleagues/</id>
    <published>2018-05-11T19:50:01.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>I always believe there are always things you can learn from your colleagues no matter what size of your company you are working for, what industry you are in, but a good company culture can help you grow faster.</p><h1 id="Days-in-OMSignal"><a href="#Days-in-OMSignal" class="headerlink" title="Days in OMSignal"></a>Days in OMSignal</h1><p>OMSignal is a wearable tech startup company, it is my first job after my graduation.<br>The thing I learnt from the first day is that ask, ask, ask. Everybody is busy and it’s not that people don’t want to tell you, without a systematic onboarding guide like a mature company does, no one really know what’s the right thing to tell you, so asking question is the best way to figure out the situation.</p><p>Be transparent, don’t hide problems, at the early days, I sometimes feel afraid to report problems since there are many, and I am afraid people will not be happy if I speak out without solving them by myself. Later I learnt that try to solve problems by yourself, but if you can’t, reporting early will avoid big problems in the future.</p><p>Another lesson I learnt is that besides getting work done, thinking about your career path, because you never know when the company doesn’t need you any more.</p><p>I left OMSignal half years later with a shock, I hope I can learn more before finding the next adventure, but this is how the world works, a company’s outlook decides whether they expanding or reduce.</p><h1 id="Days-at-SurveyMonkey"><a href="#Days-at-SurveyMonkey" class="headerlink" title="Days at SurveyMonkey"></a>Days at SurveyMonkey</h1><p>One month after I left OMSignal, I joined SurveyMonkey through alumni referral. I have so much to talk about, this is a great company that I learnt a lot, a lot.</p><p>This is not a company to the scale like Google, Facebook, Amazon, but the company is making a lot changes everyday to make it a great place to work. The onboarding experience is pretty good, you got everything you need to start. you can start contributing in one week, and at the same time, you will get a glance of how the company works. This is not a large company, but not small, it has more than 700 employees around the world.</p><p>Since I am talking about colleagues, let’s back to it. a great culture nourishes people, everyone is humble, friendly, even I am not good at speaking, but everyone is still like a good friend when you need them. This shapes my attitude in work. Be humble, be friendly, take the company as home.</p><p>Senior developers are really good mentors here, they help you understanding what your teams do, there is no dumb question here, but they helps you to not get confused with it next time, it helps you grow.</p><p>I have 1:1 meeting with my manager biweekly, they share experiences, guide you to the next level, you got a lot intuitions for your career path. I can still clearly think of the cascading path my manager guide me.<br>From designing a small system independently to participate in complex system work, from be active in a meeting to how to attract your audience. From making your code a product to increase your impact both on the tech side also team cooperation.</p><p>This is place that tolerant your fault, but it doesn’t mean people ignore it. they take it seriously with helping you get through it instead of blaming.</p><h1 id="Days-in-the-future"><a href="#Days-in-the-future" class="headerlink" title="Days in the future"></a>Days in the future</h1><p>What I learnt today will help me do better in the future. but I will remember to learning continuously.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I always believe there are always things you can learn from your colleagues no matter what size of your company you are working for, what
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Growth" scheme="http://yqian1991.github.io/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>Data pipeline with Airflow, Go serverless</title>
    <link href="http://yqian1991.github.io/System-Design/Data-pipeline-with-Airflow-Go-serverless/"/>
    <id>http://yqian1991.github.io/System-Design/Data-pipeline-with-Airflow-Go-serverless/</id>
    <published>2018-05-04T21:46:54.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>We had the Spring hackathon in the past two days at SurveyMonkey, my team designed and demoed a general data pipeline with Airflow to help testing and delivering some features quickly, and we are the runner up for the Best Platform Innovation prize. Glad this project got some recognitions.</p><h1 id="What-is-Airflow"><a href="#What-is-Airflow" class="headerlink" title="What is Airflow"></a>What is Airflow</h1><p>Apache Airflow is a workflow management tool, you can use it to easily create, schedule and monior your workflow, a common and popular use case is use it to host data pipeline.</p><p>This is a project originated from Airbnb then transfered to Apache and now still in incubating.</p><h2 id="DAGs"><a href="#DAGs" class="headerlink" title="DAGs"></a>DAGs</h2><p>Airflow uses DAGs(Directed Acyclic Graph) to organize workflow, basically a collection of tasks designed in certain way(sequential, branching, parallel), and those form a task graph. This means that you can design even very complex pipeline with DAGs.</p><h2 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h2><p>Every task(node in DAG) is an operator, airflow lib provides some basic operators that you can use, and you can also extend the base operators to create customized operators to fit your own requirements. Besides operators, Airflow also provides hooks to manage external connections such as database connection, message bus connection.</p><h2 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h2><p>Airflow plugins are plugins, they are reusable modules. There are some plugins already there, but since the project is still young, there are still a lot room to provide more powerful plugins.</p><h1 id="How-we-want-to-use-it"><a href="#How-we-want-to-use-it" class="headerlink" title="How we want to use it"></a>How we want to use it</h1><p>We start by designing a general data pipeline which has data collector, data processor and actions. our idea is that people in the company can has a easy way to access the data and play with the data and take actions fast.</p><p>Data Collector is a plugin we created that can connect to database, you just need to provide the table name or event table schema, you will be able to get all the data and pass on to next task.</p><p>Data Processor is the place you can mangling the data, experiment your ideas, get insights. it’s a simple python script, you just throw it into the data processor, it will executed by airflow scheduler.</p><p>Actions, there are many actions we can take, we make email and slack notification a plugin, based on your data analysis, you may want to take some actions. you just plug it to your data pipeline.</p><p>With this simple pipeline, we can tackle a lot use cases that has business values. One is that we use it to track the survey response rate for users, we can find low response rate users and recommending them tools and tips to improve the response rate.</p><p>We also designed another pipeline for automation tests which can integrate with Selenium, we can have a overview of web page navigation(we can call it user journey in other way) of our website from real user perspective, it can also help us find the slowness part on the site.</p><h2 id="The-goods-and-bads-of-micro-services"><a href="#The-goods-and-bads-of-micro-services" class="headerlink" title="The goods and bads of micro services"></a>The goods and bads of micro services</h2><p>The reason we want to utilize this data pipeline is that in a micro services architecture, we have a lot of pain doing some cross service works, for a very simple task, we need to touch a lot of services and even create a new services to do it, many releases, coordinations, operations work come in along the way.</p><p>Our micro services are design based on separation of responsibilities, each micro services provides a set of REST APIs, why we not just coding serverlessly, the API is already there and that is all what you need.</p><h1 id="Best-practices-in-other-companies"><a href="#Best-practices-in-other-companies" class="headerlink" title="Best practices in other companies"></a>Best practices in other companies</h1><p>More and more big companies are using Airflow, some key take aways I got:</p><ul><li>Have a good for running and monitoring airflow itself. it’s easy but it can be complicate, airflow itself is a service that you need to care about SLA.</li><li>Don’t put too complex pipelines on it, it’s just a project still in incubating.</li></ul><h1 id="Some-good-articles"><a href="#Some-good-articles" class="headerlink" title="Some good articles"></a>Some good articles</h1><p><a href="https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee" target="_blank" rel="noopener">https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee</a><br><a href="https://www.zillow.com/data-science/airflow-at-zillow/" target="_blank" rel="noopener">https://www.zillow.com/data-science/airflow-at-zillow/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We had the Spring hackathon in the past two days at SurveyMonkey, my team designed and demoed a general data pipeline with Airflow to hel
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Airflow" scheme="http://yqian1991.github.io/tags/Airflow/"/>
    
      <category term="Serverless" scheme="http://yqian1991.github.io/tags/Serverless/"/>
    
  </entry>
  
  <entry>
    <title>Git branching model for development</title>
    <link href="http://yqian1991.github.io/Software-Development/Git-branching-model-for-development/"/>
    <id>http://yqian1991.github.io/Software-Development/Git-branching-model-for-development/</id>
    <published>2018-05-02T01:58:23.000Z</published>
    <updated>2019-12-03T02:32:29.404Z</updated>
    
    <content type="html"><![CDATA[<p>A good branching model matters a lot for development efficiency, but same as many other topics, there isn’t a golden way that fits all.</p><h1 id="The-current-development-flow-I-am-using"><a href="#The-current-development-flow-I-am-using" class="headerlink" title="The current development flow I am using:"></a>The current development flow I am using:</h1><h2 id="Feature-Development"><a href="#Feature-Development" class="headerlink" title="Feature Development"></a>Feature Development</h2><p>We create a <code>feature branch</code> when we are working on a specific feature, we name the feature branch by the jira ticket name(we integrated Jira with Github, so the branch will show up on Jira tickets, e.g USER-100).<br>After feature completes and finished testing locally, we deploy the feature branch onto a test environment.</p><p>Test environment is controlled by developers, so we can do more integration tests instead of local, this is useful in micro services based architecture.</p><p>Once developer is pleased with the tests on test environment, A pull request is made and after code review by peer, the pull request is merged to develop branch.</p><h2 id="Release-candidate"><a href="#Release-candidate" class="headerlink" title="Release candidate"></a>Release candidate</h2><p>Everything on develop branch is ready for next release. we will deploy develop branch to staging environment, QA will run automation tests and gives green light for the release.</p><p>Then we create a pull request from develop to master, master always has the latest stable release.</p><p>We then merge the pull request to master, tag the release and deploy the release to production.</p><h2 id="Hotfix"><a href="#Hotfix" class="headerlink" title="Hotfix"></a>Hotfix</h2><p>hotfix is directly based on master, then merge back to master after release, we also need to create a pull request to merge it to develop to make develop branch up to date.</p><h1 id="Problems-and-improvements"><a href="#Problems-and-improvements" class="headerlink" title="Problems and improvements"></a>Problems and improvements</h1><p>Apparently, this approach is not robust and has some potential problems.</p><p>Since release candidates are always in develop and need to be merged to master, this will block other features merging to develop if the release takes long or there are many features working in parallel.</p><h2 id="Add-release-branch"><a href="#Add-release-branch" class="headerlink" title="Add release branch"></a>Add <code>release</code> branch</h2><p>So for a big project that with a lot of features, we can introduce another branch <code>release</code> branch, when the features merged back to develop and is planned for the next release, then create a <code>release</code> branch based on develop, so other features can still be merged to develop without affecting the upcoming release.</p><p>With <code>release</code> branch introduced, our flow works much like  <a href="http://nvie.com/posts/a-successful-git-branching-model/" target="_blank" rel="noopener">http://nvie.com/posts/a-successful-git-branching-model/</a>, this branching model was later be considered harmful, while in my opinion, it’s not really harmful, it’s just different development team needs a way best suits.</p><h2 id="Github-flow"><a href="#Github-flow" class="headerlink" title="Github flow"></a>Github flow</h2><p>Github flow is very straightforward, feature branches merge back to master and then master can be deployed to production, this is super efficient to integrate with CI/CD tools with no developer intervention.</p><h2 id="Gitlab-flow"><a href="#Gitlab-flow" class="headerlink" title="Gitlab flow"></a>Gitlab flow</h2><p>While this will not work for us, because our releases always rely on a configuration management system, that you need to add configuration changes first and then release your code.</p><p>In this case, a gitlab flow introduces a <code>production</code> branch, master branch becomes the release candidate or pre-release.</p><h2 id="Some-other-variations"><a href="#Some-other-variations" class="headerlink" title="Some other variations"></a>Some other variations</h2><p>Some teams do release based on cherry-pick, when feature branches merged to develop, then create a release branch based on master instead of develop, and then cherry pick all commits need to be released from develop to master. This will cause master and develop commit history diverged, and you will not be able to merge master back to develop soon.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A good branching model matters a lot for development efficiency, but same as many other topics, there isn’t a golden way that fits all.&lt;/
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Github" scheme="http://yqian1991.github.io/tags/Github/"/>
    
      <category term="CI/CD" scheme="http://yqian1991.github.io/tags/CI-CD/"/>
    
  </entry>
  
  <entry>
    <title>The poem of Python</title>
    <link href="http://yqian1991.github.io/Languages/The-poem-of-Python/"/>
    <id>http://yqian1991.github.io/Languages/The-poem-of-Python/</id>
    <published>2018-04-19T17:15:19.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import this</span><br><span class="line">The Zen <span class="keyword">of</span> Python, <span class="keyword">by</span> Tim Peters</span><br><span class="line"></span><br><span class="line">Beautiful <span class="keyword">is</span> better than ugly.</span><br><span class="line"><span class="keyword">Explicit</span> <span class="keyword">is</span> better than implicit.</span><br><span class="line">Simple <span class="keyword">is</span> better than complex.</span><br><span class="line">Complex <span class="keyword">is</span> better than complicated.</span><br><span class="line">Flat <span class="keyword">is</span> better than nested.</span><br><span class="line">Sparse <span class="keyword">is</span> better than dense.</span><br><span class="line">Readability counts.</span><br><span class="line">Special cases aren<span class="comment">'t special enough to break the rules.</span></span><br><span class="line">Although practicality beats purity.</span><br><span class="line">Errors should never pass silently.</span><br><span class="line">Unless explicitly silenced.</span><br><span class="line"><span class="keyword">In</span> the face <span class="keyword">of</span> ambiguity, refuse the temptation <span class="keyword">to</span> guess.</span><br><span class="line">There should be one-- <span class="keyword">and</span> preferably only one --obvious way <span class="keyword">to</span> <span class="keyword">do</span> it.</span><br><span class="line">Although that way may <span class="keyword">not</span> be obvious at first unless you<span class="comment">'re Dutch.</span></span><br><span class="line">Now <span class="keyword">is</span> better than never.</span><br><span class="line">Although never <span class="keyword">is</span> often better than *right* now.</span><br><span class="line"><span class="keyword">If</span> the implementation <span class="keyword">is</span> hard <span class="keyword">to</span> explain, it<span class="comment">'s a bad idea.</span></span><br><span class="line"><span class="keyword">If</span> the implementation <span class="keyword">is</span> easy <span class="keyword">to</span> explain, it may be a good idea.</span><br><span class="line">Namespaces are one honking great idea -- <span class="keyword">let</span><span class="comment">'s do more of those!</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight vbnet&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="Languages" scheme="http://yqian1991.github.io/categories/Languages/"/>
    
    
      <category term="Python" scheme="http://yqian1991.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>How to build a feed system</title>
    <link href="http://yqian1991.github.io/System-Design/how_to_build_a_feed_system/"/>
    <id>http://yqian1991.github.io/System-Design/how_to_build_a_feed_system/</id>
    <published>2018-04-17T03:05:31.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<p>I recently read some articles about designing feed system, the reason I am interested in it is because a feed system design involves a lot of knowledge in system design(NoSQL or relational, affinity ranking and scalability). This is also a popular question for technical interview.</p><a id="more"></a><p>Traditional design is pretty simple with one app sever and one database, while this is not scalable at all, maybe it works for small site, but what if a site grows large like Twitter and Facebook?</p><h2 id="How-to-store-feed-for-user"><a href="#How-to-store-feed-for-user" class="headerlink" title="How to store feed for user"></a>How to store feed for user</h2><p>A feed system is special, because usually you will need to show a feed for a user, then you need to fetch all feeds a user liked, commented, mentioned and so on. A good way is to implement a message box for each user  with redis, if the data growing vastly, we can further consider clustering with Cassandra</p><h2 id="Push-or-Pull"><a href="#Push-or-Pull" class="headerlink" title="Push or Pull"></a>Push or Pull</h2><p>When a user generated a post, do we push it to all of the user’s follower right away? or we only render it when one of the follower request to view the feed. This is a problem since celebrities’ posts always have a  large number of followers, which pushing takes a lot of time.</p><p>For users who have a large number(it’s always need a way to figure out how large is large) of followers, we should apply pull model, and apply push model for others.</p><h2 id="How-to-display-the-feed"><a href="#How-to-display-the-feed" class="headerlink" title="How to display the feed"></a>How to display the feed</h2><p>A general way is to display the feed by timestamp, but for a system that cares about user experiences, we also need to take the affinities between user and followers, and the type of post into consideration.</p><p>Basically, after we get the feed, we need an algorithm to rank the feeds, there are 3 features used here to ranking:</p><ul><li>Affinity score: For each feed, affinity score evaluates how close you are with this user. For example, you are more likely to care about feed from your close friends instead of someone you just met once.</li><li>Edge weight. Edge weight basically reflects importance of each edge. For instance, comments are worth more than likes.</li><li>Time decay. The older the story, the less likely users find it interesting.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><p><a href="http://blog.gainlo.co/index.php/2016/04/05/design-news-feed-system-part-2/" target="_blank" rel="noopener">http://blog.gainlo.co/index.php/2016/04/05/design-news-feed-system-part-2/</a><br><a href="https://thenewstack.io/building-scalable-news-feed-applications-using-redis-and-cassandra/" target="_blank" rel="noopener">https://thenewstack.io/building-scalable-news-feed-applications-using-redis-and-cassandra/</a><br><a href="https://www.slideshare.net/danmckinley/etsy-activity-feeds-architecture/" target="_blank" rel="noopener">https://www.slideshare.net/danmckinley/etsy-activity-feeds-architecture/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I recently read some articles about designing feed system, the reason I am interested in it is because a feed system design involves a lot of knowledge in system design(NoSQL or relational, affinity ranking and scalability). This is also a popular question for technical interview.&lt;/p&gt;
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Feed" scheme="http://yqian1991.github.io/tags/Feed/"/>
    
      <category term="database" scheme="http://yqian1991.github.io/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>有一個姑娘</title>
    <link href="http://yqian1991.github.io/Life/%E6%9C%89%E4%B8%80%E5%80%8B%E5%A7%91%E5%A8%98/"/>
    <id>http://yqian1991.github.io/Life/有一個姑娘/</id>
    <published>2016-09-02T02:47:35.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<p>我在太阳还没升起的地方等你<br>朝露 沾湿我衣<br>晨风 吹不乱我心</p><p>我在太阳还没升起的地方等你<br>尽管这里芳草依依<br>我只守望你的足迹</p><p>我在太阳还没升起的地方等你<br>他们都从我身邊呼嘯而去<br>只有在你身旁我才春风得意</p><p>我在太阳还没升起的地方等你<br>消耗了所有糧食 喝光了水<br>太阳来了又已西去</p><p>明天，<br>我还会在那里等你！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我在太阳还没升起的地方等你&lt;br&gt;朝露 沾湿我衣&lt;br&gt;晨风 吹不乱我心&lt;/p&gt;
&lt;p&gt;我在太阳还没升起的地方等你&lt;br&gt;尽管这里芳草依依&lt;br&gt;我只守望你的足迹&lt;/p&gt;
&lt;p&gt;我在太阳还没升起的地方等你&lt;br&gt;他们都从我身邊呼嘯而去&lt;br&gt;只有在你身旁我才春风得意&lt;/p&gt;
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="She" scheme="http://yqian1991.github.io/tags/She/"/>
    
  </entry>
  
  <entry>
    <title>Some numbers for back-of-the-envelope usage calculations</title>
    <link href="http://yqian1991.github.io/System-Design/Some-numbers-for-back-of-the-envelope-usage-calculations/"/>
    <id>http://yqian1991.github.io/System-Design/Some-numbers-for-back-of-the-envelope-usage-calculations/</id>
    <published>2016-05-12T01:02:41.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Some-quick-numbers"><a href="#Some-quick-numbers" class="headerlink" title="Some quick numbers:"></a>Some quick numbers:</h3><ul><li>Read sequentially from main memory:     4 GB/s   (1Mb needs 0.25 ms)</li><li>Read sequentially from SSD:             1 GB/s   (1Mb needs 1 ms)</li><li>Read sequentially from 1 Gbps Ethernet: 100 MB/s (1Mb needs 10 ms)</li><li>Read sequentially from disk:            30 MB/s  (1Mb needs 30 ms)</li></ul><p>main memory 4x &gt; SSD<br>SSD 10x &gt; 1 Gbps Ethernet<br>1 Gbps Ethernet 3 &gt; disk</p><p>more statistics:</p><ul><li>6-7 world-wide round trips per second</li><li>2,000 round trips per second within a data center</li><li>1 RPS = 2.5 million requests per month</li><li>40 RPS = 100 million requests per month</li><li>400 RPS = 1 billion requests per month</li></ul><h3 id="Here-is-a-detailed-table"><a href="#Here-is-a-detailed-table" class="headerlink" title="Here is a detailed table:"></a>Here is a detailed table:</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Latency Comparison Numbers</span><br><span class="line">----------------------------------------------------------------------------------------</span><br><span class="line">L1 cache reference                           <span class="number">0.5</span> ns</span><br><span class="line">Branch mispredict                            <span class="number">5</span>   ns</span><br><span class="line">L2 cache reference                           <span class="number">7</span>   ns                      <span class="number">14</span>x L1 cache</span><br><span class="line">Mutex lock/unlock                          <span class="number">100</span>   ns</span><br><span class="line">Main memory reference                      <span class="number">100</span>   ns                      <span class="number">20</span>x L2 cache, <span class="number">200</span>x L1 cache</span><br><span class="line">Compress <span class="number">1</span>K bytes with Zippy            <span class="number">10</span>,<span class="number">000</span>   ns       <span class="number">10</span> us</span><br><span class="line">Send <span class="number">1</span> KB bytes over <span class="number">1</span> Gbps network     <span class="number">10</span>,<span class="number">000</span>   ns       <span class="number">10</span> us</span><br><span class="line">Read <span class="number">4</span> KB randomly from SSD*           <span class="number">150</span>,<span class="number">000</span>   ns      <span class="number">150</span> us          ~<span class="number">1</span>GB/sec SSD</span><br><span class="line">Read <span class="number">1</span> MB sequentially from memory     <span class="number">250</span>,<span class="number">000</span>   ns      <span class="number">250</span> us</span><br><span class="line">Round trip within same datacenter      <span class="number">500</span>,<span class="number">000</span>   ns      <span class="number">500</span> us</span><br><span class="line">Read <span class="number">1</span> MB sequentially from SSD*     <span class="number">1</span>,<span class="number">000</span>,<span class="number">000</span>   ns    <span class="number">1</span>,<span class="number">000</span> us    <span class="number">1</span> ms  ~<span class="number">1</span>GB/sec SSD, <span class="number">4</span>X memory</span><br><span class="line">Disk seek                           <span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span>   ns   <span class="number">10</span>,<span class="number">000</span> us   <span class="number">10</span> ms  <span class="number">20</span>x datacenter roundtrip</span><br><span class="line">Read <span class="number">1</span> MB sequentially from <span class="number">1</span> Gbps  <span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span>   ns   <span class="number">10</span>,<span class="number">000</span> us   <span class="number">10</span> ms  <span class="number">40</span>x memory, <span class="number">10</span>X SSD</span><br><span class="line">Read <span class="number">1</span> MB sequentially from disk    <span class="number">30</span>,<span class="number">000</span>,<span class="number">000</span>   ns   <span class="number">30</span>,<span class="number">000</span> us   <span class="number">30</span> ms <span class="number">120</span>x memory, <span class="number">30</span>X SSD</span><br><span class="line">Send packet CA-&gt;Netherlands-&gt;CA    <span class="number">150</span>,<span class="number">000</span>,<span class="number">000</span>   ns  <span class="number">150</span>,<span class="number">000</span> us  <span class="number">150</span> ms</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">-----</span><br><span class="line"><span class="number">1</span> ns = <span class="number">10</span>^<span class="number">-9</span> seconds</span><br><span class="line"><span class="number">1</span> us = <span class="number">10</span>^<span class="number">-6</span> seconds = <span class="number">1</span>,<span class="number">000</span> ns</span><br><span class="line"><span class="number">1</span> ms = <span class="number">10</span>^<span class="number">-3</span> seconds = <span class="number">1</span>,<span class="number">000</span> us = <span class="number">1</span>,<span class="number">000</span>,<span class="number">000</span> ns</span><br></pre></td></tr></table></figure><h3 id="Powers-of-two-table"><a href="#Powers-of-two-table" class="headerlink" title="Powers of two table"></a>Powers of two table</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Power           Exact Value         Approx Value        Bytes</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line"><span class="number">7</span>                             <span class="number">128</span></span><br><span class="line"><span class="number">8</span>                             <span class="number">256</span></span><br><span class="line"><span class="number">10</span>                           <span class="number">1024</span>   <span class="number">1</span> thousand           <span class="number">1</span> KB</span><br><span class="line"><span class="number">16</span>                         <span class="number">65</span>,<span class="number">536</span>                       <span class="number">64</span> KB</span><br><span class="line"><span class="number">20</span>                      <span class="number">1</span>,<span class="number">048</span>,<span class="number">576</span>   <span class="number">1</span> million            <span class="number">1</span> MB</span><br><span class="line"><span class="number">30</span>                  <span class="number">1</span>,<span class="number">073</span>,<span class="number">741</span>,<span class="number">824</span>   <span class="number">1</span> billion            <span class="number">1</span> GB</span><br><span class="line"><span class="number">32</span>                  <span class="number">4</span>,<span class="number">294</span>,<span class="number">967</span>,<span class="number">296</span>                        <span class="number">4</span> GB</span><br><span class="line"><span class="number">40</span>              <span class="number">1</span>,<span class="number">099</span>,<span class="number">511</span>,<span class="number">627</span>,<span class="number">776</span>   <span class="number">1</span> trillion           <span class="number">1</span> TB</span><br></pre></td></tr></table></figure><h3 id="Example-Calculations"><a href="#Example-Calculations" class="headerlink" title="Example Calculations"></a>Example Calculations</h3><p>For example, if we are designing a financial transaction system, we need to calculate the data size of transactions:</p><p>Size per transaction:</p><ul><li>user_id - 8 bytes</li><li>created_at - 5 bytes</li><li>seller - 32 bytes</li><li>amount - 5 bytes<br>Total: ~50 bytes/transaction</li></ul><p>If the system has 5 billion transactions each month, the new transaction data size will be:<br>250 GB/month = 50 bytes * 5 billion /month, which will be<br>9 TB of new transaction content in 3 years</p><p>Assume most are new transactions instead of updates to existing ones, then we have:<br>2,000 transactions per second on average<br>200 read requests per second on average</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p><a href="https://github.com/donnemartin/system-design-primer" target="_blank" rel="noopener">https://github.com/donnemartin/system-design-primer</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Some-quick-numbers&quot;&gt;&lt;a href=&quot;#Some-quick-numbers&quot; class=&quot;headerlink&quot; title=&quot;Some quick numbers:&quot;&gt;&lt;/a&gt;Some quick numbers:&lt;/h3&gt;&lt;ul&gt;
&lt;l
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Python 2 and Python 3</title>
    <link href="http://yqian1991.github.io/Languages/Python-2-and-Python-3/"/>
    <id>http://yqian1991.github.io/Languages/Python-2-and-Python-3/</id>
    <published>2015-08-18T14:02:25.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Changes-in-Python-3-compared-to-Python2-7"><a href="#Changes-in-Python-3-compared-to-Python2-7" class="headerlink" title="Changes in Python 3 compared to Python2.7"></a>Changes in Python 3 compared to Python2.7</h3><h4 id="print-is-a-function-in-python-3"><a href="#print-is-a-function-in-python-3" class="headerlink" title="print is a function in python 3"></a><code>print</code> is a function in python 3</h4><p>  <code>print a</code> is no longer work in Python 3, always use <code>print(a)</code></p><h4 id="division-by-int-can-return-float-now"><a href="#division-by-int-can-return-float-now" class="headerlink" title="division by int can return float now"></a>division by int can return float now</h4><p>In python2.7:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="number">5</span>/<span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure></p><p>while in python3</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="number">5</span>/<span class="number">2</span></span><br><span class="line"><span class="number">2.5</span></span><br></pre></td></tr></table></figure><h4 id="Some-well-known-APIs-no-longer-return-lists-in-Python-3"><a href="#Some-well-known-APIs-no-longer-return-lists-in-Python-3" class="headerlink" title="Some well-known APIs no longer return lists in Python 3"></a>Some well-known APIs no longer return lists in Python 3</h4><ul><li><code>dict.iterkeys()</code>, <code>dict.iteritems()</code> and <code>dict.itervalues()</code> are no longer supported in python3</li><li><code>xrange</code> is replaced by range</li><li><code>zip()</code>, <code>map()</code>, <code>filter()</code> returns an iterator.</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; iter = zip([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; next(iter)</span><br><span class="line">(<span class="number">1</span>, <span class="string">'a'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; next(iter)</span><br><span class="line">(<span class="number">2</span>, <span class="string">'b'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; next(iter)</span><br><span class="line">(<span class="number">3</span>, <span class="string">'c'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; next(iter)</span><br><span class="line">Traceback (most recent call last)<span class="symbol">:</span></span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure><p>For <code>map()</code> and <code>filter()</code>, we can use <code>list(map())</code> to transform it to list.</p><h4 id="Ordering-function"><a href="#Ordering-function" class="headerlink" title="Ordering function"></a>Ordering function</h4><p>All the elements to be sorted must be comparable to each other<br><code>cmp</code> argument providing a comparison function is no longer supported</p><h4 id="All-strings-are-now-Unicode-by-default-in-python-3"><a href="#All-strings-are-now-Unicode-by-default-in-python-3" class="headerlink" title="All strings are now Unicode by default in python 3"></a>All strings are now Unicode by default in python 3</h4><h4 id="raw-input-is-abandoned-in-Python-3-and-use-input-instead"><a href="#raw-input-is-abandoned-in-Python-3-and-use-input-instead" class="headerlink" title="raw_input() is abandoned in Python 3 and use input() instead"></a>raw_input() is abandoned in Python 3 and use input() instead</h4><h4 id="Keyword-only-argument"><a href="#Keyword-only-argument" class="headerlink" title="Keyword only argument"></a>Keyword only argument</h4><p>In python 2<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(svy, question, delete_all=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> delete_all:</span><br><span class="line">        print(<span class="string">"Deleted ALL questions from survey!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Deleted single question from survey."</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>delete(svy, question1) <span class="comment"># Cool</span></span><br><span class="line">Deleted single question <span class="keyword">from</span> survey.</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>delete(svy, question1, question2) <span class="comment"># Not cool</span></span><br><span class="line">Deleted ALL questions <span class="keyword">from</span> survey!</span><br></pre></td></tr></table></figure></p><p>while in Python 3, we can define keyword only argument:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(svy, question, *, delete_all=False)</span></span><span class="symbol">:</span></span><br></pre></td></tr></table></figure><p>then <code>delete_all</code> becomes keyword only argument</p><h3 id="Metaclasses-in-Python3"><a href="#Metaclasses-in-Python3" class="headerlink" title="Metaclasses in Python3"></a>Metaclasses in Python3</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="class"><span class="keyword">class</span> <span class="title">FooBar</span>:</span></span><br><span class="line">        pass</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; type(FooBar)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">type</span>'&gt;</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; f = FooBar()</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; type(f)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">FooBar</span>'&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; isinstance(foo, Foobar)</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; isinstance(Foobar, <span class="built_in">type</span>)</span></span><br><span class="line">True</span><br></pre></td></tr></table></figure><p>These examples mean instance is an instance of a class, a class is an instance of metaclass. and <code>type</code> is a very useful metaclass in python.</p><ol><li>All classes and metaclasses including object are subclasses of object.</li><li>All classes and metaclasses including type are instances of type.</li><li>All objects including object are instances of object.</li></ol><p>we can use <code>type</code> to create a class like this:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; MyClass = type(<span class="string">'MyClass'</span>, (), &#123;&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; MyClass</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">MyClass</span>'&gt;</span></span><br></pre></td></tr></table></figure><p>As type is a metaclass, so we can also create a custom metaclass that extens type</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">Meta</span><span class="params">(type)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="Difference-between-and-xx-in-Python"><a href="#Difference-between-and-xx-in-Python" class="headerlink" title="Difference between _,  and xx__ in Python"></a>Difference between _, <strong> and </strong>xx__ in Python</h3><ul><li><p>One underscore in the beginning marks a private method or attribute</p></li><li><p>Methods with two underscores in the beginning can not be overrided.</p></li><li><p>Methods with two underscores in the beginning and in the end are magic methods which only python can call.</p></li></ul><p>Some useful methods are:<br><code>__call__, __new__, __init__, __prepare__</code></p><p>These four methods are very important for instance and class creation.</p><h3 id="Python-late-binding"><a href="#Python-late-binding" class="headerlink" title="Python late binding"></a>Python late binding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[m(<span class="number">2</span>) <span class="keyword">for</span> m <span class="keyword">in</span> (<span class="keyword">lambda</span>: [(<span class="keyword">lambda</span> x:i*x) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)])()]</span><br><span class="line">[<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><p>The values of variables used in closures are looked up at the time the inner function is called. So as a result, when any of the functions returned by multipliers() are called, the value of i is looked up in the surrounding scope at that time. By then, regardless of which of the returned functions is called, the for loop has completed and i is left with its final value of 3. Therefore, every returned function multiplies the value it is passed by 3, so since a value of 2 is passed in the above code, they all return a value of 6 (i.e., 3 x 2).</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://stackoverflow.com/questions/101268/hidden-features-of-python" target="_blank" rel="noopener">https://stackoverflow.com/questions/101268/hidden-features-of-python</a><br><a href="https://medium.com/instamojo-matters/become-a-pdb-power-user-e3fc4e2774b2#.1egkm87sm" target="_blank" rel="noopener">https://medium.com/instamojo-matters/become-a-pdb-power-user-e3fc4e2774b2#.1egkm87sm</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Changes-in-Python-3-compared-to-Python2-7&quot;&gt;&lt;a href=&quot;#Changes-in-Python-3-compared-to-Python2-7&quot; class=&quot;headerlink&quot; title=&quot;Changes in
      
    
    </summary>
    
      <category term="Languages" scheme="http://yqian1991.github.io/categories/Languages/"/>
    
    
      <category term="Python" scheme="http://yqian1991.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
