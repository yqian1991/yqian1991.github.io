<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu of Daphne</title>
  
  <subtitle>春秋笔法·丹枫嫩寒</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yqian1991.github.io/"/>
  <updated>2020-05-31T03:41:48.685Z</updated>
  <id>http://yqian1991.github.io/</id>
  
  <author>
    <name>Yu Qian</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark Structured Streaming Internal</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/</id>
    <published>2020-05-31T02:58:25.000Z</published>
    <updated>2020-05-31T03:41:48.685Z</updated>
    
    <content type="html"><![CDATA[<p>Spark structured streaming is implemented in spark sql module.</p><h1 id="Spark-Session"><a href="#Spark-Session" class="headerlink" title="Spark Session:"></a>Spark Session:</h1><p>sparkSession is the entry to Spark sql and structured streaming.</p><ul><li>Spark Context: spark context is the entry point of spark, so it’s naturally the bedrock of spark session as well.</li><li>readStream: Read streaming data in as a DataFrame, this method returns an DataStreamReader object</li><li>StreamingQueryManager: Managing the execution of all streaming quires</li><li>createDataFrame: Generate Data frames from various sources,(DataFrame=Dataset[Row])</li><li>createDataset:</li><li>sql: execute sql quires and return data as DataFrame</li></ul><h1 id="DataStreamReader-and-DataStreamWriter"><a href="#DataStreamReader-and-DataStreamWriter" class="headerlink" title="DataStreamReader and DataStreamWriter"></a>DataStreamReader and DataStreamWriter</h1><ul><li>DataStreamReader: Load streaming data from external sources</li><li>DataStreamWriter: Output streaming data to external sources<br>DataStreamWriter has a start() method which calls <code>df.sparkSession.sessionState.StreamingQueryManager.startQuery()</code> to start streaming query.<br>As you can see, the query is started through StreamingQueryManager.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">streamSource = spark \</span><br><span class="line">  .readStream \</span><br><span class="line">  .format(<span class="string">"kafka"</span>) \</span><br><span class="line">  .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="type">KAFKA_BROKER</span>) \</span><br><span class="line">  .option(<span class="string">"subscribe"</span>, <span class="type">KAFKA_TOPIC</span>) \</span><br><span class="line">  .option(<span class="string">"startingOffsets"</span>, <span class="string">"earliest"</span>) \</span><br><span class="line">  .option(<span class="string">"group.id"</span>,  <span class="type">CONSUMER_ID</span>) \</span><br><span class="line">  .load()   <span class="comment">// DataStreamReader</span></span><br><span class="line"></span><br><span class="line">output = streamSource \</span><br><span class="line">  .writeStream \</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>) \</span><br><span class="line">  .trigger(processingTime = trigger_interval) \</span><br><span class="line">  .foreach(sink) \</span><br><span class="line">  .start() <span class="comment">// DataStreamWriter</span></span><br></pre></td></tr></table></figure><h1 id="Dataset-API"><a href="#Dataset-API" class="headerlink" title="Dataset API"></a>Dataset API</h1><p>Dataset is a strong typed data structure used to do transformations and actions in structured streaming.</p><ul><li>logicalPlan: After transformations and actions defined on a Dataset, it will be analyzed to logical plan, then optimized to physical plan for final execution.</li></ul><p>Some features can be applied are:</p><ul><li>withWatermark: A watermark defines a point in time before which we assume no more late data is going to arrive. This is useful for late data in streaming situation.</li><li>checkpoint: Apply dataset checkpointing, either eagerly or non eagerly.</li><li><p>cache: Cache dataset to memory.</p><p>A lot transformations can be applied:</p><ul><li>groupBy:</li><li>groupByKey: This will return a KeyValueGroupedDataset</li><li>agg:</li><li>repartition/coalesce: Returns a new dataset by specified partitioning.</li></ul></li></ul><h1 id="StreamingQueryManager"><a href="#StreamingQueryManager" class="headerlink" title="StreamingQueryManager"></a>StreamingQueryManager</h1><p>Manages all the StreamingQuerys active in a <code>SparkSession</code>.</p><ul><li><code>startQuery</code> method: create query with DataFrame and call StreamingExecution start()</li></ul><h1 id="StreamingExecution"><a href="#StreamingExecution" class="headerlink" title="StreamingExecution"></a>StreamingExecution</h1><p>StreamingExecution is a implementation of trait StreamingQuery, it can be:</p><ul><li>ContinousExecution</li><li>MicrobatchExecution</li></ul><p>Some methods in StreamingExecution:</p><ul><li>start(): start a thread of queryEexcutionThread</li><li>queryExecutionThread: This thread will run method runStream()</li><li>runStream(): The method to materialize the streaming query</li><li>runActivatedStream: All implementations need to implement this method. working on the logical plan</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spark structured streaming is implemented in spark sql module.&lt;/p&gt;
&lt;h1 id=&quot;Spark-Session&quot;&gt;&lt;a href=&quot;#Spark-Session&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Understanding</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/</id>
    <published>2020-05-19T20:12:02.000Z</published>
    <updated>2020-05-19T20:35:10.300Z</updated>
    
    <content type="html"><![CDATA[<p>When a streaming context starts, it will start a Job Scheduler:</p><ul><li>Start ReceiverTracker: receive data from source and use BlockManager to save it to memoryStore(BlockManager)</li><li>Start JobGenerator: Periodically check interval, create Jobs based on graph given timestamp and OutputStream</li><li>Other resources: like job listener etc</li></ul><p>Each scheduler is run in eventLoop</p><p>For a streaming application, it usually contains:</p><ul><li>An InputStream: Some examples are SocketInputStream, KafkaInputStream, FileInputStream etc</li><li>Transformations: functions that generate another stream, example functions can be filter, map etc</li><li>OutputStream: Output actions of the streaming application, this will trigger stream to be materialized</li></ul><p>Streaming Context includes a property <code>graph</code> which Input Stream and output stream are registered on.</p><p>How is data read from memoryStore?</p><ul><li>Every DStream has methods to generateJob, getOrCompute, then compute (load from memory)</li></ul><p>A simple class/entity relationship:</p><p>SparkStreamingContext:</p><ul><li>sparkContext: sparkContext can be run by calling runJob method which invoke DagScheduler</li><li>graph: A list of DStream which includes output stream and input stream.</li><li>JobScheduler: See below</li></ul><p>JobScheduler: eventLoop</p><ul><li>JobGenerator: generate jobs based on <code>ssc.graph</code> at <code>batchDuration</code> interval, then submit jobs to JobExecutor</li><li>JobExecutor: execute submitted jobs from JobGenerator, job executed by calling sparkContext runJob func.</li><li>receiverTracker:</li></ul><p>JobGenerator(jobScheduler): eventLoop</p><ul><li>RecurringTimer: interval is batchDuration, trigger <code>GenerateJob</code> action</li><li>generateJobs: generate jobs for each outputStream in <code>ssc.graph</code></li><li>Submit Jobs to jobScheduler</li></ul><p>DStream(ssc):</p><ul><li>foreachRDD: register as outputStream</li><li>register: add stream to outputStream that used to generateJobs()</li><li>generateJob,: call compute method to get RDDs and create Job object</li><li>getOrCompute/compute: get RDDs, either from parent or from memory if it’s InputDStream</li><li>generatedRDDs:</li></ul><p>SparkContext</p><ul><li>DagScheduler</li></ul><p>DagScheduler: eventLoop</p><ul><li>Create stages of a RDD based on RDD dependencies</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source: <span class="type">DStream</span></span><br><span class="line"><span class="keyword">val</span> transformed: <span class="type">DStream</span> = source.transform() <span class="comment">// transformed will store dependency source</span></span><br><span class="line"><span class="keyword">val</span> output = transformed.foreachRDD(println) <span class="comment">// outputStream</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;When a streaming context starts, it will start a Job Scheduler:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start ReceiverTracker: receive data from source and use Bloc
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>The way to async I/O</title>
    <link href="http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/"/>
    <id>http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/</id>
    <published>2020-04-15T22:02:39.000Z</published>
    <updated>2020-04-16T03:02:40.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Concurrency-vs-Parallelism"><a href="#Concurrency-vs-Parallelism" class="headerlink" title="Concurrency vs Parallelism"></a>Concurrency vs Parallelism</h1><ul><li><p>Concurrency: Do multiple tasks at the same time.</p></li><li><p>Parallelism: Do one task at a time, but the task can be splitted to multiple sub tasks which can be executed in parallel.</p></li></ul><h1 id="CPU-intensive-vs-I-O-intensive"><a href="#CPU-intensive-vs-I-O-intensive" class="headerlink" title="CPU intensive vs I/O intensive"></a>CPU intensive vs I/O intensive</h1><p>If your program is not interacting with disks, media, devices, network and peripheries, then it is CPU intensive, otherwise it is I/O intensive.</p><p>The mode really effects the performance of your program. For example, if you want to know how many RPS my program can handle:</p><ul><li><p>For CPU intensive: (Number of Cores) / time_to_complete_a_request_in_seconds</p></li><li><p>For I/O intensive: (RAM / worker memory) / time_to_complete_a_request_in_seconds</p><p>In I/O intensive scenarios, CPU is doing nothing, so the performance is limited by how many workers are running, thus memory related.</p></li></ul><h1 id="Blocking-vs-Non-blocking"><a href="#Blocking-vs-Non-blocking" class="headerlink" title="Blocking vs Non-blocking"></a>Blocking vs Non-blocking</h1><p>When a program spending most of the time dealing with I/O and not doing anything else, then it will be blocked by the I/O operations, thus CPU stay there and do nothing.</p><p>In order to reuse CPU during waiting, we need to make it non blocking.</p><p>Essentially, it means instead of waiting, it periodically checking the status of I/O operation, only back to handle it if it finishes, otherwise, allow the system to do other tasks.</p><h1 id="Synchronous-vs-asynchronous"><a href="#Synchronous-vs-asynchronous" class="headerlink" title="Synchronous vs asynchronous"></a>Synchronous vs asynchronous</h1><p>Blocking and synchronous are almost the same, a thread focuses on doing one task, no distractions.</p><p>But the difference between non-blocking and asynchronous sometimes is hard to understand. They can be the same in many ways, especially when you don’t deep dive into it.</p><p>If we understand it at thread level, asynchronous means task can be delegated to a different thread, responses can be communicated by other ways like event driven or an callback mechanism, whereas, non-blocking, the thread needs to periodically checking result until task finished.</p><h1 id="Different-models"><a href="#Different-models" class="headerlink" title="Different models"></a>Different models</h1><p>You can design a program as:</p><ul><li><p>synchronous, non-blocking I/O: Since we still need to periodically checking task status in the same thread, and concurrency can only be achieved by spawning more threads, thus more overhead doing context switch.</p></li><li><p>asynchronous, non-blocking I/O: This is preferred by modern web servers. E.g you can achieve this by one thread using event loop. Some real life examples are Python Twisted, Java Netty.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Concurrency-vs-Parallelism&quot;&gt;&lt;a href=&quot;#Concurrency-vs-Parallelism&quot; class=&quot;headerlink&quot; title=&quot;Concurrency vs Parallelism&quot;&gt;&lt;/a&gt;Concurre
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Async" scheme="http://yqian1991.github.io/tags/Async/"/>
    
  </entry>
  
  <entry>
    <title>Canary Deployment with k8s ingress-controller</title>
    <link href="http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/"/>
    <id>http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/</id>
    <published>2020-03-03T01:48:52.000Z</published>
    <updated>2020-03-03T02:02:00.156Z</updated>
    
    <content type="html"><![CDATA[<p>Canary deployment is useful in many scenarios, like A/B testing.<br>k8s ingress-controller makes canary deployment easy, I will introduce the steps of it using the example of a microservice we have.</p><p>This document will not cover how to set up ELB. It will focus only on how to create a canary deployment along side an existing deployment.</p><h2 id="Situation"><a href="#Situation" class="headerlink" title="Situation"></a>Situation</h2><p>We have a service A running in k8s cluster, there is an external host name setup to expose the API to public, Requests sent through this API will be route to this service,</p><p>Here is the public host name setup:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl <span class="builtin-name">get</span> ingress -n ingress</span><br><span class="line">NAME                   HOSTS              <span class="built_in"> ADDRESS </span>  PORTS   AGE</span><br><span class="line">service-a          public.domain.com             80      297d</span><br></pre></td></tr></table></figure></p><p>and it also has k8s service set up:</p><p>So far, this is running happily on production,<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n emaildelivery</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">5</span>          <span class="number">159</span>d</span><br></pre></td></tr></table></figure></p><h2 id="Canary-Deployment"><a href="#Canary-Deployment" class="headerlink" title="Canary Deployment"></a>Canary Deployment</h2><h3 id="Deploy-new-pods-with-correct-annotation"><a href="#Deploy-new-pods-with-correct-annotation" class="headerlink" title="Deploy new pods with correct annotation"></a>Deploy new pods with correct annotation</h3><p>You will need to deploy new pods for sure, without overwriting current pods, you can do this by deploying new pods with a new application name or using a different namespace.</p><p>I want to reuse the same namespace, so I changed the application name to <code>service-a-canary</code>.</p><p>Then comes to the fun part, Keep all the necessary labels as <code>service-a</code> in the resources. Resources you will need to change include <code>service.yaml</code>, <code>deployment.yaml</code>, <code>ingress.yaml</code>.</p><p>Key changes are as follows:</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  namespace:</span> service-a</span><br><span class="line"><span class="symbol">  name:</span> service-a-canary</span><br><span class="line"><span class="symbol">  labels:</span></span><br><span class="line"><span class="symbol">    name:</span> service-a</span><br><span class="line">...</span><br><span class="line"><span class="symbol">selector:</span></span><br><span class="line"><span class="symbol">    matchLabels:</span></span><br><span class="line"><span class="symbol">      app:</span> service-a</span><br></pre></td></tr></table></figure><p>If you release the new pods at this time point, new pods will running fine, but serving no traffic:</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n service-a</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">159</span>d</span><br><span class="line">service-a-canary<span class="number">-6</span>f7784fc7-wg7kz   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d20h</span><br></pre></td></tr></table></figure><h3 id="Enable-canary-with-ingress-controller"><a href="#Enable-canary-with-ingress-controller" class="headerlink" title="Enable canary with ingress-controller"></a>Enable canary with ingress-controller</h3><p>To make canary deployment serving real traffic, simply adding few lines in ingress.yaml</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary: <span class="string">"true"</span></span><br><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary-weight: <span class="string">"10"</span></span><br></pre></td></tr></table></figure><p>nginx.ingress.kubernetes.io/canary-weight is the percentage of traffic that will be routed to canary deployment.</p><p>NOW, you can check your logs to see if traffic routed to canary deployment.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Canary deployment is useful in many scenarios, like A/B testing.&lt;br&gt;k8s ingress-controller makes canary deployment easy, I will introduce
      
    
    </summary>
    
      <category term="SRE" scheme="http://yqian1991.github.io/categories/SRE/"/>
    
    
      <category term="k8s" scheme="http://yqian1991.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Distributed OLAP Datastore</title>
    <link href="http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/"/>
    <id>http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/</id>
    <published>2020-02-20T03:32:16.000Z</published>
    <updated>2020-03-28T14:30:29.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Apache-Pinot"><a href="#Apache-Pinot" class="headerlink" title="Apache Pinot"></a>Apache Pinot</h1><p>Pinot is a realtime distributed OLAP datastore for scalable real time analytics with low latency.</p><p>It’s using Apache Helix for cluster management, data stored as segment in Pinot.</p><h2 id="Components"><a href="#Components" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Manage brokers and servers, responsible for assigning segments to servers.</p><h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>Accepting queries from clients and return query results to clients.</p><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>Hosts one or more segments, and respond to queries from broker.<br>Data is stored as segment in servers. Segment is a columnar storage.</p><h2 id="Data-Ingestion"><a href="#Data-Ingestion" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Data can be ingested in real time or offline ingestion mode</p><h3 id="Real-time-ingestion"><a href="#Real-time-ingestion" class="headerlink" title="Real time ingestion"></a>Real time ingestion</h3><p>Data from Kafka or other streaming source can be ingested to Pinot servers directly, and can serve query right away.</p><h3 id="Offline-ingestion"><a href="#Offline-ingestion" class="headerlink" title="Offline ingestion"></a>Offline ingestion</h3><p>Data in storage can be ingested through Pinot controller, and pinot controller will assign segments to Pinot servers.</p><ul><li>Add Schema</li><li>Add Table</li><li>Create Segment</li><li>Upload Segment</li></ul><h2 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h2><p>Pinot query language is very similar to standard query language except that <code>JOIN</code> and <code>LIMIT</code> are not supported.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT &lt;outputColumn&gt; (, outputColumn, outputColumn,<span class="built_in">..</span>.)</span><br><span class="line">  <span class="keyword">FROM</span> &lt;tableName&gt;</span><br><span class="line">  (WHERE <span class="built_in">..</span>. |<span class="built_in"> GROUP </span>BY <span class="built_in">..</span>. | ORDER BY <span class="built_in">..</span>. | TOP <span class="built_in">..</span>. | LIMIT <span class="built_in">..</span>.)</span><br></pre></td></tr></table></figure><h3 id="Indexing-technology"><a href="#Indexing-technology" class="headerlink" title="Indexing technology"></a>Indexing technology</h3><h1 id="Apache-Druid"><a href="#Apache-Druid" class="headerlink" title="Apache Druid"></a>Apache Druid</h1><p>Druid is very similar to Pinot in many ways: both for real time queries, both support real time and offline ingestions. Instead of Helix, Druid uses Apache Zookeeper for coordination.</p><h2 id="Components-1"><a href="#Components-1" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Master-Server-Coordinator-and-overlord-processes"><a href="#Master-Server-Coordinator-and-overlord-processes" class="headerlink" title="Master Server(Coordinator and overlord processes)"></a>Master Server(Coordinator and overlord processes)</h3><p>Manages data availability and ingestion, similar to Pinot controller.</p><h3 id="Query-Server-Broker-and-Router"><a href="#Query-Server-Broker-and-Router" class="headerlink" title="Query Server(Broker and Router)"></a>Query Server(Broker and Router)</h3><p>Same as Pinot, accepting queries from external clients, routing queries to brokers, coordinators and overlords.</p><h3 id="Data-Server-Historical-and-Middle-Manager-processes"><a href="#Data-Server-Historical-and-Middle-Manager-processes" class="headerlink" title="Data Server(Historical and Middle Manager processes)"></a>Data Server(Historical and Middle Manager processes)</h3><p>This is similar to server in Pinot, handles ingestion workloads and stores all queryable data.<br>Druid also provides a Deep Storage component as backup of data.</p><p>Data is stored as segment in Druid as well, but Druid segment always comes with a timestamp.</p><p>Druid supports tiering which allows old data can be moved to clusters with more disk storage but less memory and CPU, This can improve query efficiency.</p><h2 id="Data-Ingestion-1"><a href="#Data-Ingestion-1" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Druid also support batch and real time ingestion.</p><h2 id="Query-1"><a href="#Query-1" class="headerlink" title="Query"></a>Query</h2><p>Druid’s native query language is JSON over HTTP, beside this, Druid also provides Druid SQL.</p><h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><h1 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h1><p>Presto was designed for OLAP to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. But unlike Pinot and Druid, Presto is used to connect and query from external data sources, varies from HDFS to Cassandra and traditional database like MySQL.</p><h2 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h2><p>Parsing statements, planning queries, and managing Presto worker nodes.</p><h2 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h2><p>Executing tasks and processing data, Worker nodes fetch data from connectors and exchange intermediate data with each other.</p><h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Since Presto has no its own data storage, it relies on different kind of connectors to get data.<br>Data is then modeled as Catalog, schema and table in Presto.</p><h2 id="Query-Execution"><a href="#Query-Execution" class="headerlink" title="Query Execution"></a>Query Execution</h2><p>Statement -&gt; Queries -&gt; Stages -&gt; Tasks</p><h1 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h1><h1 id="Extra-Reads"><a href="#Extra-Reads" class="headerlink" title="Extra Reads"></a>Extra Reads</h1><p><a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7" target="_blank" rel="noopener">https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7</a></p><p><a href="https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e" target="_blank" rel="noopener">https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Apache-Pinot&quot;&gt;&lt;a href=&quot;#Apache-Pinot&quot; class=&quot;headerlink&quot; title=&quot;Apache Pinot&quot;&gt;&lt;/a&gt;Apache Pinot&lt;/h1&gt;&lt;p&gt;Pinot is a realtime distribute
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>100 Questions About Cassandra</title>
    <link href="http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/"/>
    <id>http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/</id>
    <published>2020-01-11T17:41:47.000Z</published>
    <updated>2020-01-16T04:06:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fast-facts-about-Cassandra"><a href="#Fast-facts-about-Cassandra" class="headerlink" title="Fast facts about Cassandra"></a>Fast facts about Cassandra</h2><ul><li>A Columnar based fault tolerant NoSQL database</li><li>An AP system (Sacrifice Consistency for Available and Partition)</li><li>Easy to scale horizontally (No master)</li><li>No join or subquery for aggregation</li></ul><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h2><h3 id="What-is-Cassandra’s-Replication-Strategy"><a href="#What-is-Cassandra’s-Replication-Strategy" class="headerlink" title="What is Cassandra’s Replication Strategy?"></a>What is Cassandra’s Replication Strategy?</h3><p>Replication strategies define the technique how the replicas are placed in a cluster.<br>There are mainly two types of Replication Strategy:</p><ul><li>Simple strategy: For single data center</li><li>Network Topology Strategy: For multi-datacenter</li></ul><h3 id="What-is-Cassandra-Consistency-Level"><a href="#What-is-Cassandra-Consistency-Level" class="headerlink" title="What is Cassandra Consistency Level?"></a>What is Cassandra Consistency Level?</h3><p>The minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful</p><ul><li>Write Consistency: ALL, ANY, ONE, EACH_QUORUM, LOCAL_ONE, LOCAL_QUORUM</li><li>Read Consistency: ALL, ONE, TWO, THREE, QUORUM, LOCAL_ONE, LOCAL_QUORUM</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quorum = (sum_of_replication_factors / 2) + 1</span><br></pre></td></tr></table></figure><p><a href="https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html" target="_blank" rel="noopener">https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html</a></p><h3 id="What-is-Cassandra’s-compaction-strategy"><a href="#What-is-Cassandra’s-compaction-strategy" class="headerlink" title="What is Cassandra’s compaction strategy?"></a>What is Cassandra’s compaction strategy?</h3><p>To improve read performance as well as to utilize disk space, Cassandra periodically does compaction to create &amp; use new consolidated SSTable files instead of multiple old SSTables.</p><ul><li>SizeTieredCompactionStrategy: for write-intensive workloads</li><li>LeveledCompactionStrategy: read-intensive workloads</li></ul><h3 id="Partition-key-and-clustering-key"><a href="#Partition-key-and-clustering-key" class="headerlink" title="Partition key and clustering key"></a>Partition key and clustering key</h3><p>Partition key is similar to primary key in relational databases, it decides which node to store the record.</p><p>Clustering key is responsible for sorting data within a partition</p><h4 id="Compound-key"><a href="#Compound-key" class="headerlink" title="Compound key"></a>Compound key</h4><p>Compound key are partition keys with multiple columns, but only the first column is considered as partition key and the rest are clustering keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY (p1, c1, c2, c3)</span><br></pre></td></tr></table></figure><h4 id="Composite-key"><a href="#Composite-key" class="headerlink" title="Composite key"></a>Composite key</h4><p>Composite keys are partition keys that consist of multiple columns.<br>But when you do query, you will need to include all partition keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY ((p1, p2), c1, c2)</span><br></pre></td></tr></table></figure><h3 id="What-are-some-of-Cassandra’s-limitations"><a href="#What-are-some-of-Cassandra’s-limitations" class="headerlink" title="What are some of Cassandra’s limitations?"></a>What are some of Cassandra’s limitations?</h3><ul><li>A single column value is recommended to &lt;= 1 Mb (max is 2Gb)</li><li>Number of rows within a partition is better to below 100,000 items and disk size under 100 Mb</li></ul><h3 id="How-does-Cassandra-use-bloom-filters"><a href="#How-does-Cassandra-use-bloom-filters" class="headerlink" title="How does Cassandra use bloom filters?"></a>How does Cassandra use bloom filters?</h3><p>Cassandra uses bloom filters to check if a partition key exists in any of the SSTables or not, without actually having to read their contents.</p><p>Each SSTable has a bloom filter, bloom filter will be updated when a memtable is flushed to disk.</p><h3 id="What-are-seed-node-in-Cassandra-cluster-setup"><a href="#What-are-seed-node-in-Cassandra-cluster-setup" class="headerlink" title="What are seed node in Cassandra cluster setup?"></a>What are seed node in Cassandra cluster setup?</h3><p>Seeds are used during startup to discover the cluster. Seeds are also referred by new nodes on bootstrap to learn other nodes in ring. When you add a new node to ring, you need to specify at least one live seed to contact. Once a node join the ring, it learns about the other nodes, so it doesn’t need seed on subsequent boot.</p><h3 id="How-to-add-a-new-node-to-a-single-datacenter-cluster"><a href="#How-to-add-a-new-node-to-a-single-datacenter-cluster" class="headerlink" title="How to add a new node to a single datacenter cluster?"></a>How to add a new node to a single datacenter cluster?</h3><ul><li>Calculate tokens for the new node, below is the script to generate tokens with Murmur3Partitioner.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">"print [str(((2**64 / number_of_tokens) * i) - 2**63) for i in range(number_of_tokens)]"</span></span><br></pre></td></tr></table></figure><ul><li><p>Install Cassandra on the node with proper <code>cassandra.yaml</code></p></li><li><p>Use <code>nodetool move</code> to assign new token for it.</p></li><li><p>Use <code>nodetool cleanup</code> to remove keys that no longer belong to the previously existing nodes.</p></li></ul><p><a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html" target="_blank" rel="noopener">https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fast-facts-about-Cassandra&quot;&gt;&lt;a href=&quot;#Fast-facts-about-Cassandra&quot; class=&quot;headerlink&quot; title=&quot;Fast facts about Cassandra&quot;&gt;&lt;/a&gt;Fast fac
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Cassandra" scheme="http://yqian1991.github.io/tags/Cassandra/"/>
    
      <category term="Database" scheme="http://yqian1991.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>我们的2019</title>
    <link href="http://yqian1991.github.io/Life/%E6%88%91%E4%BB%AC%E7%9A%842019/"/>
    <id>http://yqian1991.github.io/Life/我们的2019/</id>
    <published>2020-01-04T18:28:28.000Z</published>
    <updated>2020-01-11T19:09:54.815Z</updated>
    
    <content type="html"><![CDATA[<p>Feb: We travelled to LA and Daphne watched Jay Chou concert,<br>     I then participated company hackathon in San Mateo, got first prize (received a GoPro)<br>     I passed my G license</p><p>March: I moved to Toronto from Ottawa and started remote working for 4 months till June.<br>       We had our first car.</p><p>April:</p><p>May: We shoot pre-wedding photograph in Toronto.</p><p>June 9: We hold a small wedding ceremony with friends (happy to receive a lot of gifts)</p><p>July: I started a new job in downtown Toronto</p><p>Aug: Daphne’s parents in Canada</p><p>Sep: We spent a long weekend in Algonquin lodge (fishing, boating, hiking etc)</p><p>Oct: We hold Wedding ceremony in China and spent one day in Guangzhou, meet Daphne’s friend</p><p>Oct - Dec: Daphne did a lot Yoga session,  we also went badminton every Friday night.</p><p>Dec: Daphne had a Mont Tremblant - Montreal travel with friends, which I should be there too,<br>     but I went back China because of family emergency.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Feb: We travelled to LA and Daphne watched Jay Chou concert,&lt;br&gt;     I then participated company hackathon in San Mateo, got first prize 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="We" scheme="http://yqian1991.github.io/tags/We/"/>
    
  </entry>
  
  <entry>
    <title>Best practices to use Apache Spark</title>
    <link href="http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/"/>
    <id>http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/</id>
    <published>2019-12-01T16:23:03.000Z</published>
    <updated>2019-12-01T17:51:32.939Z</updated>
    
    <content type="html"><![CDATA[<p>Learning notes from DataBricks talks</p><h2 id="Optimizing-File-Loading-And-Partition-Discovery"><a href="#Optimizing-File-Loading-And-Partition-Discovery" class="headerlink" title="Optimizing File Loading And Partition Discovery"></a>Optimizing File Loading And Partition Discovery</h2><p>Data loading is the first step of spark application, when dataset becomes large, data loading time becomes an issue.</p><ul><li>Use Datasource tables</li></ul><p>With tables, partition metadata and schema is managed by Hive, based on which partition pruning can be done at logical planning stage. You can also use Spark SQL API directly when loading from a table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.table(<span class="string">"../path"</span>)</span><br><span class="line"></span><br><span class="line">spark.write.partitionBy(<span class="string">"date"</span>).saveAsTable(<span class="string">"table"</span>)</span><br></pre></td></tr></table></figure><ul><li>Specify <code>basePath</code> if loading from external files directly(e.g CSV or Json files)</li></ul><p>When loading from a file, partition discovery is done for each DataFrame creation<br>, also spark needs to infer schema from files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"inferSchema"</span>, true).load(file)</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(knownSchema).load(file)</span><br></pre></td></tr></table></figure><h2 id="Optimizing-File-Storage-and-Layout"><a href="#Optimizing-File-Storage-and-Layout" class="headerlink" title="Optimizing File Storage and Layout"></a>Optimizing File Storage and Layout</h2><ul><li><p>Prefer columnar over text for analytical queries</p></li><li><p>Compression with splittable storage format</p></li><li><p>Avoid large GZIP files</p></li><li><p>Partitioning and bucketing</p></li></ul><p>Parquet + Snappy is a good candidate</p><h2 id="Optimizing-Queries"><a href="#Optimizing-Queries" class="headerlink" title="Optimizing Queries"></a>Optimizing Queries</h2><ul><li>Tuning spark sql shuffle partitions<br><code>spark.sql.shuffle.partitions</code> is used in shuffle operations like groupBy, repartition, join and window. the default value is <code>200</code>.</li></ul><p>This is hard to tune because the parameter is applied on the whole job, which many operations maybe taken, tuning for certain operations may do no good to others.</p><ul><li><p>Adaptive Execution(&gt; Spark 2.0)<br><code>spark.sql.adaptive.enabled</code><br><code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>: default is 64Mb</p></li><li><p>Understanding Unions</p></li><li><p>Data Skipping Index</p></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.youtube.com/watch?v=iwQel6JHMpA" target="_blank" rel="noopener">Youtube Talks</a><br><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/" target="_blank" rel="noopener">Understanding Spark Source Code</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Learning notes from DataBricks talks&lt;/p&gt;
&lt;h2 id=&quot;Optimizing-File-Loading-And-Partition-Discovery&quot;&gt;&lt;a href=&quot;#Optimizing-File-Loading-And-P
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Starting a new journey</title>
    <link href="http://yqian1991.github.io/Life/Starting-a-new-journey/"/>
    <id>http://yqian1991.github.io/Life/Starting-a-new-journey/</id>
    <published>2019-07-01T18:36:56.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to say goodbye.</p><p>Many colleagues were wondering why I leave when it’s only half year to my 4 years anniversary. This is one of the reasons that makes Survey Monkey great. You will get extra 4 weeks vacation every 4 years. The company cares about employees life a lot and try to balance our work and life. Besides the generous take 4 policy, they provide free breakfast and lunch, support you to attend conference every year, allow working from home if you want, monthly bonus points(can redeem Amazon gift cards) etc. just name a few.</p><p>It was not easy to say goodbye to such amazing benefits, leaving an amazing team is also risky. Although all my team members except me located in Bay area, I never feel I am alone, daily stand up, biweekly 1:1 with manager, sprint planning meetings every two weeks and a lot slack calls for discuss or pair programming, we have established a solid way to do remote communication. because of 3 hours time difference, they try their best to no bother me after 6:00pm (EST), and I try my best to cover incidents or issues happen in the morning (before 12:00pm PST), which proved to work very well.</p><p>I was impressed the warm words the team delivered to me upon hearing my news, they even prepared a thank you card and gift to me which I was really touched. I was even questioning myself: is leaving  good?.</p><p>Nobody know a decision is good or not until it proved to be. we experienced a lot good things in the past, nice people, wonderful place, but as life moving on and us growing up, everything has an end. Leaving is exactly a graduation for me. SurveyMonkey is indeed a university for me, where I focused on technology but also took a lot ‘courses’ in cooperation, leading, communication, understanding. And just like 6 years ago, and almost the same time when I graduated from University. maybe this is also a time for my graduation as an intermediate software engineer.</p><p>Graduation marks the starting of a new journey, a new school, in the new journey, I will be a curious student to learn, be a good team player to contribute, be a better man.</p><p>Last but not least, I will be a good husband to support my family. Thanks to my wife for accompanying me since 2016, the journey is meaningless without you.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="Growth" scheme="http://yqian1991.github.io/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>写给Daphne的诗</title>
    <link href="http://yqian1991.github.io/Life/%E5%86%99%E7%BB%99Daphne%E7%9A%84%E8%AF%97/"/>
    <id>http://yqian1991.github.io/Life/写给Daphne的诗/</id>
    <published>2019-05-18T12:36:20.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<pre><code>第一章： 萌芽</code></pre><p>你要问我，我们的故事从哪儿开始，</p><p>走出考场的那一刻，我以为将是故事的结局</p><p>而微信上的只言片语，难道只是我一如既往的淡定？</p><p>也许大家都羡慕一见钟情，</p><p>可比一见钟情更浪漫的，是一聊倾心</p><pre><code>第二章：启 城</code></pre><p>即便我有一双翅膀，我也会将它折断</p><p>因为唾手可得的，到头来也可能只是冷面</p><p>而纵览八百里路云和月，我却恋上你的眼</p><p>如果那双翅膀还在，我必将它修复，</p><p>因为与你的日夜，多想跨过无尽的高山大海</p><p>未来的春夏秋冬，我们可以走得更远</p><p>夜阑人静，午夜梦回，</p><p>你就是我的那双翅膀</p><p>尽管摇摇晃晃， 跌跌撞撞</p><p>但是回头是喜悦，前路是希望</p><pre><code>第三章：誓言</code></pre><p>我记得你每一滴掉下的眼泪，每一次撅起的小嘴</p><p>我记得你激动地辩驳，又原谅的叹息</p><p>相知诚不如相爱那么容易，但是多难都要在一起。</p><p>我记得，你所有有味道的陪伴，邪性的范儿</p><p>我记得，你魔舞的腰枝，磅礴的胃气</p><p>从此，我也会记得，稀饭要稠，汤要浓</p><p>誓言无声，贵在默默坚守</p><p>我心永恒，坚信细水长流</p><p>从此，让我们的爱情澎湃，人生隽永</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;第一章： 萌芽
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;你要问我，我们的故事从哪儿开始，&lt;/p&gt;
&lt;p&gt;走出考场的那一刻，我以为将是故事的结局&lt;/p&gt;
&lt;p&gt;而微信上的只言片语，难道只是我一如既往的淡定？&lt;/p&gt;
&lt;p&gt;也许大家都羡慕一见钟情，&lt;/p&gt;
&lt;p&gt;可比一
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="She" scheme="http://yqian1991.github.io/tags/She/"/>
    
  </entry>
  
  <entry>
    <title>Fun topics in distributed system</title>
    <link href="http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/"/>
    <id>http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/</id>
    <published>2019-04-26T19:54:05.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one after one.</p><ul><li>Micro services architecture</li><li>CDN, Caching, load balancer</li><li>Event Sourcing</li><li>Messaging</li><li>CAP<br>…</li></ul><p>Needless to say, each of these topics are complex enough and takes years to learn. What we expect is: when we are tasked with a system design problem, we will be able to have a robust architecture overall.</p><p>Well, this is important, but when after you see tons of architectures in different use cases, still there is no one fit all solution. There are always tough challenges come up. and usually, the issue that stops you is not the high level design, it’s some small problems instead.</p><ul><li>Sticky Session</li></ul><p>Now we have a lot machines behind the load balancer can serve the traffic, that means requests from a same user can be served by different nodes, hmm, do I need to sign in again every time if the node doesn’t have my session info?</p><p>Absolutely not necessary, now you may think of sticky session which routes the requests for a particular session to the same physical machine that serviced the first request for that session. But problem is not totally solved at this point yet, this method may still cause uneven loads to service which actually we want to solve by distributed system.</p><p><a href="http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/" target="_blank" rel="noopener">http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/</a></p><ul><li>Distributed transactions</li></ul><p>When we throw out a nice distributed architecture, we are confident that it supports high throughputs with decent performance, and you also adds on that, it can handle traffic bursts by adding more machines since it’s a distributed oriented. This looks nice, <em>but how do you handle failures in payment transactions?</em></p><p>I will stuck here for a while, yes, this is not easy, in one machine environment, we can locking and waiting, but if the system is distributed, how can we make sure the transaction ACID?.</p><p>I am not a innovator, but curiosity drives me to find answers with two phase commits and PAXOS. There are tons of materials about those, just giving some I read:<br><a href="https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/" target="_blank" rel="noopener">https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/</a></p><ul><li>Distributed Tracing.<br>It’s common that a user request may cause cascading services calls in the backend, in case of data analysis or debugging, how can we stitch requests to the same origin?</li></ul><p>Using same RequestID is a common strategy, but usually, you will need another service/lib dedicated for distributed tracing. There are some popular frameworks for it, like opentracing.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Distributed system is good, but also introduces more problems you need to solve.<br>Generate a fancy system architecture is easy, what’s more important is how to solve the problems it brings.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Companies (Toronto)</title>
    <link href="http://yqian1991.github.io/Careers/Hidden-Companies/"/>
    <id>http://yqian1991.github.io/Careers/Hidden-Companies/</id>
    <published>2019-04-20T15:32:04.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.</p><p>But there is still a ton of jobs outside those popular sites. And also, most of time, we are interested in opportunities in our local area.</p><p>I did find a lot hidden companies that not easy to be aware of(based in Toronto). Usually due to the following reasons:</p><ul><li><p>They don’t have physical offices in Toronto, but support remote working:<br>Stripe<br>Github<br>DataDog<br>Zapier</p></li><li><p>They are actually good ones/Unicorns, but they only run a tiny team in Toronto, thus less advertised and less presence:<br>Etsy<br>Snap: Through acquisition of Bitmoji<br>Okta<br>iHerb<br>Pivotal (CloudFoundry)<br>Instacart: This one is getting bigger and bigger in Toronto.</p></li></ul><p>Many companies entered Toronto tech scene through acquiring local startups</p><ul><li>They are emerging startups, but some of them will become popular.<br>Dessa<br>Element AI</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.&lt;/p&gt;
&lt;p&gt;But there is still a ton of jobs out
      
    
    </summary>
    
      <category term="Careers" scheme="http://yqian1991.github.io/categories/Careers/"/>
    
    
      <category term="Job seeking" scheme="http://yqian1991.github.io/tags/Job-seeking/"/>
    
  </entry>
  
  <entry>
    <title>NLP in big companies</title>
    <link href="http://yqian1991.github.io/System-Design/NLP-in-big-companies/"/>
    <id>http://yqian1991.github.io/System-Design/NLP-in-big-companies/</id>
    <published>2019-04-19T17:06:13.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out how some other companies build their platforms.</p><h1 id="Uber"><a href="#Uber" class="headerlink" title="Uber"></a>Uber</h1><h2 id="NLP-platform-to-process-customer-support-tickets"><a href="#NLP-platform-to-process-customer-support-tickets" class="headerlink" title="NLP platform to process customer support tickets"></a>NLP platform to process customer support tickets</h2><p>In practice, we may not need very fancy algorithms and some simple ones just work, at least it’s a good starting point of building an engineering product.<br>Uber tried to use NLP to process customer support tickets with a classification model(logistic regression), Data processing is always the key task before machine learning, like how to encoding ticket and transform text, category to numerical vectors, apparently word2vec can be used here.</p><p>In the future, more complex and high performance algorithms an deep learning frameworks can be adopted like WordCNN.</p><p>In terms of the foundation platform, Uber utilizes Spark + hive for big data processing and scaled prediction.</p><p><a href="https://eng.uber.com/nlp-deep-learning-uber-maps/" target="_blank" rel="noopener">https://eng.uber.com/nlp-deep-learning-uber-maps/</a></p><h2 id="Uber-one-click-chat"><a href="#Uber-one-click-chat" class="headerlink" title="Uber one click chat"></a>Uber one click chat</h2><p>This is a smart reply system that auto-reply to user messages.</p><p>From this system, we can have a sense that a typical machine learning platform usually has two components:</p><ul><li><p>Offline training:<br>Using NLP and ML pipelines to do intent detection. Here is where NLP models got applied like Doc2Vec model</p></li><li><p>Online serving:<br>A message will be encoded as fixed-length vector representation via the pre-trained Doc2vec model, after which the vector and the intent detection classifier will be used to predict the message’s possible intent.</p></li></ul><p>The system then retrieve the most relevant replies based on the detected intent and surface them to the driver-partner receiving the message</p><p><a href="https://eng.uber.com/one-click-chat/" target="_blank" rel="noopener">https://eng.uber.com/one-click-chat/</a></p><h1 id="Airbnb"><a href="#Airbnb" class="headerlink" title="Airbnb"></a>Airbnb</h1><p>Airbnb built an online risk mitigation system which it mentioned some requirements of a machine learning platform:</p><ul><li>Fast</li><li>Robust</li><li>Scale</li></ul><p>Although it used an open source framework(OpenScoring) for it, we should know the typical pipeline is the same most of the times like what we saw in Airbnb platforms above. Feel free to check out some characteristics of OpenScoring.</p><p><a href="https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60</a><br><a href="https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d</a></p><h1 id="Zendesk"><a href="#Zendesk" class="headerlink" title="Zendesk"></a>Zendesk</h1><p>Zendesk summarizes customer support tickets to topics.</p><p>A big take-way is how they support 50k models on a daily basis with AWS Batch,<br>AWS batch supports auto scaling and job management, it also provides GPU support.<br>In terms of job management, this is a hot topic and demand need for building large scale platforms. A lot products emerges in this regards, like Airflow.</p><p><a href="https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9" target="_blank" rel="noopener">https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9</a></p><h1 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h1><p>Cortex</p><h1 id="LinkedIn"><a href="#LinkedIn" class="headerlink" title="LinkedIn"></a>LinkedIn</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out ho
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Natural Language Processing 101</title>
    <link href="http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/"/>
    <id>http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/</id>
    <published>2019-04-17T14:41:53.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.</p><h1 id="What-is-Natural-Language-Processing"><a href="#What-is-Natural-Language-Processing" class="headerlink" title="What is Natural Language Processing?"></a>What is Natural Language Processing?</h1><p>Natural Language Processing (NLP) is an important sub category in Artificial Intelligence that enabling computers to understand and process human languages, it tries to get computers closer to a human-level understanding of language.</p><h2 id="Some-research-topics-in-NLP"><a href="#Some-research-topics-in-NLP" class="headerlink" title="Some research topics in NLP"></a>Some research topics in NLP</h2><ul><li>Information Retrieval/Extraction/Filtering</li><li>Machine Translation</li><li>Document/Topic Classification/Summarization</li><li>Question Answering</li><li>Text Mining</li><li>Sentiment Analysis</li><li>Speech Recognition</li><li>Machine Writing/Content Genetation</li></ul><h1 id="Statistical-Language-Models"><a href="#Statistical-Language-Models" class="headerlink" title="Statistical Language Models"></a>Statistical Language Models</h1><p>This is to compute the probability of a sentence or sequence of words.</p><h2 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h2><p>N-gram is a popular statistical language model.<br>After building a model, we usually use cross-entropy and perplexity to evaluate the model.<br>Lower perplexities correspond to higher likelihoods, so lower scores are better on this<br>metric.</p><p>A major concern in language modeling is to avoid the situation <code>p(w) = 0</code>, which could arise as a result of a single unseen n-gram, the solution is using smoothing methods, some smoothing methods includes:</p><ul><li>Add-One(Laplace) smoothing</li><li>Good-Turing smoothing</li><li>Kneser-Ney smoothing</li><li>Witten-Bell smoothing</li></ul><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>A sentence/document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, POS tag, semantics and syntax etc, are all discarded.</p><h2 id="Probabilistic-Graphical-Models"><a href="#Probabilistic-Graphical-Models" class="headerlink" title="Probabilistic Graphical Models"></a>Probabilistic Graphical Models</h2><p>This is an important math theory/algorithm used in NLP tasks.</p><ul><li>Bayesian Network</li><li>Markov Network</li><li>Condition Random Fields</li><li>Hidden Markov Models</li><li>Estimation Maximization</li><li>Max Entropy</li></ul><h2 id="Topic-Model"><a href="#Topic-Model" class="headerlink" title="Topic Model"></a>Topic Model</h2><ul><li>Latent Dirichlet Allocation (LDA): Based on probabilistic graphical models</li><li>LSA: Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra</li><li>NMF: Non-Negative Matrix Factorization – Based on Linear Algebra</li></ul><h1 id="Some-popular-tasks-in-NLP"><a href="#Some-popular-tasks-in-NLP" class="headerlink" title="Some popular tasks in NLP"></a>Some popular tasks in NLP</h1><p>These are some tasks that may not be the solution to any particular NLP problem but are done as pre-requisites to simplify a lot of different problems in NLP. These are pretty much like reading comprehension we learn in school.</p><h2 id="Parts-of-Speech-Tagging"><a href="#Parts-of-Speech-Tagging" class="headerlink" title="Parts of Speech Tagging"></a>Parts of Speech Tagging</h2><p>Identify Proper nouns, Common nouns, Verbs, Adjectives, Preposition etc.</p><h2 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h2><p>Identify name of people, location etc.</p><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><h2 id="Morphosyntactic-Attributes"><a href="#Morphosyntactic-Attributes" class="headerlink" title="Morphosyntactic Attributes"></a>Morphosyntactic Attributes</h2><h1 id="Deep-Learning-in-NLP"><a href="#Deep-Learning-in-NLP" class="headerlink" title="Deep Learning in NLP"></a>Deep Learning in NLP</h1><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Previously, there are some other popular distributed representation of word as vectors, like Tf-Idf.<br> But they are sparse and long which is not computing efficient. Word2Vec instead is a dense vector representation of words(commonly 100-500 dimensions). and it models the meaning of a word as an embedding.</p><p>But how to get the dense vectors? Singular value    decomposition(Latent Semantic    Analysis) can be used, but a more successful way is through neural network inspired learning strategy.</p><ul><li>CBOW: Predict center/target word based on context words</li><li>Skip-grams: Predict context words based on center/target word.</li></ul><p>Other vector based models include: fastText, Doc2Vec, GloVe etc.</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>Stay tuned…</p><p>Highly recommend <a href="https://people.cs.umass.edu/~miyyer/cs585/" target="_blank" rel="noopener">https://people.cs.umass.edu/~miyyer/cs585/</a> as 101 course for NLP.<br>More advanced courses:<br><a href="https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6" target="_blank" rel="noopener">https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.&lt;/p&gt;
&lt;h1 
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Searching with bloom filter</title>
    <link href="http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/"/>
    <id>http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/</id>
    <published>2018-10-16T21:09:45.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>Our platform is sending 4 million emails per day, and many of them contains a lot user generated content which has potential risk of spam. A very important action we need to take is: if we know the sender is already marked as a bad email, we should stop sending emails for it right away.</p><p>We now have more than ten thousands of blacklisted emails stored in database, when sending an email, we need to check against this big list to see if we can send the current email out. retrieving all emails from db is not realistic since it has significant latency.</p><h1 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h1><h2 id="Let-DB-do-the-search"><a href="#Let-DB-do-the-search" class="headerlink" title="Let DB do the search"></a>Let DB do the search</h2><p>This is simple to implement and fit the current situation well since all emails are stored in database, we just need a stored procedure to take an email as input and return True if the email is in the table, otherwise false.</p><h2 id="Bloom-filter"><a href="#Bloom-filter" class="headerlink" title="Bloom filter"></a>Bloom filter</h2><p>Thinking out of the box, store with database itself may not be a good solution considering latency of DB read and write. Bloom filter is an excellent algorithm for this kind of large scale search.</p><p>The algorithm is very ideal, but when comes to engineering, we still need to consider the trade-offs based on the current service architecture.</p><ul><li>The current black listed emails are stored in database, and the write operation is done by a different service, we need to consider do we need to change the storage for bloom filter</li><li><p>bloom filter needs tuning to get low false positive rates. considering the growth rate of black listed emails, how do we adjust the size of hash functions and bit array lengths</p><p>Here is a useful tool to do the math: <a href="https://hur.st/bloomfilter/" target="_blank" rel="noopener">https://hur.st/bloomfilter/</a></p></li></ul><h1 id="My-take-aways"><a href="#My-take-aways" class="headerlink" title="My take aways"></a>My take aways</h1><ul><li>Thinking out of the box can bring you more brilliant ideas</li><li>Sometimes, excellent algorithm is not always fit</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Problem-statement&quot;&gt;&lt;a href=&quot;#Problem-statement&quot; class=&quot;headerlink&quot; title=&quot;Problem statement&quot;&gt;&lt;/a&gt;Problem statement&lt;/h1&gt;&lt;p&gt;Our platfo
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Algorithm" scheme="http://yqian1991.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Compare streaming frameworks</title>
    <link href="http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/"/>
    <id>http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/</id>
    <published>2018-10-16T20:06:49.000Z</published>
    <updated>2019-12-03T02:25:50.445Z</updated>
    
    <content type="html"><![CDATA[<p>The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is said that master/slave is not good words now). We use spark streaming to read messages from Kafka and rollup metrics, then send to RabbitMQ which will be used to send out notifications.</p><p>There are 20 million questions answered each day on our platform, to better keep customers engaged and provide them better insights, it’s nice to send notifications to the survey owners about the responses they collected, this needs to be happen in a timely manner, so Apache Spark seems to be a good fit.</p><p>Spark is so popular at that time, it is the only streaming framework I know, and I even thought that is the only streaming framework ever exists. Later on, I heard more and more voices that spark is not a real streaming tool, instead a batching process. Now I understand that Apache spark is for batch processing and spark streaming is for mini-batch processing. With the arising of LAMBDA framework, spark becomes popular since it can do both batch and “streaming” process.</p><p>There are some other streaming frameworks in the market, Apache Storm, Apache Samza, Apache Flink etc. It is easy to get lost when facing too many choices, it is also true that each of them has suitable use cases. what’s important is to understand the problem we are going to solve.</p><p>Here are some facts of those frameworks:</p><h1 id="Apache-Storm"><a href="#Apache-Storm" class="headerlink" title="Apache Storm"></a>Apache Storm</h1><p>Open sourced by Twitter and was initially released in 2011.</p><p>Key concepts:</p><ul><li>Spouts: (data source)</li><li>Bolts: functions</li><li>Tuple: stream</li></ul><p>Storm is able to process +1M msgs/second/node, A cluster can be configured to have &gt; 1k workers, Storm provides at least once delivery</p><p>On top of its core, Storm also provides Trident API for micro batch processing,<br>it assures exactly once guarantee.</p><p>With Trident API, you can do aggregation, merge, join, grouping, functions, filters etc.</p><p>Storm supports data transfer protocol: Avro, thrift.</p><h1 id="Apache-Flink"><a href="#Apache-Flink" class="headerlink" title="Apache Flink"></a>Apache Flink</h1><p>Flink is originated from an academic project called <code>Stratosphere</code>, it had the first release after transferring to Apache incubator at 2014.</p><p>Flink unified stream and batch processing, streaming is the nature of flink, and batch is regarded as a special case of streaming which in contrast of Spark.</p><p>Flink provides exactly once processing.</p><p>It has rich higher level API compared to Storm, also support out of order process</p><p>It is able to process 1.5M msgs/second/node</p><h1 id="Apache-Spark"><a href="#Apache-Spark" class="headerlink" title="Apache Spark"></a>Apache Spark</h1><p>Originally developed at the University of California, Berkeley’s AMPLab in 2009 and open sourced in 2010.</p><p>Spark can do exactly once delivery as well, it is working with RDD, but now suggested to use DataFrame/DataSet.</p><p>Spark is also known as hard to tune those complicate parameters for better performance.</p><h1 id="Apache-Samza"><a href="#Apache-Samza" class="headerlink" title="Apache Samza"></a>Apache Samza</h1><p>The project entered Apache Incubator in 2013 and was originally created at LinkedIn.</p><p>Samza achieves at least once delivery.</p><p>Samza is famous for its state management based on RockDB, but it’s tightly coupled with Kafka and Yarn.</p><p>It lacks some advanced features like watermarks, sessions.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is 
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Notes on data science self learning</title>
    <link href="http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/"/>
    <id>http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/</id>
    <published>2018-08-15T14:54:58.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from two people:</p><ul><li><a href="https://www.coxy1989.com/curriculum.html" target="_blank" rel="noopener">https://www.coxy1989.com/curriculum.html</a></li><li><a href="http://karlrosaen.com/ml/" target="_blank" rel="noopener">http://karlrosaen.com/ml/</a>.</li></ul><p>Thanks a lot to them for sharing their own experiences.</p><p>This blog is a guideline that I will continue update. Side by side, I may post learning logs with details on what I learned.</p><h1 id="Stage-1-Foundations"><a href="#Stage-1-Foundations" class="headerlink" title="Stage 1 - Foundations"></a>Stage 1 - Foundations</h1><p>This is to lay a good foundation for later machine learning, which includes:</p><h2 id="Probability-and-Statistics"><a href="#Probability-and-Statistics" class="headerlink" title="Probability and Statistics"></a>Probability and Statistics</h2><p>Bloomberg ML EDU course which I am following at this moment: <a href="https://bloomberg.github.io/foml/#about" target="_blank" rel="noopener">https://bloomberg.github.io/foml/#about</a><br>More resources on statistics: <a href="https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html" target="_blank" rel="noopener">https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html</a><br>Problems and solutions: <a href="http://karlrosaen.com/ml/hw/" target="_blank" rel="noopener">http://karlrosaen.com/ml/hw/</a></p><p>Some supporting materials:</p><ul><li>Mathematics background check: <a href="https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf" target="_blank" rel="noopener">https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf</a></li><li>Simple statistics crib-sheet: <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf" target="_blank" rel="noopener">http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf</a></li></ul><h2 id="Machine-learning-101"><a href="#Machine-learning-101" class="headerlink" title="Machine learning 101"></a>Machine learning 101</h2><p>Now you can have a glance of what machine learning is, and use your mathematics learned to understand concepts and practice on easy tasks.</p><p>Machine learning by Andrew Ng: <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a><br>Python Machine Learning: <a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition" target="_blank" rel="noopener">https://github.com/rasbt/python-machine-learning-book-2nd-edition</a></p><p>There are some good podcasts:</p><ul><li>Talking machine podcasts:<a href="https://www.thetalkingmachines.com/" target="_blank" rel="noopener">https://www.thetalkingmachines.com/</a></li><li>Becoming a data scientist podcasts: <a href="https://www.becomingadatascientist.com/category/podcast/" target="_blank" rel="noopener">https://www.becomingadatascientist.com/category/podcast/</a></li><li>The Master Algorithm: <a href="https://player.fm/series/data-skeptic/the-master-algorithm" target="_blank" rel="noopener">https://player.fm/series/data-skeptic/the-master-algorithm</a></li></ul><p>To complete this stage, 5 months are recommended.</p><h1 id="Stage-2-Applied"><a href="#Stage-2-Applied" class="headerlink" title="Stage 2 - Applied"></a>Stage 2 - Applied</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>[译] Airflow: 一个工作流程管理平台</title>
    <link href="http://yqian1991.github.io/System-Design/%E8%AF%91-Airflow-%E4%B8%80%E4%B8%AA%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0/"/>
    <id>http://yqian1991.github.io/System-Design/译-Airflow-一个工作流程管理平台/</id>
    <published>2018-07-28T02:28:02.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>原文地址：<a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" target="_blank" rel="noopener">Airflow: a workflow management platform</a></li><li>原文作者：<a href="https://medium.com/@maximebeauchemin" target="_blank" rel="noopener">Maxime Beauchemin</a></li><li>译文出自：<a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a></li><li>本文永久链接：<a href="https://github.com/xitu/gold-miner/blob/master/TODO1/airflow-a-workflow-management-platform.md" target="_blank" rel="noopener">https://github.com/xitu/gold-miner/blob/master/TODO1/airflow-a-workflow-management-platform.md</a></li><li>译者：<a href="https://github.com/yqian1991" target="_blank" rel="noopener">yqian1991</a></li><li>校对者：<a href="https://github.com/Park-ma" target="_blank" rel="noopener">Park-ma</a> <a href="https://github.com/DerekDick" target="_blank" rel="noopener">DerekDick</a></li></ul></blockquote><h1 id="Airflow-一个工作流程管理平台"><a href="#Airflow-一个工作流程管理平台" class="headerlink" title="Airflow: 一个工作流程管理平台"></a>Airflow: 一个工作流程管理平台</h1><p>出自 <a href="https://medium.com/@maximebeauchemin" target="_blank" rel="noopener">Maxime Beauchemin</a></p><p><img src="https://cdn-images-1.medium.com/max/800/0*277Imf2r7ouTXOVy.png" alt></p><p><strong>Airbnb</strong> 是一个快速增长的、数据启示型的公司。我们的数据团队和数据量都在快速地增长，同时我们所面临的挑战的复杂性也在同步增长。我们正在扩张的数据工程师、数据科学家和分析师团队在使用 <strong>Airflow</strong>，它是我们搭建的一个可以快速推进工作，保持发展优势的平台，因为我们可以自己编辑、监控和改写 <strong>数据管道</strong>。</p><p>今天，我们非常自豪地宣布我们要 <strong>开源</strong> 和 <strong>共享</strong> 我们的工作流程管理平台：<strong>Airflow</strong>。</p><p><a href="https://github.com/apache/incubator-airflow" target="_blank" rel="noopener">https://github.com/airbnb/airflow</a></p><hr><h3 id="有向无环图（DAGs）呈绽放之势"><a href="#有向无环图（DAGs）呈绽放之势" class="headerlink" title="有向无环图（DAGs）呈绽放之势"></a>有向无环图（DAGs）呈绽放之势</h3><p>当与数据打交道的工作人员开始将他们的流程自动化，那么写批处理作业是不可避免的。这些作业必须按照一个给定的时间安排执行，它们通常依赖于一组已有的数据集，并且其它的作业也会依赖于它们。即使你让好几个数据工作节点在一起工作很短的一段时间，用于计算的批处理作业也会很快地扩大成一个复杂的图。现在，如果有一个工作节奏快、中型规模的数据团队，而且他们在几年之内要面临不断改进的数据基础设施，并且手头上还有大量复杂的计算作业网络。那这个复杂性就成为数据团队需要处理，甚至深入了解的一个重要负担。</p><p>这些作业网络通常就是 <strong>有向无环图</strong>（<strong>DAGs</strong>），它们具有以下属性：</p><ul><li><strong>已排程：</strong> 每个作业应该按计划好的时间间隔运行</li><li><strong>关键任务：</strong> 如果一些作业没有运行，那我们就有麻烦了</li><li><strong>演进：</strong> 随着公司和数据团队的成熟，数据处理也会变得成熟</li><li><strong>异质性：</strong> 现代化的分析技术栈正在快速发生着改变，而且大多数公司都运行着好几个需要被粘合在一起的系统</li></ul><h3 id="每个公司都有一个（或者多个）"><a href="#每个公司都有一个（或者多个）" class="headerlink" title="每个公司都有一个（或者多个）"></a>每个公司都有一个（或者多个）</h3><p><strong>工作流程管理</strong> 已经成为一个常见的需求，因为大多数公司内部有多种创建和调度作业的方式。你总是可以从古老的 cron 调度器开始，并且很多供应商的开发包都自带调度功能。下一步就是创建脚本来调用其它的脚本，这在短期时间内是可以工作的。最终，一些为了解决作业状态存储和依赖的简单框架就涌现了。</p><p>通常，这些解决方案都是 <strong>被动增长</strong> 的，它们都是为了响应特定作业调度需求的增长，而这通常也是因为现有的这种系统的变种连简单的扩展都做不到。同时也请注意，那些编写数据管道的人通常不是软件工程师，并且他们的任务和竞争力都是围绕着处理和分析数据的，而不是搭建工作流程管理系统。</p><p>鉴于公司内部工作流程管理系统的成长总是比公司的需求落后至少一代，作业的编辑、调度和错误排查之间的 <strong>摩擦</strong> 制造了大量低效且令人沮丧的事情，这使得数据工作者和他们的高产出路线背道而驰。</p><h3 id="Airflow"><a href="#Airflow" class="headerlink" title="Airflow"></a>Airflow</h3><p>在评审完开源解决方案，同时听取 Airbnb 的员工对他们过去使用的系统的见解后，我们得出的结论是市场上没有任何可以满足我们当前和未来需求的方案。我们决定搭建一个崭新的系统来正确地解决这个问题。随着这个项目的开发进展，我们意识到我们有一个极好的机会去回馈我们也极度依赖的开源社区。因此，我们决定依照 Apache 的许可开源这个项目。</p><p>这里是 Airbnb 的一些靠 Airflow 推动的处理工序：</p><ul><li><strong>数据仓储：</strong> 清洗、组织规划、数据质量检测并且将数据发布到我们持续增长的数据仓库中去</li><li><strong>增长分析：</strong> 计算关于住客和房主参与度的指标以及增长审计</li><li><strong>试验：</strong> 计算我们 A/B 测试试验框架的逻辑并进行合计</li><li><strong>定向电子邮件：</strong> 对目标使用规则并且通过群发邮件来吸引用户</li><li><strong>会话（Sessionization）：</strong> 计算点击流和停留时间的数据集</li><li><strong>搜索：</strong> 计算搜索排名相关的指标</li><li><strong>数据基础架构维护：</strong> 数据库抓取、文件夹清理以及应用数据留存策略…</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>就像英语是商务活动经常使用的语言一样，Python 已经稳固地将自己树立为数据工作的语言。Airflow 从创建之初就是用 Python 编写的。代码库可扩展、文档齐全、风格一致、语法过检并且有很高的单元测试覆盖率。</p><p>管道的编写也是用 Python 完成的，这意味着通过配置文件或者其他元数据进行动态管道生成是与生俱来的。“<strong>配置即代码</strong>” 是我们为了达到这个目的而坚守的准则。虽然基于 yaml 或者 json 的作业配置方式可以让我们用任何语言来生成 Airflow 数据管道，但是我们感觉到转化过程中的一些流动性丧失了。能够内省代码（ipython！和集成开发工具）子类和元程序并且使用导入的库来帮助编写数据管道为 Airflow 增加了巨大的价值。注意，只要你能写 Python 代码来解释配置，你还是可以用任何编程语言或者标记语言来编辑作业。</p><p>你仅需几行命令就可以让 Airflow 运行起来，但是它的完整架构包含有下面这么多组件：</p><ul><li><strong>作业定义</strong>，包含在源代码控制中。</li><li>一个丰富的 <strong>命令行工具</strong> (命令行接口) 用来测试、运行、回填、描述和清理你的有向无环图的组成部件。</li><li>一个 <strong>web 应用程序</strong>，用来浏览有向无环图的定义、依赖项、进度、元数据和日志。web 服务器打包在 Airflow 里面并且是基于 Python web 框架 Flask 构建的。</li><li>一个 <strong>元数据仓库</strong>，通常是一个 MySQL 或者 Postgres 数据库，Airflow 可以用它来记录任务作业状态和其它持久化的信息。</li><li>一组 <strong>工作节点</strong>，以分布式的方式运行作业的任务实例。</li><li><strong>调度</strong> 程序，触发准备运行的任务实例。</li></ul><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>Airflow 自带各种与 Hive、Presto、MySQL、HDFS、Postgres 和 S3 这些常用系统交互的方法，并且允许你触发任意的脚本，基础模块也被设计得非常容易进行扩展。</p><p><strong>Hooks</strong> 被定义成外部系统的抽象并且共享同样的接口。Hooks 使用中心化的 vault 数据库将主机/端口/登录名/密码信息进行抽象并且提供了可供调用的方法来跟这些系统进行交互。</p><p><strong>操作符</strong> 利用 hooks 生成特定的任务，这些任务在实例化后就变成了数据流程中的节点。所有的操作符都派生自 BaseOperator 并且继承了一组丰富的属性和方法。三种主流的操作符分别是：</p><ul><li>执行 <strong>动作</strong> 的操作符, 或者通知其它系统去执行一个动作</li><li><strong>转移</strong> 操作符将数据从一个系统移动到另一个系统</li><li><strong>传感器</strong> 是一类特定的操作符，它们会一直运行直到满足了特定的条件</li></ul><p><strong>执行器（Executors）</strong> 实现了一个接口，它可以让 Airflow 组件（命令行接口、调度器和 web 服务器）可以远程执行作业。目前，Airflow 自带一个 SequentialExecutor（用来做测试）、一个多线程的 LocalExecutor、一个使用了 <a href="http://www.celeryproject.org/" target="_blank" rel="noopener">Celery</a> 的 CeleryExecutor 和一个超棒的基于分布式消息传递的异步任务队列。我们也计划在不久后开源 YarnExecutor。</p><h3 id="一个绚丽的用户界面"><a href="#一个绚丽的用户界面" class="headerlink" title="一个绚丽的用户界面"></a>一个绚丽的用户界面</h3><p>虽然 Airflow 提供了一个丰富的<a href="https://airflow.apache.org/cli.html" target="_blank" rel="noopener">命令行接口</a>，但是最好的工作流监控和交互办法还是使用 web 用户接口。你可以容易地图形化显示管道依赖项、查看进度、轻松获取日志、查阅相关代码、触发任务、修正 false positives/negatives 以及分析任务消耗的时间，同时你也能得到一个任务通常在每天什么时候结束的全面视图。用户界面也提供了一些管理功能：管理连接、池和暂停有向无环图的进程。</p><p><img src="https://cdn-images-1.medium.com/max/400/1*nbwR8O-CDH67fkHrXVDvYw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*0Mask8UZw_aCsd_7JM2Rjw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*JNOJotSnC3t0TIQC8gYcsg.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/600/1*qqOg_8bMS_MzDgWSbgdtOw.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*rNaZuJ2168jvUYiEkdu1ww.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*ojItdtSC6etsUWOZIK8trw.png" alt></p><p>锦上添花的是，用户界面有一个 <a href="https://airflow.apache.org/profiling.html" target="_blank" rel="noopener">Data Profiling</a> 区，可以让用户在注册好的连接上进行 SQL 查询、浏览结果集，同时也提供了创建和分享一些简单图表的方法。这个制图应用是由 <a href="http://www.highcharts.com/" target="_blank" rel="noopener">Highcharts</a>、<a href="https://flask-admin.readthedocs.org/en/v1.0.9/" target="_blank" rel="noopener">Flask Admin</a> 的增删改查接口以及 Airflow 的 <a href="https://airflow.apache.org/code.html#hooks" target="_blank" rel="noopener">hooks</a> 和 <a href="https://airflow.apache.org/code.html#macros" target="_blank" rel="noopener">宏</a>库混搭而成的。URL 参数可以传递给你图表中使用的 SQL，Airflow 的宏是通过 <a href="http://jinja.pocoo.org/" target="_blank" rel="noopener">Jinja templating</a> 的方式工作的。有了这些特性和查询功能，Airflow 用户可以很容易的创建和分享结果集和图表。</p><p><img src="https://cdn-images-1.medium.com/max/400/1*8SD5x-62kLVzZ9SSfAXKCg.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*2L-uvEnYDvf5FG3eMuknuQ.png" alt></p><p><img src="https://cdn-images-1.medium.com/max/400/1*EbUXRyeS65GZTXbCPWrF7w.png" alt></p><h3 id="一种催化剂"><a href="#一种催化剂" class="headerlink" title="一种催化剂"></a>一种催化剂</h3><p>使用 Airflow 之后，Airbnb 的员工进行数据工作的生产率和热情提高了好g几倍。管道的编写也加速了，监控和错误排查所花费的时间也显著减少了。更重要的是，这个平台允许人们从一个更高级别的抽象中去创建可重用的模块、计算框架以及服务。</p><h3 id="说得够多的了！"><a href="#说得够多的了！" class="headerlink" title="说得够多的了！"></a>说得够多的了！</h3><p>我们已经通过一个启发式的教程把试用 Airflow 变得极其简单。想看到示例结果也只需要执行几个 shell 命令。看一看 <a href="https://airflow.apache.org/" target="_blank" rel="noopener">Airflow 文档</a> 的<a href="https://airflow.apache.org/start.html" target="_blank" rel="noopener">快速上手</a>和<a href="https://airflow.apache.org/tutorial.html" target="_blank" rel="noopener">教程</a>部分，你可以在几分钟之内就让你的 Airflow web 程序以及它自带的交互式实例跑起来！</p><p><a href="https://github.com/apache/incubator-airflow" target="_blank" rel="noopener">https://github.com/airbnb/airflow</a></p><p><img src="https://cdn-images-1.medium.com/max/800/1*YsUOrWx3mRxZZljtc9xZyw.png" alt></p><h4 id="在-airbnb-io-上查看我们所有的开源项目并-在-Twitter-上关注我们：-AirbnbEng-AirbnbData"><a href="#在-airbnb-io-上查看我们所有的开源项目并-在-Twitter-上关注我们：-AirbnbEng-AirbnbData" class="headerlink" title="在 airbnb.io 上查看我们所有的开源项目并 在 Twitter 上关注我们：@AirbnbEng + @AirbnbData"></a>在 <a href="http://airbnb.io" target="_blank" rel="noopener">airbnb.io</a> 上查看我们所有的开源项目并 在 Twitter 上关注我们：<a href="https://twitter.com/AirbnbEng" target="_blank" rel="noopener">@AirbnbEng</a> + <a href="https://twitter.com/AirbnbData" target="_blank" rel="noopener">@AirbnbData</a></h4><blockquote><p>如果发现译文存在错误或其他需要改进的地方，欢迎到 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 <strong>本文永久链接</strong> 即为本文在 GitHub 上的 MarkDown 链接。</p></blockquote><hr><blockquote><p><a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im" target="_blank" rel="noopener">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android" target="_blank" rel="noopener">Android</a>、<a href="https://github.com/xitu/gold-miner#ios" target="_blank" rel="noopener">iOS</a>、<a href="https://github.com/xitu/gold-miner#前端" target="_blank" rel="noopener">前端</a>、<a href="https://github.com/xitu/gold-miner#后端" target="_blank" rel="noopener">后端</a>、<a href="https://github.com/xitu/gold-miner#区块链" target="_blank" rel="noopener">区块链</a>、<a href="https://github.com/xitu/gold-miner#产品" target="_blank" rel="noopener">产品</a>、<a href="https://github.com/xitu/gold-miner#设计" target="_blank" rel="noopener">设计</a>、<a href="https://github.com/xitu/gold-miner#人工智能" target="_blank" rel="noopener">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi" target="_blank" rel="noopener">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi" target="_blank" rel="noopener">知乎专栏</a>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原文地址：&lt;a href=&quot;https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8&quot; target=&quot;_blan
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Airflow" scheme="http://yqian1991.github.io/tags/Airflow/"/>
    
  </entry>
  
  <entry>
    <title>[译] 我们是如何高效实现一致性哈希的</title>
    <link href="http://yqian1991.github.io/System-Design/How-we-implemented-consistent-hashing-efficiently/"/>
    <id>http://yqian1991.github.io/System-Design/How-we-implemented-consistent-hashing-efficiently/</id>
    <published>2018-07-22T14:01:32.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>原文地址：<a href="https://blog.ably.io/how-to-implement-consistent-hashing-efficiently-fe038d59fff2" target="_blank" rel="noopener">How we implemented consistent hashing efficiently</a></li><li>原文作者：<a href="https://blog.ably.io/@n.srushtika?source=post_header_lockup" target="_blank" rel="noopener">Srushtika Neelakantam</a></li><li>译文出自：<a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a></li><li>本文永久链接：<a href="https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-implement-consistent-hashing-efficiently.md" target="_blank" rel="noopener">https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-implement-consistent-hashing-efficiently.md</a></li><li>译者：<a href="https://github.com/yqian1991" target="_blank" rel="noopener">yqian1991</a></li><li>校对者：<a href="https://github.com/Starrier" target="_blank" rel="noopener">Starrier</a></li></ul></blockquote><h1 id="我们是如何高效实现一致性哈希的"><a href="#我们是如何高效实现一致性哈希的" class="headerlink" title="我们是如何高效实现一致性哈希的"></a>我们是如何高效实现一致性哈希的</h1><h2 id="Ably-的实时平台分布在超过-14-个物理数据中心和-100-多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。"><a href="#Ably-的实时平台分布在超过-14-个物理数据中心和-100-多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。" class="headerlink" title="Ably 的实时平台分布在超过 14 个物理数据中心和 100 多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。"></a>Ably 的实时平台分布在超过 14 个物理数据中心和 100 多个节点上。为了保证负载和数据都能够均匀并且一致的分布到所有的节点上，我们采用了一致性哈希算法。</h2><p>在这篇文章中，我们将会理解一致性哈希到底是怎么回事，为什么它是可伸缩的分布式系统架构中的一个重要工具。然后更进一步，我们会介绍可以用来高效率规模化实现一致性哈希算法的数据结构。最后，我们也会带大家看一看用这个算法实现的一个可工作实例。</p><h3 id="再谈哈希"><a href="#再谈哈希" class="headerlink" title="再谈哈希"></a>再谈哈希</h3><p>还记得大学里学的那个古老而原始的哈希方法吗？通过使用哈希函数，我们确保了计算机程序所需要的资源可以通过一种高效的方式存储在内存中，也确保了内存数据结构能被均匀加载。我们也确保了这种资源存储策略使信息检索变得更高效，从而让程序运行得更快。</p><p>经典的哈希方法用一个哈希函数来生成一个伪随机数，然后这个伪随机数被内存空间大小整除，从而将一个随机的数值标识转换成可用内存空间里的一个位置。就如同下面这个函数所示：</p><p><code>location = hash(key) mod size</code></p><p><img src="https://cdn-images-1.medium.com/max/800/1*ojknKxQ7uxGaJEam2nQYWQ.png" alt></p><h3 id="既然如此，我们为什么不能用同样的方法来处理网络请求呢？"><a href="#既然如此，我们为什么不能用同样的方法来处理网络请求呢？" class="headerlink" title="既然如此，我们为什么不能用同样的方法来处理网络请求呢？"></a>既然如此，我们为什么不能用同样的方法来处理网络请求呢？</h3><p>在各种不同的程序、计算机或者用户从多个服务器请求资源的场景里，我们需要一种机制来将请求均匀地分布到可用的服务器上，从而保证负载均衡，并且保持稳定一致的性能。我们可以将这些服务器节点看做是一个或多个请求可以被映射到的位置。</p><p>现在让我们先退一步。在传统的哈希方法中，我们总是假设：</p><ul><li>内存位置的数量是已知的，并且</li><li>这个数量从不改变</li></ul><p>例如，在 Ably，我们一整天里通常需要扩大或者缩减集群的大小，而且我们也要处理一些意外的故障。但是，如果我们考虑前面提到的这些场景的话，我们就不能保证服务器数量是不变的。如果其中一个服务器发生意外故障了怎么办？如果继续使用最简单的哈希方法，结果就是我们需要对每个哈希键重新计算哈希值，因为新的映射完全决定于服务器节点或者内存地址的数量，如下图所示：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*ojknKxQ7uxGaJEam2nQYWQ.png" alt></p><p>节点变化之前</p><p><img src="https://cdn-images-1.medium.com/max/800/1*8wnQ4y-9waQPC6sHdmZgdg.png" alt></p><p>节点变化之后</p><p>在分布式系统中使用简单再哈希存在的问题 — 每个哈希键的存放位置都会变化 — 就是因为每个节点都存放了一个状态；哪怕只是集群数目的一个非常小的变化，都可能导致需要重新排列集群上的所有数据，从而产生巨大的工作量。随着集群的增长，重新哈希的方法是没法持续使用的，因为重新哈希所需要的工作量会随着集群的大小而线性地增长。这就是一致性哈希的概念被引入的场景。</p><h3 id="一致性哈希-—-它到底是什么？"><a href="#一致性哈希-—-它到底是什么？" class="headerlink" title="一致性哈希 — 它到底是什么？"></a>一致性哈希 — 它到底是什么？</h3><p>一致性哈希可以用下面的方式描述：</p><ul><li>它用虚拟环形的结构来表示资源请求者（为了叙述方便，后文将称之为“请求”）和服务器节点，这个环通常被称作一个 <strong>hashring</strong>。</li><li>存储位置的数量不再是确定的，但是我们认为这个环上有无穷多个点并且服务器节点可以被放置到环上的任意位置。当然，我们仍然可以使用哈希函数来选择这个随机数，但是之前的第二个步骤，也就是除以存储位置数量的那一步被省略了，因为存储位置的数量不再是一个有限的数值。</li><li>请求，例如用户，计算机或者无服务（serverless）程序，这些就等同于传统哈希方法中的键，也使用同样的哈希函数被放置到同样的环上。</li></ul><p><img src="https://cdn-images-1.medium.com/max/800/1*002BDjvoadVRbPyo0lkuiQ.png" alt></p><p>那么它到底是如何决定请求被哪个服务器所服务呢？如果我们假设这个环是有序的，而且在环上进行顺时针遍历就对应着存储地址的增长顺序，每个请求可以被顺时针遍历过程中所遇到的第一个节点所服务；也就是说，第一个在环上的地址比请求的地址大的服务器会服务这个请求。如果请求的地址比节点中的最大地址还大，那它会反过来被拥有最小地址的那个服务器服务，因为在这个环上的遍历是以循环的方式进行的。方法用下图进行了阐明：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*yhBejrSaatHa4b0gr96tvQ.png" alt></p><p>理论上，每个服务器‘拥有’哈希环（hashring）上的一段区间范围，任何映射到这个范围里的请求都将被同一个服务器服务。现在好了，如果其中一个服务器出现故障了怎么办，就以节点 3 为例吧，这个时候下一个服务器节点在环上的地址范围就会扩大，并且映射到这个范围的任何请求会被分派给新的服务器。仅此而已。只有对应到故障节点的区间范围内的哈希需要被重新分配，而哈希环上其余的部分和请求 - 服务器的分配仍然不会受到影响。这跟传统的哈希技术正好是相反的，在传统的哈希中，哈希表大小的变化会影响 <em>全部</em> 的映射。因为有了 <strong>一致性哈希</strong>，只有一部分（这跟环的分布因子有关）请求会受已知的哈希环变化的影响。（节点增加或者删除会导致环的变化，从而引起一些请求 - 服务器之间的映射发生改变。）</p><p><img src="https://cdn-images-1.medium.com/max/800/1*59Mn6sT0Wu7qQJmX1FOhtw.png" alt></p><h3 id="一种高效的实现方法"><a href="#一种高效的实现方法" class="headerlink" title="一种高效的实现方法"></a>一种高效的实现方法</h3><p>现在我们对什么是哈希环已经熟悉了…</p><p>我们需要实现以下内容来让它工作：</p><ol><li>一个从哈希空间到集群上所有服务器节点之间的映射，让我们能找到可以服务指定请求的节点。</li><li>一个集群上每个节点所服务的请求的集合。在后面，这个集合可以让我们找到哪些哈希因为节点的增加或者删除而受到了影响。</li></ol><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><p>要完成上述的第一个部分，我们需要以下内容：</p><ul><li>一个哈希函数，用来计算已知请求的标识（ID）在环上对应的位置。</li><li>一种方法，用来寻找转换为哈希值的请求标识所对应的节点。</li></ul><p>为了找到与特定请求相对应的节点，我们可以用一种简单的数据结构来阐释，它由以下内容组成：</p><ul><li>一个与环上的节点一一对应的哈希数组。</li><li>一张图（哈希表），用来寻找与已知请求相对应的服务器节点。</li></ul><p>这实际上就是一个有序图的原始表示。</p><p>为了能在以上数据结构中找到可以服务于已知哈希值的节点，我们需要：</p><ul><li>执行修改过的二分搜索，在数组中查找到第一个等于或者大于（≥）你要查询的哈希值所对应的节点 — 哈希映射。</li><li>查找在图中发现的节点 — 哈希映射所对应的那个节点。</li></ul><h4 id="节点的增加或者删除"><a href="#节点的增加或者删除" class="headerlink" title="节点的增加或者删除"></a>节点的增加或者删除</h4><p>在这篇文章的开头我们已经看到了，当一个节点被添加，哈希环上的一部分区间范围，以及它所包括的各种请求，都必须被分配到这个新节点。反过来，当一个节点被删除，过去被分配到这个节点的请求都将需要被其他节点处理。</p><h4 id="如何寻找到被哈希环的改变所影响的那些请求？"><a href="#如何寻找到被哈希环的改变所影响的那些请求？" class="headerlink" title="如何寻找到被哈希环的改变所影响的那些请求？"></a>如何寻找到被哈希环的改变所影响的那些请求？</h4><p>一种解决方法就是遍历分配到一个节点的所有请求。对每个请求，我们判断它是否处在环发生变化的区间范围内，如果有需要的话，把它转移到其他地方。</p><p>然而，这么做所需要的工作量会随着节点上请求数量的增加而增加。让情况变得更糟糕的是，随着节点数量的增加，环上发生变化的数量也可能会增加。最坏的情况是，由于环的变化通常与局部故障有关，与环变化相关联的瞬时负载也可能增加其他受影响节点发生故障的可能性，有可能导致整个系统发生级联故障。</p><p>考虑到这个因素，我们希望请求的重定位做到尽可能高效。最理想的情况是，我们可以将所有请求保存在一种数据结构里，这样我们能找到环上任何地方发生哈希变化时受到影响的请求。</p><h4 id="高效查找受影响的哈希值"><a href="#高效查找受影响的哈希值" class="headerlink" title="高效查找受影响的哈希值"></a>高效查找受影响的哈希值</h4><p>在集群上增加或者删除一个节点将改变环上一部分请求的分配，我们称之为 <strong>受影响范围</strong>（<strong>affected range</strong>）。如果我们知道受影响范围的边界，我们就可以把请求转移到正确的位置。</p><p>为了寻找受影响范围的边界，我们从增加或者删除掉的一个节点的哈希值 H 开始，从 H 开始绕着环向后移动（图中的逆时针方向），直到找到另外一个节点。让我们将这个节点的哈希值定义为 S（作为开始）。从这个节点开始逆时针方向上的请求会被指定给它（S），因此它们不会受到影响。</p><p><strong>注意：这只是实际将发生的情况的一个简化描述；在实践中，数据结构和算法都更加复杂，因为我们使用的复制因子（replication factors）数目大于 1，并且当任意给定的请求都只有一部分节点可用的情况下，我们还会使用专门的复制策略。</strong></p><p>那些哈希值在被找到的节点和增加（或者删除）的节点范围之间的请求就是需要被移动的。</p><h4 id="高效查找受影响范围内的请求"><a href="#高效查找受影响范围内的请求" class="headerlink" title="高效查找受影响范围内的请求"></a>高效查找受影响范围内的请求</h4><p>一种解决方法就是简单的遍历对应于一个节点的所有请求，并且更新那些哈希值映射到此范围内的请求。</p><p>在 JavaScript 中类似这样：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> request <span class="keyword">of</span> requests) &#123;</span><br><span class="line">  <span class="keyword">if</span> (contains(S, H, request.hash)) &#123;</span><br><span class="line">    <span class="comment">/* 这个请求受环变化的影响 */</span></span><br><span class="line">    request.relocate();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">contains</span>(<span class="params">lowerBound, upperBound, hash</span>) </span>&#123;</span><br><span class="line">   <span class="keyword">const</span> wrapsOver = upperBound &lt; lowerBound;</span><br><span class="line">   <span class="keyword">const</span> aboveLower = hash &gt;= lowerBound;</span><br><span class="line">   <span class="keyword">const</span> belowUpper = upperBound &gt;= hash;</span><br><span class="line">   <span class="keyword">if</span> (wrapsOver) &#123;</span><br><span class="line">     <span class="keyword">return</span> aboveLower || belowUpper;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">return</span> aboveLower &amp;&amp; belowUpper;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于哈希环是环状的，仅仅查找 S &lt;= r &lt; H 之间的请求是不够的，因为 S 可能比 H 大（表明这个区间范围包含了哈希环的最顶端的开始部分）。函数 <code>contains()</code> 可以处理这种情况。</p><p>只要请求数量相对较少，或者节点的增加或者删除的情况也相对较少出现，遍历一个给定节点的所有请求还是可行的。</p><p>然而，随着节点上的请求数量的增加，所需的工作量也随之增加，更糟糕的是，随着节点的增加，环变化也可能发生得更频繁，无论是因为在自动节点伸缩（automated scaling）或者是故障转换（failover）的情况下为了重新均衡访问请求而触发的整个系统上的并发负载。</p><p>最糟的情况是，与这些变化相关的负载可能增加其它节点发生故障的可能性，有可能导致整个系统范围的级联故障。</p><p>为了减轻这种影响，我们也可以将请求存储到类似于之前讨论过的一个单独的环状数据结构中，在这个环里，一个哈希值直接映射到这个哈希对应的请求。</p><p>这样我们就能通过以下步骤来定位受影响范围内的所有请求：</p><ul><li>定位从 S 开始的第一个请求。</li><li>顺时针遍历直到你找到了这个范围以外的一个哈希值。</li><li>重新定位落在这个范围之内的请求。</li></ul><p>当一个哈希更新时所需要遍历的请求数量平均是 R/N，R 是定位到这个节点范围内的请求数量，N 是环上哈希值的数量，这里我们假设请求是均匀分布的。</p><hr><p>让我们通过一个可工作的例子将以上解释付诸实践：</p><p>假设我们有一个包含节点 A 和 B 的集群。</p><p>让我们随机的产生每个节点的 ‘哈希分配’：（假设是32位的哈希），因此我们得到了</p><p><code>A:0x5e6058e5</code></p><p><code>B:0xa2d65c0</code></p><p>在此我们将节点放到一个虚拟的环上，数值 <code>0x0</code>、<code>0x1</code> 和 <code>0x2</code>… 是被连续放置到环上的直到 <code>0xffffffff</code>，就这样在环上绕一个圈后 <code>0xffffffff</code> 的后面正好跟着的就是 <code>0x0</code>。</p><p>由于节点 A 的哈希是 <code>0x5e6058e5</code>，它负责的就是从 <code>0xa2d65c0+1</code> 到 <code>0xffffffff</code>，以及从 <code>0x0</code> 到 <code>0x5e6058e5</code> 范围里的任何请求，如下图所示：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*inKL8q-CTZ6Asl_uSpDYew.png" alt></p><p>另一方面，B 负责的是从 <code>0x5e6058e5+1</code> 到 <code>0xa2d65c0</code> 的范围。如此，整个哈希空间都被划分了。</p><p>从节点到它们的哈希之间的映射在整个集群上是共享的，这样保证了每次环计算的结果总是一致的。因此，任何节点在需要服务请求的时候都可以判断请求放在哪里。</p><p>比如我们需要寻找 （或者创建）一个新的请求，这个请求的标识符是 ‘<a href="mailto:bobs.blog@example.com" target="_blank" rel="noopener">bobs.blog@example.com</a>’。</p><ol><li>我们计算这个标识的哈希 H ，比如得到的是 <code>0x89e04a0a</code></li><li>我们在环上寻找拥有比 H 大的哈希值的第一个节点。这里我们找到了 B。</li></ol><p>因此 B 是负责这个请求的节点。如果我们再次需要这个请求，我们将重复以上步骤并且又会得到同样的节点，它会包含我们需要的的状态。</p><p>这个例子是过于简单了。在实际情况中，只给每个节点一个哈希可能导致负载非常不均匀的分布。你可能已经注意到了，在这个例子中，B 负责环的 <code>(0xa2d656c0-0x5e6058e5)/232 = 26.7%</code>，同时 A 负责剩下的部分。理想的情况是，每个节点可以负责环上同等大小的一部分。</p><p>让分布更均衡合理的一种方法是为每个节点产生多个随机哈希，像下面这样：</p><p><img src="https://cdn-images-1.medium.com/max/800/1*7qNhuMpoIWhatDWSOJaZVA.png" alt></p><p>事实上，我们发现这样做的结果照样令人不满意，因此我们将环分成 64 个同样大小的片段并且确保每个节点都会被放到每个片段中的某个位置；这个的细节就不是那么重要了。反正目的就是确保每个节点能负责环上同等大小的一部分，因此保证负载是均匀分布的。（为每个节点产生多个哈希的另一个优势就是哈希可以在环上逐渐的被增加或者删除，这样就避免了负载的突然间的变化。）</p><p>假设我们现在在环上增加一个新节点叫做 C，我们为 C 产生一个随机哈希值。</p><p><code>A:0x5e6058e5</code></p><p><code>B:0xa2d65c0</code></p><p><code>C:0xe12f751c</code></p><p>现在，<code>0xa2d65c0 + 1</code> 和 <code>0xe12f751c</code> （以前是属于A的部分）之间的环空间被分配给了 C。所有其他的请求像以前一样继续被哈希到同样的节点。为了处理节点职责的变化，这个范围内的已经分配给 A 的所有请求需要将它们的所有状态转移给 C。</p><p><img src="https://cdn-images-1.medium.com/max/800/1*9EH-yVTX8U9dxRdQ7pjK1Q.png" alt></p><p>现在你理解了为什么在分布式系统中均衡负载是需要哈希的。然而我们需要一致性哈希来确保在环发生任何变化的时候最小化集群上所需要的工作量。</p><p>另外，节点需要存在于环上的多个地方，这样可以从统计学的角度保证负载被均匀分布。每次环发生变化都遍历整个哈希环的效率是不高的，随着你的分布式系统的伸缩，有一种更高效的方法来决定什么发生了变化是很必要的，它能帮助你尽可能的最小化环变化带来的性能上的影响。我们需要新的索引和数据类型来解决这个问题。</p><hr><p>构建分布式系统是很难的事情。但是我们热爱它并且我们喜欢谈论它。如果你需要依靠一种分布式系统的话，选择 Ably。如果你想跟我们谈一谈的话，联系我们！</p><p>在此特别感谢 Ably 的分布式系统工程师 <a href="https://github.com/jdmnd" target="_blank" rel="noopener">John Diamond</a> 对本文的贡献。</p><hr><p><img src="https://cdn-images-1.medium.com/max/800/1*L6S-jVuznYcx4W1o4i9i9w.jpeg" alt></p><p>Srushtika 是 <a href="http://ably.io" target="_blank" rel="noopener">Ably Realtime</a>的软件开发顾问</p><p><img src="https://cdn-images-1.medium.com/max/800/1*g_I_lIRmw4_IODWKLJweqw.png" alt></p><p>感谢 <a href="https://medium.com/@john_91129?source=post_page" target="_blank" rel="noopener">John Diamond</a> 和 <a href="https://medium.com/@matt.at.ably?source=post_page" target="_blank" rel="noopener">Matthew O’Riordan</a>。</p><blockquote><p>如果发现译文存在错误或其他需要改进的地方，欢迎到 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 <strong>本文永久链接</strong> 即为本文在 GitHub 上的 MarkDown 链接。</p></blockquote><hr><blockquote><p><a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im" target="_blank" rel="noopener">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android" target="_blank" rel="noopener">Android</a>、<a href="https://github.com/xitu/gold-miner#ios" target="_blank" rel="noopener">iOS</a>、<a href="https://github.com/xitu/gold-miner#前端" target="_blank" rel="noopener">前端</a>、<a href="https://github.com/xitu/gold-miner#后端" target="_blank" rel="noopener">后端</a>、<a href="https://github.com/xitu/gold-miner#区块链" target="_blank" rel="noopener">区块链</a>、<a href="https://github.com/xitu/gold-miner#产品" target="_blank" rel="noopener">产品</a>、<a href="https://github.com/xitu/gold-miner#设计" target="_blank" rel="noopener">设计</a>、<a href="https://github.com/xitu/gold-miner#人工智能" target="_blank" rel="noopener">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner" target="_blank" rel="noopener">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi" target="_blank" rel="noopener">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi" target="_blank" rel="noopener">知乎专栏</a>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原文地址：&lt;a href=&quot;https://blog.ably.io/how-to-implement-consistent-hashing-efficiently-fe038d59fff2&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
      <category term="Algorithm" scheme="http://yqian1991.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Things I learnt from colleagues</title>
    <link href="http://yqian1991.github.io/Software-Development/Things-I-learnt-from-colleagues/"/>
    <id>http://yqian1991.github.io/Software-Development/Things-I-learnt-from-colleagues/</id>
    <published>2018-05-11T19:50:01.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>I always believe there are always things you can learn from your colleagues no matter what size of your company you are working for, what industry you are in, but a good company culture can help you grow faster.</p><h1 id="Days-in-OMSignal"><a href="#Days-in-OMSignal" class="headerlink" title="Days in OMSignal"></a>Days in OMSignal</h1><p>OMSignal is a wearable tech startup company, it is my first job after my graduation.<br>The thing I learnt from the first day is that ask, ask, ask. Everybody is busy and it’s not that people don’t want to tell you, without a systematic onboarding guide like a mature company does, no one really know what’s the right thing to tell you, so asking question is the best way to figure out the situation.</p><p>Be transparent, don’t hide problems, at the early days, I sometimes feel afraid to report problems since there are many, and I am afraid people will not be happy if I speak out without solving them by myself. Later I learnt that try to solve problems by yourself, but if you can’t, reporting early will avoid big problems in the future.</p><p>Another lesson I learnt is that besides getting work done, thinking about your career path, because you never know when the company doesn’t need you any more.</p><p>I left OMSignal half years later with a shock, I hope I can learn more before finding the next adventure, but this is how the world works, a company’s outlook decides whether they expanding or reduce.</p><h1 id="Days-at-SurveyMonkey"><a href="#Days-at-SurveyMonkey" class="headerlink" title="Days at SurveyMonkey"></a>Days at SurveyMonkey</h1><p>One month after I left OMSignal, I joined SurveyMonkey through alumni referral. I have so much to talk about, this is a great company that I learnt a lot, a lot.</p><p>This is not a company to the scale like Google, Facebook, Amazon, but the company is making a lot changes everyday to make it a great place to work. The onboarding experience is pretty good, you got everything you need to start. you can start contributing in one week, and at the same time, you will get a glance of how the company works. This is not a large company, but not small, it has more than 700 employees around the world.</p><p>Since I am talking about colleagues, let’s back to it. a great culture nourishes people, everyone is humble, friendly, even I am not good at speaking, but everyone is still like a good friend when you need them. This shapes my attitude in work. Be humble, be friendly, take the company as home.</p><p>Senior developers are really good mentors here, they help you understanding what your teams do, there is no dumb question here, but they helps you to not get confused with it next time, it helps you grow.</p><p>I have 1:1 meeting with my manager biweekly, they share experiences, guide you to the next level, you got a lot intuitions for your career path. I can still clearly think of the cascading path my manager guide me.<br>From designing a small system independently to participate in complex system work, from be active in a meeting to how to attract your audience. From making your code a product to increase your impact both on the tech side also team cooperation.</p><p>This is place that tolerant your fault, but it doesn’t mean people ignore it. they take it seriously with helping you get through it instead of blaming.</p><h1 id="Days-in-the-future"><a href="#Days-in-the-future" class="headerlink" title="Days in the future"></a>Days in the future</h1><p>What I learnt today will help me do better in the future. but I will remember to learning continuously.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I always believe there are always things you can learn from your colleagues no matter what size of your company you are working for, what
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Growth" scheme="http://yqian1991.github.io/tags/Growth/"/>
    
  </entry>
  
</feed>
