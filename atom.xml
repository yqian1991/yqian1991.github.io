<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu of Daphne</title>
  
  <subtitle>春秋笔法·丹枫嫩寒</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yqian1991.github.io/"/>
  <updated>2020-10-07T21:18:20.545Z</updated>
  <id>http://yqian1991.github.io/</id>
  
  <author>
    <name>Yu Qian</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Enable JMX for Scala/Sbt Project</title>
    <link href="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/"/>
    <id>http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/</id>
    <published>2020-10-07T20:22:13.000Z</published>
    <updated>2020-10-07T21:18:20.545Z</updated>
    
    <content type="html"><![CDATA[<p>Java Management Extensions (JMX) is a Java technology that supplies tools for managing and monitoring applications, system objects, devices (such as printers) and service-oriented networks. Those resources are represented by objects called MBeans (for Managed Bean).</p><p>In this blog, I will show the steps to enable it for Scala/Sbt project and also show how to connect VisualVM to a remote server</p><h1 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h1><p>Follow the link below to install VisualVM: <a href="https://visualvm.github.io/" target="_blank" rel="noopener">https://visualvm.github.io/</a>.</p><p>If you don’t want to use VisualVM, Jconsole is an option too.</p><h1 id="Setup-sbt-project-with-JMX"><a href="#Setup-sbt-project-with-JMX" class="headerlink" title="Setup sbt project with JMX"></a>Setup sbt project with JMX</h1><p>Enable jmx settings in <code>build.sbt</code></p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javaOptions <span class="keyword">in</span> Universal ++= Seq(</span><br><span class="line">  <span class="string">"-J-Djavax.management.builder.initial="</span>,</span><br><span class="line">  <span class="string">"-J-Djava.rmi.server.hostname=127.0.0.1"</span>,</span><br><span class="line">  <span class="string">"-J-Dcom.sun.management.jmxremote=true"</span>,</span><br><span class="line">  <span class="string">"-J-Dcom.sun.management.jmxremote.port=9186"</span>,</span><br><span class="line">  <span class="string">"-J-Dcom.sun.management.jmxremote.rmi.port=9186"</span>,</span><br><span class="line">  <span class="string">"-J-Dcom.sun.management.jmxremote.ssl=false"</span>,</span><br><span class="line">  <span class="string">"-J-Dcom.sun.management.jmxremote.authenticate=false"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>With this configured and if you run your program locally, jconsole and VisualVM can auto detect running processes already.</p><h1 id="Setup-for-remote-access"><a href="#Setup-for-remote-access" class="headerlink" title="Setup for remote access"></a>Setup for remote access</h1><p>If you want to to debug service running on a remote server. e.g staging environment. you will need to expose the jmx port at the same time.</p><p>Below instructions are based on a helm chart context, since all our services are deployed to EKS through helm chart.</p><h2 id="Add-ports-to-Service"><a href="#Add-ports-to-Service" class="headerlink" title="Add ports to Service"></a>Add ports to <code>Service</code></h2><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">apiVersion</span>: v1</span><br><span class="line"><span class="attribute">kind</span>: Service</span><br><span class="line"><span class="attribute">metadata</span>:</span><br><span class="line">  ...</span><br><span class="line"><span class="attribute">spec</span>:</span><br><span class="line">  <span class="attribute">type</span>: &#123;&#123; .Values.service.type &#125;&#125;</span><br><span class="line">  <span class="attribute">ports</span>:</span><br><span class="line">    - <span class="attribute">port</span>: &#123;&#123; .Values.service.jmx.port &#125;&#125;</span><br><span class="line">      <span class="attribute">targetPort</span>: jmx</span><br><span class="line">      <span class="attribute">protocol</span>: TCP</span><br><span class="line">      <span class="attribute">name</span>: jmx</span><br><span class="line">  <span class="attribute">selector</span>:</span><br><span class="line">    <span class="attribute">app</span>: &#123;&#123; include <span class="string">"application.name"</span> . &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="Add-ports-to-deployment-yaml"><a href="#Add-ports-to-deployment-yaml" class="headerlink" title="Add ports to deployment.yaml"></a>Add ports to deployment.yaml</h2><p>Under, Deployment.spec.template.spec.containers</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ports:</span><br><span class="line">  - name: jmx</span><br><span class="line">    containerPort: &#123;&#123; <span class="selector-class">.Values</span><span class="selector-class">.container</span><span class="selector-class">.jmx</span><span class="selector-class">.port</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>Finally, you can provide the port number for different envs in Values.yaml</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">container:</span></span><br><span class="line"><span class="symbol">  jmx:</span></span><br><span class="line"><span class="symbol">    port:</span> <span class="number">9186</span></span><br><span class="line"></span><br><span class="line"><span class="symbol">service:</span></span><br><span class="line"><span class="symbol">  type:</span> ClusterIP</span><br><span class="line"><span class="symbol">  jmx:</span></span><br><span class="line"><span class="symbol">    port:</span> <span class="number">9186</span></span><br></pre></td></tr></table></figure><p>Start port forwarding after service deployed to EKS</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kubectl</span> <span class="selector-tag">port-forward</span> <span class="selector-tag">-n</span> <span class="selector-attr">[namespace_name]</span> <span class="selector-tag">service</span>/<span class="selector-attr">[app_name]</span> <span class="selector-tag">9186</span><span class="selector-pseudo">:9186</span></span><br></pre></td></tr></table></figure><h1 id="Visualize-with-jconsole"><a href="#Visualize-with-jconsole" class="headerlink" title="Visualize with jconsole"></a>Visualize with jconsole</h1><p>Start jconsole in command line easily with</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">jconsole</span></span><br></pre></td></tr></table></figure><h2 id="Add-Connection"><a href="#Add-Connection" class="headerlink" title="Add Connection"></a>Add Connection</h2><p><img src="./1jconsole.png" alt="set jconsole connection"></p><h2 id="View-Metrics"><a href="#View-Metrics" class="headerlink" title="View Metrics"></a>View Metrics</h2><p><img src="./1jconsole_2.png" alt="jconsole stats"></p><h1 id="Visualize-with-VisualVM"><a href="#Visualize-with-VisualVM" class="headerlink" title="Visualize with VisualVM"></a>Visualize with VisualVM</h1><h2 id="Add-JMX-Connection"><a href="#Add-JMX-Connection" class="headerlink" title="Add JMX Connection"></a>Add JMX Connection</h2><p><img src="./1open_jmx_connection.png" alt="open jconsole connection"></p><h2 id="Input-JMX-Connection"><a href="#Input-JMX-Connection" class="headerlink" title="Input JMX Connection"></a>Input JMX Connection</h2><p><img src="./1set_jmx_connection.png" alt="set jmx connection"></p><h2 id="Start-exploring"><a href="#Start-exploring" class="headerlink" title="Start exploring"></a>Start exploring</h2><p>Click ok button, wait for a second, then you should see:</p><p><img src="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/1view_jmx.png" alt="JMX stat"></p><p>Start to exploring more data</p><p><img src="http://yqian1991.github.io/System-Design/Enable-JMX-for-Scala-Sbt-Project/1view_jmx_2.png" alt="More JMX stats"></p><h1 id="Tuning-JVM-parameters"><a href="#Tuning-JVM-parameters" class="headerlink" title="Tuning JVM parameters"></a>Tuning JVM parameters</h1><p>If you want to test different JVM settings, this can be done in helm chart Values.yaml:</p><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- <span class="built_in">key</span>: <span class="string">"JAVA_OPTS"</span>    </span><br><span class="line">  <span class="built_in">value</span>: <span class="string">"-XX:+UnlockExperimentalVMOptions -Xms3000M -Xmx3000M -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ParallelGCThreads=4 -XX:+CMSClassUnloadingEnabled -XX:MaxRAMFraction=2 -XX:NewSize=3000M -XX:MaxNewSize=3000M -XX:+CMSParallelRemarkEnabled -XX:MaxMetaspaceSize=1000M -XX:+UseCGroupMemoryLimitForHeap"</span></span><br></pre></td></tr></table></figure><p>This can be added in <code>build.sbt</code> as well, but it loses the flexibility to tune them for different environments e.g staging, performance, production.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Java Management Extensions (JMX) is a Java technology that supplies tools for managing and monitoring applications, system objects, devic
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Design a web hook service</title>
    <link href="http://yqian1991.github.io/System-Design/Design-a-web-hook-service/"/>
    <id>http://yqian1991.github.io/System-Design/Design-a-web-hook-service/</id>
    <published>2020-10-05T20:18:20.000Z</published>
    <updated>2020-10-05T20:28:47.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Compared to traditional API services which is pull based, a push based webhook service stands out in many ways:</p><ul><li>Performance: In real time scenarios (for example, you want to receive real time update of information, metrics or alerts), polling(in an interval) is not efficient.<ul><li>Polling too often may waste resources, polling too late may not process the data in a timely manner.</li></ul></li></ul><ul><li>User experience: Instead of working around the API to pull data, it’s much easier to just handle the data pushed to you based on defined schema.<br>Only process when data is available.</li></ul><p>Many product provide web hooks to strengthen their features like SendGrid webhook, Sparkpost web hook.</p><h1 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h1><p>To have a web hook service working, 3 problems need to be engineered.</p><h2 id="Setup-subscription"><a href="#Setup-subscription" class="headerlink" title="Setup subscription"></a>Setup subscription</h2><p>Allow clients to subscribe to the web hook, an authentication method preferred. Use OAuth 2 or built your own</p><h2 id="Event-Categorization"><a href="#Event-Categorization" class="headerlink" title="Event Categorization"></a>Event Categorization</h2><p>Make it capable for clients to subscribe to different kinds of event categories.</p><p>At the minimum, clients need to provide the following informations to subscribe to the web hook:</p><ul><li>callback_url: String,    For where the web hook send events to</li><li>event categories: List,    For what events web hook should send, eg: unsubscribe events.</li><li>requester: String, For security check.</li><li>status:    String,    Status of the subscription, subscribe, suspend, unsubscribe</li></ul><h2 id="Sending-events"><a href="#Sending-events" class="headerlink" title="Sending events"></a>Sending events</h2><p>Once a subscription is validated, you should send events as soon as them arrive.</p><p>There are different triggering mechanisms for a web hook:</p><ul><li>realtime: Always on sending action.</li><li>scheduled: Useful if only hourly/daily/monthly data required</li></ul><h2 id="Event-schema"><a href="#Event-schema" class="headerlink" title="Event schema"></a>Event schema</h2><p>For events sending, we also needs to design a universal event schema.</p><h2 id="Events-Delivery-Guarantee"><a href="#Events-Delivery-Guarantee" class="headerlink" title="Events Delivery Guarantee"></a>Events Delivery Guarantee</h2><p>Client needs to send a response to acknowledge they received the events.</p><p>If web hook failed to send events or client replied back with a non success response, we should resent events.</p><p>Back pressure in place in case client’s response rate is lag behind the web hook publish rate</p><h2 id="Manage-Web-hook-status"><a href="#Manage-Web-hook-status" class="headerlink" title="Manage Web hook status"></a>Manage Web hook status</h2><p>Since web hook is push based, you should always keep your clients up-to-date the current health status of the web hook.</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>This implementation is based on the requirements of serving push campaign metrics, which will show up in push management dashboard</p><p>TODO</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://mandrill.zendesk.com/hc/en-us/articles/205583257-How-to-Authenticate-Webhook-Requests" target="_blank" rel="noopener">https://mandrill.zendesk.com/hc/en-us/articles/205583257-How-to-Authenticate-Webhook-Requests</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h1&gt;&lt;p&gt;Compared to traditional API servic
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>High Performance Spark Reading Notes</title>
    <link href="http://yqian1991.github.io/System-Design/High-Performance-Spark-I/"/>
    <id>http://yqian1991.github.io/System-Design/High-Performance-Spark-I/</id>
    <published>2020-07-03T16:02:54.000Z</published>
    <updated>2020-07-03T16:33:49.821Z</updated>
    
    <content type="html"><![CDATA[<p>As like many other programming languages or frameworks, writing a toy program is easy but it’s always difficult how to make a program high performant, especially with Spark, which is popular running in distributed environment.</p><p>Spark provides a rich lib of functions to meet various requirements, but you may only need some of them in your daily work. Here are some notes I took during reading the book “High Performance Spark”, with this notes, hope developers can have a better design of spark programs at upfront instead of learn from failures themselves. While a key takeaway is that, instead of only knowing how to use what, it’s more important why it is better.</p><p>Pyspark seems very popular in the market because of the popularity of big data, but since Spark relies heavily on inline function definitions and lambda expressions, which are much more naturally supported in Scala, also lazy evaluation within memory computations makes it particularly unique, this also makes programming in Scala has its own advantages over Python.</p><h1 id="General-RDDs"><a href="#General-RDDs" class="headerlink" title="General RDDs"></a>General RDDs</h1><p>Evert RDD has the following properties:</p><ul><li>A list of partition objects,</li><li>A function for computing an iterator of each partition,</li><li>A list of dependencies on other RDDs,</li><li>A partitioner,</li><li>A list of preferred locations</li></ul><p>Transformations and actions are two categories of operations that working on your data,</p><ul><li>A transformation is a function that return another RDD. e.g: <code>flatMap</code>, <code>map</code>, <code>reduceByKey</code>, <code>groupByKey</code> etc</li><li>An action is an operation that returns something other than an RDD,<br>Actions trigger the scheduler, which builds a directed acyclic graph (called the DAG), based on the dependencies between RDD transformations (I also mentioned this in a previous post).</li></ul><p>Note that not all transformations are 100% lazy. <code>sortByKey</code> needs to evaluate the RDD to determine the range of data, so it involves both a transformation and an action.</p><p>When persisting RDDs, the default implementation of RDDs evicts the least recently used partition (called LRU caching) if the space it takes is required to compute or to cache a new partition. However, you can change this behaviour and control Spark’s memory prioritization with the <code>persistencePriority()</code> function in the RDD class</p><h1 id="Knowing-your-Spark-Program"><a href="#Knowing-your-Spark-Program" class="headerlink" title="Knowing your Spark Program"></a>Knowing your Spark Program</h1><p>The Spark program itself runs in the driver node and sends some instructions to the executors</p><p>When the SparkContext is started, a driver and a series of executors are started on the worker nodes of the cluster. The SparkContext determines how many resources are allotted to each executor</p><p>With each action in a spark application, the Spark scheduler builds an execution graph and launches a Spark job, A job is then divided into stages and tasks. Stage boundary is determined by wide transformations which requires shuffle (narrow and wide transformations).</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J<span class="function"><span class="title">ob</span> -&gt;</span> <span class="function"><span class="title">stages</span> -&gt;</span> tasks</span><br></pre></td></tr></table></figure><p>The execution of job is taken care of by different kinds of schedulers:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dag<span class="built_in"> Scheduler </span>-&gt; Task Scheduler</span><br></pre></td></tr></table></figure><h1 id="DataSet-and-DataFrame"><a href="#DataSet-and-DataFrame" class="headerlink" title="DataSet and DataFrame"></a>DataSet and DataFrame</h1><p>DataFrames and Datasets can be read using the Spark SQL equivalent to a SparkContext, the SparkSession.</p><p>DataFrames are Datasets of a special Row object, which doesn’t provide any compile-time type checking, DataFrames allow Spark’s optimizer to better understand our code and our data, which allows for a new class of optimizations.</p><p>DataSet API is quite different when compared with general RDD APIs, it has its own optimizer and execution plans.<br>jobs are transformed to logical plan and then physical plan which has better performance.</p><h1 id="Spark-ML-and-Spark-MLlib"><a href="#Spark-ML-and-Spark-MLlib" class="headerlink" title="Spark ML and Spark MLlib"></a>Spark ML and Spark MLlib</h1><p>Spark ML provides a higher-level API than MLlib with the goal of allowing users to more easily create practical machine learning pipelines. Spark MLlib is primarily built on top of RDDs and uses functions from Spark Core, while ML is built on top of Spark SQL DataFrames</p><h1 id="Some-optimization-tips"><a href="#Some-optimization-tips" class="headerlink" title="Some optimization tips"></a>Some optimization tips</h1><ul><li><p>Sharing the same partitioner with RDDs are materialized by the same action, so thet will end up being co-located (which can even reduce network traffic).</p></li><li><p>Speed up joins by broad cast join, real world application is way more complicate than this, it depends on the property of the two datasets you want to join. If you are joining a big dataset with a small one, then you can broadcast the small one.</p></li><li><p>Since we can’t control the partitioner for DataFrames or Datasets, so we can’t manually avoid shuffles as you did with core Spark joins</p></li><li><p>Minimizing object creation in your RDD operations:</p><ul><li>Reusing existing objects: but note this may cause object mutable</li><li>Using smaller data structures, using primitive types instead of case classes, objects</li><li>Reduce setup overhead: db connection etc</li><li>Reusing RDD: persist, cache, checkpoint, shuffle files. but notice it is space intensive to store data in memory and will take time to serialize and deserialize</li></ul></li><li><p>Some common issues to avoid when doing key-value transformations:</p><ul><li>Memory error on Spark Driver: Avoid calling <code>collection</code>, <code>collectionAsMap</code> on large dataset.</li><li>OOM on executors: Spark parameter tuning, reduce data loading by filtering or aggregation first</li><li>Shuffle failures: Try to preserve partitioning across narrow transformations to avoid reshuffling data</li><li>Straggler tasks: Make partition size even by customizing partitioner</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;As like many other programming languages or frameworks, writing a toy program is easy but it’s always difficult how to make a program hig
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Spark" scheme="http://yqian1991.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Structured Streaming Internal</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Structured-Streaming-Internal/</id>
    <published>2020-05-31T02:58:25.000Z</published>
    <updated>2020-06-02T20:09:49.342Z</updated>
    
    <content type="html"><![CDATA[<p>Spark structured streaming is implemented in spark sql module.</p><h1 id="Spark-Session"><a href="#Spark-Session" class="headerlink" title="Spark Session:"></a>Spark Session:</h1><p>sparkSession is the entry to Spark sql and structured streaming.</p><ul><li>Spark Context: spark context is the entry point of spark, so it’s naturally the bedrock of spark session as well.</li><li>readStream: Read streaming data in as a DataFrame, this method returns an DataStreamReader object</li><li>StreamingQueryManager: Managing the execution of all streaming quires</li><li>createDataFrame: Generate Data frames from various sources,(DataFrame=Dataset[Row])</li><li>createDataset:</li><li>sql: execute sql quires and return data as DataFrame</li></ul><h1 id="DataStreamReader-and-DataStreamWriter"><a href="#DataStreamReader-and-DataStreamWriter" class="headerlink" title="DataStreamReader and DataStreamWriter"></a>DataStreamReader and DataStreamWriter</h1><ul><li>DataStreamReader: Load streaming data from external sources</li><li>DataStreamWriter: Output streaming data to external sources<br>DataStreamWriter has a start() method which calls <code>df.sparkSession.sessionState.StreamingQueryManager.startQuery()</code> to start streaming query.<br>As you can see, the query is started through StreamingQueryManager.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">streamSource = spark \</span><br><span class="line">  .readStream \</span><br><span class="line">  .format(<span class="string">"kafka"</span>) \</span><br><span class="line">  .option(<span class="string">"kafka.bootstrap.servers"</span>, <span class="type">KAFKA_BROKER</span>) \</span><br><span class="line">  .option(<span class="string">"subscribe"</span>, <span class="type">KAFKA_TOPIC</span>) \</span><br><span class="line">  .option(<span class="string">"startingOffsets"</span>, <span class="string">"earliest"</span>) \</span><br><span class="line">  .option(<span class="string">"group.id"</span>,  <span class="type">CONSUMER_ID</span>) \</span><br><span class="line">  .load()   <span class="comment">// DataStreamReader</span></span><br><span class="line"></span><br><span class="line">output = streamSource \</span><br><span class="line">  .writeStream \</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>) \</span><br><span class="line">  .trigger(processingTime = trigger_interval) \</span><br><span class="line">  .foreach(sink) \</span><br><span class="line">  .start() <span class="comment">// DataStreamWriter</span></span><br></pre></td></tr></table></figure><h1 id="Dataset-API"><a href="#Dataset-API" class="headerlink" title="Dataset API"></a>Dataset API</h1><p>Dataset is a strong typed data structure used to do transformations and actions in structured streaming.</p><ul><li>logicalPlan: After transformations and actions defined on a Dataset, it will be analyzed to logical plan, then optimized to physical plan for final execution.</li></ul><p>Some features can be applied are:</p><ul><li>withWatermark: A watermark defines a point in time before which we assume no more late data is going to arrive. This is useful for late data in streaming situation.</li><li>checkpoint: Apply dataset checkpointing, either eagerly or non eagerly.</li><li><p>cache: Cache dataset to memory.</p><p>A lot transformations can be applied:</p><ul><li>groupBy:</li><li>groupByKey: This will return a KeyValueGroupedDataset</li><li>agg:</li><li>repartition/coalesce: Returns a new dataset by specified partitioning.</li></ul></li></ul><h1 id="StreamingQueryManager"><a href="#StreamingQueryManager" class="headerlink" title="StreamingQueryManager"></a>StreamingQueryManager</h1><p>Manages all the StreamingQuerys active in a <code>SparkSession</code>.</p><ul><li><code>startQuery</code> method: create query with DataFrame and call StreamingExecution start()</li></ul><h1 id="StreamingExecution"><a href="#StreamingExecution" class="headerlink" title="StreamingExecution"></a>StreamingExecution</h1><p>StreamingExecution is a implementation of trait StreamingQuery, it can be:</p><ul><li>ContinousExecution: This mode doesn’t support <code>complete</code> outputMode and aggregation either.</li><li>MicrobatchExecution:</li></ul><p>Some methods in StreamingExecution:</p><ul><li>start(): start a thread of queryEexcutionThread</li><li>queryExecutionThread: This thread will run method runStream()</li><li>runStream(): The method to materialize the streaming query</li><li>runActivatedStream: All implementations need to implement this method. working on the logical plan</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spark structured streaming is implemented in spark sql module.&lt;/p&gt;
&lt;h1 id=&quot;Spark-Session&quot;&gt;&lt;a href=&quot;#Spark-Session&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Understanding</title>
    <link href="http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/"/>
    <id>http://yqian1991.github.io/System-Design/Spark-Streaming-Source-Code-Understanding/</id>
    <published>2020-05-19T20:12:02.000Z</published>
    <updated>2020-05-19T20:35:10.300Z</updated>
    
    <content type="html"><![CDATA[<p>When a streaming context starts, it will start a Job Scheduler:</p><ul><li>Start ReceiverTracker: receive data from source and use BlockManager to save it to memoryStore(BlockManager)</li><li>Start JobGenerator: Periodically check interval, create Jobs based on graph given timestamp and OutputStream</li><li>Other resources: like job listener etc</li></ul><p>Each scheduler is run in eventLoop</p><p>For a streaming application, it usually contains:</p><ul><li>An InputStream: Some examples are SocketInputStream, KafkaInputStream, FileInputStream etc</li><li>Transformations: functions that generate another stream, example functions can be filter, map etc</li><li>OutputStream: Output actions of the streaming application, this will trigger stream to be materialized</li></ul><p>Streaming Context includes a property <code>graph</code> which Input Stream and output stream are registered on.</p><p>How is data read from memoryStore?</p><ul><li>Every DStream has methods to generateJob, getOrCompute, then compute (load from memory)</li></ul><p>A simple class/entity relationship:</p><p>SparkStreamingContext:</p><ul><li>sparkContext: sparkContext can be run by calling runJob method which invoke DagScheduler</li><li>graph: A list of DStream which includes output stream and input stream.</li><li>JobScheduler: See below</li></ul><p>JobScheduler: eventLoop</p><ul><li>JobGenerator: generate jobs based on <code>ssc.graph</code> at <code>batchDuration</code> interval, then submit jobs to JobExecutor</li><li>JobExecutor: execute submitted jobs from JobGenerator, job executed by calling sparkContext runJob func.</li><li>receiverTracker:</li></ul><p>JobGenerator(jobScheduler): eventLoop</p><ul><li>RecurringTimer: interval is batchDuration, trigger <code>GenerateJob</code> action</li><li>generateJobs: generate jobs for each outputStream in <code>ssc.graph</code></li><li>Submit Jobs to jobScheduler</li></ul><p>DStream(ssc):</p><ul><li>foreachRDD: register as outputStream</li><li>register: add stream to outputStream that used to generateJobs()</li><li>generateJob,: call compute method to get RDDs and create Job object</li><li>getOrCompute/compute: get RDDs, either from parent or from memory if it’s InputDStream</li><li>generatedRDDs:</li></ul><p>SparkContext</p><ul><li>DagScheduler</li></ul><p>DagScheduler: eventLoop</p><ul><li>Create stages of a RDD based on RDD dependencies</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source: <span class="type">DStream</span></span><br><span class="line"><span class="keyword">val</span> transformed: <span class="type">DStream</span> = source.transform() <span class="comment">// transformed will store dependency source</span></span><br><span class="line"><span class="keyword">val</span> output = transformed.foreachRDD(println) <span class="comment">// outputStream</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;When a streaming context starts, it will start a Job Scheduler:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start ReceiverTracker: receive data from source and use Bloc
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>The way to async I/O</title>
    <link href="http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/"/>
    <id>http://yqian1991.github.io/Software-Development/Understanding-blocking-non-blocking-sync-async/</id>
    <published>2020-04-15T22:02:39.000Z</published>
    <updated>2020-04-16T03:02:40.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Concurrency-vs-Parallelism"><a href="#Concurrency-vs-Parallelism" class="headerlink" title="Concurrency vs Parallelism"></a>Concurrency vs Parallelism</h1><ul><li><p>Concurrency: Do multiple tasks at the same time.</p></li><li><p>Parallelism: Do one task at a time, but the task can be splitted to multiple sub tasks which can be executed in parallel.</p></li></ul><h1 id="CPU-intensive-vs-I-O-intensive"><a href="#CPU-intensive-vs-I-O-intensive" class="headerlink" title="CPU intensive vs I/O intensive"></a>CPU intensive vs I/O intensive</h1><p>If your program is not interacting with disks, media, devices, network and peripheries, then it is CPU intensive, otherwise it is I/O intensive.</p><p>The mode really effects the performance of your program. For example, if you want to know how many RPS my program can handle:</p><ul><li><p>For CPU intensive: (Number of Cores) / time_to_complete_a_request_in_seconds</p></li><li><p>For I/O intensive: (RAM / worker memory) / time_to_complete_a_request_in_seconds</p><p>In I/O intensive scenarios, CPU is doing nothing, so the performance is limited by how many workers are running, thus memory related.</p></li></ul><h1 id="Blocking-vs-Non-blocking"><a href="#Blocking-vs-Non-blocking" class="headerlink" title="Blocking vs Non-blocking"></a>Blocking vs Non-blocking</h1><p>When a program spending most of the time dealing with I/O and not doing anything else, then it will be blocked by the I/O operations, thus CPU stay there and do nothing.</p><p>In order to reuse CPU during waiting, we need to make it non blocking.</p><p>Essentially, it means instead of waiting, it periodically checking the status of I/O operation, only back to handle it if it finishes, otherwise, allow the system to do other tasks.</p><h1 id="Synchronous-vs-asynchronous"><a href="#Synchronous-vs-asynchronous" class="headerlink" title="Synchronous vs asynchronous"></a>Synchronous vs asynchronous</h1><p>Blocking and synchronous are almost the same, a thread focuses on doing one task, no distractions.</p><p>But the difference between non-blocking and asynchronous sometimes is hard to understand. They can be the same in many ways, especially when you don’t deep dive into it.</p><p>If we understand it at thread level, asynchronous means task can be delegated to a different thread, responses can be communicated by other ways like event driven or an callback mechanism, whereas, non-blocking, the thread needs to periodically checking result until task finished.</p><h1 id="Different-models"><a href="#Different-models" class="headerlink" title="Different models"></a>Different models</h1><p>You can design a program as:</p><ul><li><p>synchronous, non-blocking I/O: Since we still need to periodically checking task status in the same thread, and concurrency can only be achieved by spawning more threads, thus more overhead doing context switch.</p></li><li><p>asynchronous, non-blocking I/O: This is preferred by modern web servers. E.g you can achieve this by one thread using event loop. Some real life examples are Python Twisted, Java Netty.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Concurrency-vs-Parallelism&quot;&gt;&lt;a href=&quot;#Concurrency-vs-Parallelism&quot; class=&quot;headerlink&quot; title=&quot;Concurrency vs Parallelism&quot;&gt;&lt;/a&gt;Concurre
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Async" scheme="http://yqian1991.github.io/tags/Async/"/>
    
  </entry>
  
  <entry>
    <title>Canary Deployment with k8s ingress-controller</title>
    <link href="http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/"/>
    <id>http://yqian1991.github.io/SRE/Canary-Deployment-with-k8s-ingress-controller/</id>
    <published>2020-03-03T01:48:52.000Z</published>
    <updated>2020-03-03T02:02:00.156Z</updated>
    
    <content type="html"><![CDATA[<p>Canary deployment is useful in many scenarios, like A/B testing.<br>k8s ingress-controller makes canary deployment easy, I will introduce the steps of it using the example of a microservice we have.</p><p>This document will not cover how to set up ELB. It will focus only on how to create a canary deployment along side an existing deployment.</p><h2 id="Situation"><a href="#Situation" class="headerlink" title="Situation"></a>Situation</h2><p>We have a service A running in k8s cluster, there is an external host name setup to expose the API to public, Requests sent through this API will be route to this service,</p><p>Here is the public host name setup:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl <span class="builtin-name">get</span> ingress -n ingress</span><br><span class="line">NAME                   HOSTS              <span class="built_in"> ADDRESS </span>  PORTS   AGE</span><br><span class="line">service-a          public.domain.com             80      297d</span><br></pre></td></tr></table></figure></p><p>and it also has k8s service set up:</p><p>So far, this is running happily on production,<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n emaildelivery</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">5</span>          <span class="number">159</span>d</span><br></pre></td></tr></table></figure></p><h2 id="Canary-Deployment"><a href="#Canary-Deployment" class="headerlink" title="Canary Deployment"></a>Canary Deployment</h2><h3 id="Deploy-new-pods-with-correct-annotation"><a href="#Deploy-new-pods-with-correct-annotation" class="headerlink" title="Deploy new pods with correct annotation"></a>Deploy new pods with correct annotation</h3><p>You will need to deploy new pods for sure, without overwriting current pods, you can do this by deploying new pods with a new application name or using a different namespace.</p><p>I want to reuse the same namespace, so I changed the application name to <code>service-a-canary</code>.</p><p>Then comes to the fun part, Keep all the necessary labels as <code>service-a</code> in the resources. Resources you will need to change include <code>service.yaml</code>, <code>deployment.yaml</code>, <code>ingress.yaml</code>.</p><p>Key changes are as follows:</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  namespace:</span> service-a</span><br><span class="line"><span class="symbol">  name:</span> service-a-canary</span><br><span class="line"><span class="symbol">  labels:</span></span><br><span class="line"><span class="symbol">    name:</span> service-a</span><br><span class="line">...</span><br><span class="line"><span class="symbol">selector:</span></span><br><span class="line"><span class="symbol">    matchLabels:</span></span><br><span class="line"><span class="symbol">      app:</span> service-a</span><br></pre></td></tr></table></figure><p>If you release the new pods at this time point, new pods will running fine, but serving no traffic:</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">▶ kubectl get pods -n service-a</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-pckg9         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">195</span>d</span><br><span class="line">service-a<span class="number">-68</span>db48dff7-rxtm4         <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">159</span>d</span><br><span class="line">service-a-canary<span class="number">-6</span>f7784fc7-wg7kz   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d20h</span><br></pre></td></tr></table></figure><h3 id="Enable-canary-with-ingress-controller"><a href="#Enable-canary-with-ingress-controller" class="headerlink" title="Enable canary with ingress-controller"></a>Enable canary with ingress-controller</h3><p>To make canary deployment serving real traffic, simply adding few lines in ingress.yaml</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary: <span class="string">"true"</span></span><br><span class="line">nginx<span class="selector-class">.ingress</span><span class="selector-class">.kubernetes</span><span class="selector-class">.io</span>/canary-weight: <span class="string">"10"</span></span><br></pre></td></tr></table></figure><p>nginx.ingress.kubernetes.io/canary-weight is the percentage of traffic that will be routed to canary deployment.</p><p>NOW, you can check your logs to see if traffic routed to canary deployment.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Canary deployment is useful in many scenarios, like A/B testing.&lt;br&gt;k8s ingress-controller makes canary deployment easy, I will introduce
      
    
    </summary>
    
      <category term="SRE" scheme="http://yqian1991.github.io/categories/SRE/"/>
    
    
      <category term="k8s" scheme="http://yqian1991.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Distributed OLAP Datastore</title>
    <link href="http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/"/>
    <id>http://yqian1991.github.io/System-Design/Realtime-Distributed-OLAP-Datastore/</id>
    <published>2020-02-20T03:32:16.000Z</published>
    <updated>2020-03-28T14:30:29.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Apache-Pinot"><a href="#Apache-Pinot" class="headerlink" title="Apache Pinot"></a>Apache Pinot</h1><p>Pinot is a realtime distributed OLAP datastore for scalable real time analytics with low latency.</p><p>It’s using Apache Helix for cluster management, data stored as segment in Pinot.</p><h2 id="Components"><a href="#Components" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Manage brokers and servers, responsible for assigning segments to servers.</p><h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>Accepting queries from clients and return query results to clients.</p><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>Hosts one or more segments, and respond to queries from broker.<br>Data is stored as segment in servers. Segment is a columnar storage.</p><h2 id="Data-Ingestion"><a href="#Data-Ingestion" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Data can be ingested in real time or offline ingestion mode</p><h3 id="Real-time-ingestion"><a href="#Real-time-ingestion" class="headerlink" title="Real time ingestion"></a>Real time ingestion</h3><p>Data from Kafka or other streaming source can be ingested to Pinot servers directly, and can serve query right away.</p><h3 id="Offline-ingestion"><a href="#Offline-ingestion" class="headerlink" title="Offline ingestion"></a>Offline ingestion</h3><p>Data in storage can be ingested through Pinot controller, and pinot controller will assign segments to Pinot servers.</p><ul><li>Add Schema</li><li>Add Table</li><li>Create Segment</li><li>Upload Segment</li></ul><h2 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h2><p>Pinot query language is very similar to standard query language except that <code>JOIN</code> and <code>LIMIT</code> are not supported.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT &lt;outputColumn&gt; (, outputColumn, outputColumn,<span class="built_in">..</span>.)</span><br><span class="line">  <span class="keyword">FROM</span> &lt;tableName&gt;</span><br><span class="line">  (WHERE <span class="built_in">..</span>. |<span class="built_in"> GROUP </span>BY <span class="built_in">..</span>. | ORDER BY <span class="built_in">..</span>. | TOP <span class="built_in">..</span>. | LIMIT <span class="built_in">..</span>.)</span><br></pre></td></tr></table></figure><h3 id="Indexing-technology"><a href="#Indexing-technology" class="headerlink" title="Indexing technology"></a>Indexing technology</h3><h1 id="Apache-Druid"><a href="#Apache-Druid" class="headerlink" title="Apache Druid"></a>Apache Druid</h1><p>Druid is very similar to Pinot in many ways: both for real time queries, both support real time and offline ingestions. Instead of Helix, Druid uses Apache Zookeeper for coordination.</p><h2 id="Components-1"><a href="#Components-1" class="headerlink" title="Components:"></a>Components:</h2><h3 id="Master-Server-Coordinator-and-overlord-processes"><a href="#Master-Server-Coordinator-and-overlord-processes" class="headerlink" title="Master Server(Coordinator and overlord processes)"></a>Master Server(Coordinator and overlord processes)</h3><p>Manages data availability and ingestion, similar to Pinot controller.</p><h3 id="Query-Server-Broker-and-Router"><a href="#Query-Server-Broker-and-Router" class="headerlink" title="Query Server(Broker and Router)"></a>Query Server(Broker and Router)</h3><p>Same as Pinot, accepting queries from external clients, routing queries to brokers, coordinators and overlords.</p><h3 id="Data-Server-Historical-and-Middle-Manager-processes"><a href="#Data-Server-Historical-and-Middle-Manager-processes" class="headerlink" title="Data Server(Historical and Middle Manager processes)"></a>Data Server(Historical and Middle Manager processes)</h3><p>This is similar to server in Pinot, handles ingestion workloads and stores all queryable data.<br>Druid also provides a Deep Storage component as backup of data.</p><p>Data is stored as segment in Druid as well, but Druid segment always comes with a timestamp.</p><p>Druid supports tiering which allows old data can be moved to clusters with more disk storage but less memory and CPU, This can improve query efficiency.</p><h2 id="Data-Ingestion-1"><a href="#Data-Ingestion-1" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h2><p>Druid also support batch and real time ingestion.</p><h2 id="Query-1"><a href="#Query-1" class="headerlink" title="Query"></a>Query</h2><p>Druid’s native query language is JSON over HTTP, beside this, Druid also provides Druid SQL.</p><h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><h1 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h1><p>Presto was designed for OLAP to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. But unlike Pinot and Druid, Presto is used to connect and query from external data sources, varies from HDFS to Cassandra and traditional database like MySQL.</p><h2 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h2><p>Parsing statements, planning queries, and managing Presto worker nodes.</p><h2 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h2><p>Executing tasks and processing data, Worker nodes fetch data from connectors and exchange intermediate data with each other.</p><h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Since Presto has no its own data storage, it relies on different kind of connectors to get data.<br>Data is then modeled as Catalog, schema and table in Presto.</p><h2 id="Query-Execution"><a href="#Query-Execution" class="headerlink" title="Query Execution"></a>Query Execution</h2><p>Statement -&gt; Queries -&gt; Stages -&gt; Tasks</p><h1 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h1><h1 id="Extra-Reads"><a href="#Extra-Reads" class="headerlink" title="Extra Reads"></a>Extra Reads</h1><p><a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7" target="_blank" rel="noopener">https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7</a></p><p><a href="https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e" target="_blank" rel="noopener">https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Apache-Pinot&quot;&gt;&lt;a href=&quot;#Apache-Pinot&quot; class=&quot;headerlink&quot; title=&quot;Apache Pinot&quot;&gt;&lt;/a&gt;Apache Pinot&lt;/h1&gt;&lt;p&gt;Pinot is a realtime distribute
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>100 Questions About Cassandra</title>
    <link href="http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/"/>
    <id>http://yqian1991.github.io/System-Design/100-Questions-About-Cassandra/</id>
    <published>2020-01-11T17:41:47.000Z</published>
    <updated>2020-01-16T04:06:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fast-facts-about-Cassandra"><a href="#Fast-facts-about-Cassandra" class="headerlink" title="Fast facts about Cassandra"></a>Fast facts about Cassandra</h2><ul><li>A Columnar based fault tolerant NoSQL database</li><li>An AP system (Sacrifice Consistency for Available and Partition)</li><li>Easy to scale horizontally (No master)</li><li>No join or subquery for aggregation</li></ul><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h2><h3 id="What-is-Cassandra’s-Replication-Strategy"><a href="#What-is-Cassandra’s-Replication-Strategy" class="headerlink" title="What is Cassandra’s Replication Strategy?"></a>What is Cassandra’s Replication Strategy?</h3><p>Replication strategies define the technique how the replicas are placed in a cluster.<br>There are mainly two types of Replication Strategy:</p><ul><li>Simple strategy: For single data center</li><li>Network Topology Strategy: For multi-datacenter</li></ul><h3 id="What-is-Cassandra-Consistency-Level"><a href="#What-is-Cassandra-Consistency-Level" class="headerlink" title="What is Cassandra Consistency Level?"></a>What is Cassandra Consistency Level?</h3><p>The minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful</p><ul><li>Write Consistency: ALL, ANY, ONE, EACH_QUORUM, LOCAL_ONE, LOCAL_QUORUM</li><li>Read Consistency: ALL, ONE, TWO, THREE, QUORUM, LOCAL_ONE, LOCAL_QUORUM</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quorum = (sum_of_replication_factors / 2) + 1</span><br></pre></td></tr></table></figure><p><a href="https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html" target="_blank" rel="noopener">https://teddyma.gitbooks.io/learncassandra/content/replication/turnable_consistency.html</a></p><h3 id="What-is-Cassandra’s-compaction-strategy"><a href="#What-is-Cassandra’s-compaction-strategy" class="headerlink" title="What is Cassandra’s compaction strategy?"></a>What is Cassandra’s compaction strategy?</h3><p>To improve read performance as well as to utilize disk space, Cassandra periodically does compaction to create &amp; use new consolidated SSTable files instead of multiple old SSTables.</p><ul><li>SizeTieredCompactionStrategy: for write-intensive workloads</li><li>LeveledCompactionStrategy: read-intensive workloads</li></ul><h3 id="Partition-key-and-clustering-key"><a href="#Partition-key-and-clustering-key" class="headerlink" title="Partition key and clustering key"></a>Partition key and clustering key</h3><p>Partition key is similar to primary key in relational databases, it decides which node to store the record.</p><p>Clustering key is responsible for sorting data within a partition</p><h4 id="Compound-key"><a href="#Compound-key" class="headerlink" title="Compound key"></a>Compound key</h4><p>Compound key are partition keys with multiple columns, but only the first column is considered as partition key and the rest are clustering keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY (p1, c1, c2, c3)</span><br></pre></td></tr></table></figure><h4 id="Composite-key"><a href="#Composite-key" class="headerlink" title="Composite key"></a>Composite key</h4><p>Composite keys are partition keys that consist of multiple columns.<br>But when you do query, you will need to include all partition keys.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PRIMARY KEY ((p1, p2), c1, c2)</span><br></pre></td></tr></table></figure><h3 id="What-are-some-of-Cassandra’s-limitations"><a href="#What-are-some-of-Cassandra’s-limitations" class="headerlink" title="What are some of Cassandra’s limitations?"></a>What are some of Cassandra’s limitations?</h3><ul><li>A single column value is recommended to &lt;= 1 Mb (max is 2Gb)</li><li>Number of rows within a partition is better to below 100,000 items and disk size under 100 Mb</li></ul><h3 id="How-does-Cassandra-use-bloom-filters"><a href="#How-does-Cassandra-use-bloom-filters" class="headerlink" title="How does Cassandra use bloom filters?"></a>How does Cassandra use bloom filters?</h3><p>Cassandra uses bloom filters to check if a partition key exists in any of the SSTables or not, without actually having to read their contents.</p><p>Each SSTable has a bloom filter, bloom filter will be updated when a memtable is flushed to disk.</p><h3 id="What-are-seed-node-in-Cassandra-cluster-setup"><a href="#What-are-seed-node-in-Cassandra-cluster-setup" class="headerlink" title="What are seed node in Cassandra cluster setup?"></a>What are seed node in Cassandra cluster setup?</h3><p>Seeds are used during startup to discover the cluster. Seeds are also referred by new nodes on bootstrap to learn other nodes in ring. When you add a new node to ring, you need to specify at least one live seed to contact. Once a node join the ring, it learns about the other nodes, so it doesn’t need seed on subsequent boot.</p><h3 id="How-to-add-a-new-node-to-a-single-datacenter-cluster"><a href="#How-to-add-a-new-node-to-a-single-datacenter-cluster" class="headerlink" title="How to add a new node to a single datacenter cluster?"></a>How to add a new node to a single datacenter cluster?</h3><ul><li>Calculate tokens for the new node, below is the script to generate tokens with Murmur3Partitioner.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">"print [str(((2**64 / number_of_tokens) * i) - 2**63) for i in range(number_of_tokens)]"</span></span><br></pre></td></tr></table></figure><ul><li><p>Install Cassandra on the node with proper <code>cassandra.yaml</code></p></li><li><p>Use <code>nodetool move</code> to assign new token for it.</p></li><li><p>Use <code>nodetool cleanup</code> to remove keys that no longer belong to the previously existing nodes.</p></li></ul><p><a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html" target="_blank" rel="noopener">https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsAddRplSingleTokenNodes.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fast-facts-about-Cassandra&quot;&gt;&lt;a href=&quot;#Fast-facts-about-Cassandra&quot; class=&quot;headerlink&quot; title=&quot;Fast facts about Cassandra&quot;&gt;&lt;/a&gt;Fast fac
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Cassandra" scheme="http://yqian1991.github.io/tags/Cassandra/"/>
    
      <category term="Database" scheme="http://yqian1991.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>我们的2019</title>
    <link href="http://yqian1991.github.io/Life/%E6%88%91%E4%BB%AC%E7%9A%842019/"/>
    <id>http://yqian1991.github.io/Life/我们的2019/</id>
    <published>2020-01-04T18:28:28.000Z</published>
    <updated>2020-01-11T19:09:54.815Z</updated>
    
    <content type="html"><![CDATA[<p>Feb: We travelled to LA and Daphne watched Jay Chou concert,<br>     I then participated company hackathon in San Mateo, got first prize (received a GoPro)<br>     I passed my G license</p><p>March: I moved to Toronto from Ottawa and started remote working for 4 months till June.<br>       We had our first car.</p><p>April:</p><p>May: We shoot pre-wedding photograph in Toronto.</p><p>June 9: We hold a small wedding ceremony with friends (happy to receive a lot of gifts)</p><p>July: I started a new job in downtown Toronto</p><p>Aug: Daphne’s parents in Canada</p><p>Sep: We spent a long weekend in Algonquin lodge (fishing, boating, hiking etc)</p><p>Oct: We hold Wedding ceremony in China and spent one day in Guangzhou, meet Daphne’s friend</p><p>Oct - Dec: Daphne did a lot Yoga session,  we also went badminton every Friday night.</p><p>Dec: Daphne had a Mont Tremblant - Montreal travel with friends, which I should be there too,<br>     but I went back China because of family emergency.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Feb: We travelled to LA and Daphne watched Jay Chou concert,&lt;br&gt;     I then participated company hackathon in San Mateo, got first prize 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="We" scheme="http://yqian1991.github.io/tags/We/"/>
    
  </entry>
  
  <entry>
    <title>Best practices to use Apache Spark</title>
    <link href="http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/"/>
    <id>http://yqian1991.github.io/System-Design/Best-practises-to-use-Apache-Spark/</id>
    <published>2019-12-01T16:23:03.000Z</published>
    <updated>2019-12-01T17:51:32.939Z</updated>
    
    <content type="html"><![CDATA[<p>Learning notes from DataBricks talks</p><h2 id="Optimizing-File-Loading-And-Partition-Discovery"><a href="#Optimizing-File-Loading-And-Partition-Discovery" class="headerlink" title="Optimizing File Loading And Partition Discovery"></a>Optimizing File Loading And Partition Discovery</h2><p>Data loading is the first step of spark application, when dataset becomes large, data loading time becomes an issue.</p><ul><li>Use Datasource tables</li></ul><p>With tables, partition metadata and schema is managed by Hive, based on which partition pruning can be done at logical planning stage. You can also use Spark SQL API directly when loading from a table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.table(<span class="string">"../path"</span>)</span><br><span class="line"></span><br><span class="line">spark.write.partitionBy(<span class="string">"date"</span>).saveAsTable(<span class="string">"table"</span>)</span><br></pre></td></tr></table></figure><ul><li>Specify <code>basePath</code> if loading from external files directly(e.g CSV or Json files)</li></ul><p>When loading from a file, partition discovery is done for each DataFrame creation<br>, also spark needs to infer schema from files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"inferSchema"</span>, true).load(file)</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(knownSchema).load(file)</span><br></pre></td></tr></table></figure><h2 id="Optimizing-File-Storage-and-Layout"><a href="#Optimizing-File-Storage-and-Layout" class="headerlink" title="Optimizing File Storage and Layout"></a>Optimizing File Storage and Layout</h2><ul><li><p>Prefer columnar over text for analytical queries</p></li><li><p>Compression with splittable storage format</p></li><li><p>Avoid large GZIP files</p></li><li><p>Partitioning and bucketing</p></li></ul><p>Parquet + Snappy is a good candidate</p><h2 id="Optimizing-Queries"><a href="#Optimizing-Queries" class="headerlink" title="Optimizing Queries"></a>Optimizing Queries</h2><ul><li>Tuning spark sql shuffle partitions<br><code>spark.sql.shuffle.partitions</code> is used in shuffle operations like groupBy, repartition, join and window. the default value is <code>200</code>.</li></ul><p>This is hard to tune because the parameter is applied on the whole job, which many operations maybe taken, tuning for certain operations may do no good to others.</p><ul><li><p>Adaptive Execution(&gt; Spark 2.0)<br><code>spark.sql.adaptive.enabled</code><br><code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>: default is 64Mb</p></li><li><p>Understanding Unions</p></li><li><p>Data Skipping Index</p></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.youtube.com/watch?v=iwQel6JHMpA" target="_blank" rel="noopener">Youtube Talks</a><br><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/" target="_blank" rel="noopener">Understanding Spark Source Code</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Learning notes from DataBricks talks&lt;/p&gt;
&lt;h2 id=&quot;Optimizing-File-Loading-And-Partition-Discovery&quot;&gt;&lt;a href=&quot;#Optimizing-File-Loading-And-P
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Starting a new journey</title>
    <link href="http://yqian1991.github.io/Life/Starting-a-new-journey/"/>
    <id>http://yqian1991.github.io/Life/Starting-a-new-journey/</id>
    <published>2019-07-01T18:36:56.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to say goodbye.</p><p>Many colleagues were wondering why I leave when it’s only half year to my 4 years anniversary. This is one of the reasons that makes Survey Monkey great. You will get extra 4 weeks vacation every 4 years. The company cares about employees life a lot and try to balance our work and life. Besides the generous take 4 policy, they provide free breakfast and lunch, support you to attend conference every year, allow working from home if you want, monthly bonus points(can redeem Amazon gift cards) etc. just name a few.</p><p>It was not easy to say goodbye to such amazing benefits, leaving an amazing team is also risky. Although all my team members except me located in Bay area, I never feel I am alone, daily stand up, biweekly 1:1 with manager, sprint planning meetings every two weeks and a lot slack calls for discuss or pair programming, we have established a solid way to do remote communication. because of 3 hours time difference, they try their best to no bother me after 6:00pm (EST), and I try my best to cover incidents or issues happen in the morning (before 12:00pm PST), which proved to work very well.</p><p>I was impressed the warm words the team delivered to me upon hearing my news, they even prepared a thank you card and gift to me which I was really touched. I was even questioning myself: is leaving  good?.</p><p>Nobody know a decision is good or not until it proved to be. we experienced a lot good things in the past, nice people, wonderful place, but as life moving on and us growing up, everything has an end. Leaving is exactly a graduation for me. SurveyMonkey is indeed a university for me, where I focused on technology but also took a lot ‘courses’ in cooperation, leading, communication, understanding. And just like 6 years ago, and almost the same time when I graduated from University. maybe this is also a time for my graduation as an intermediate software engineer.</p><p>Graduation marks the starting of a new journey, a new school, in the new journey, I will be a curious student to learn, be a good team player to contribute, be a better man.</p><p>Last but not least, I will be a good husband to support my family. Thanks to my wife for accompanying me since 2016, the journey is meaningless without you.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;June 28th, marked the end of my journey at SurveyMonkey, a great company I had worked for more than 3 years. It’s a bittersweet heart to 
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="Growth" scheme="http://yqian1991.github.io/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>写给Daphne的诗</title>
    <link href="http://yqian1991.github.io/Life/%E5%86%99%E7%BB%99Daphne%E7%9A%84%E8%AF%97/"/>
    <id>http://yqian1991.github.io/Life/写给Daphne的诗/</id>
    <published>2019-05-18T12:36:20.000Z</published>
    <updated>2019-09-21T00:35:13.443Z</updated>
    
    <content type="html"><![CDATA[<pre><code>第一章： 萌芽</code></pre><p>你要问我，我们的故事从哪儿开始，</p><p>走出考场的那一刻，我以为将是故事的结局</p><p>而微信上的只言片语，难道只是我一如既往的淡定？</p><p>也许大家都羡慕一见钟情，</p><p>可比一见钟情更浪漫的，是一聊倾心</p><pre><code>第二章：启 城</code></pre><p>即便我有一双翅膀，我也会将它折断</p><p>因为唾手可得的，到头来也可能只是冷面</p><p>而纵览八百里路云和月，我却恋上你的眼</p><p>如果那双翅膀还在，我必将它修复，</p><p>因为与你的日夜，多想跨过无尽的高山大海</p><p>未来的春夏秋冬，我们可以走得更远</p><p>夜阑人静，午夜梦回，</p><p>你就是我的那双翅膀</p><p>尽管摇摇晃晃， 跌跌撞撞</p><p>但是回头是喜悦，前路是希望</p><pre><code>第三章：誓言</code></pre><p>我记得你每一滴掉下的眼泪，每一次撅起的小嘴</p><p>我记得你激动地辩驳，又原谅的叹息</p><p>相知诚不如相爱那么容易，但是多难都要在一起。</p><p>我记得，你所有有味道的陪伴，邪性的范儿</p><p>我记得，你魔舞的腰枝，磅礴的胃气</p><p>从此，我也会记得，稀饭要稠，汤要浓</p><p>誓言无声，贵在默默坚守</p><p>我心永恒，坚信细水长流</p><p>从此，让我们的爱情澎湃，人生隽永</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;第一章： 萌芽
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;你要问我，我们的故事从哪儿开始，&lt;/p&gt;
&lt;p&gt;走出考场的那一刻，我以为将是故事的结局&lt;/p&gt;
&lt;p&gt;而微信上的只言片语，难道只是我一如既往的淡定？&lt;/p&gt;
&lt;p&gt;也许大家都羡慕一见钟情，&lt;/p&gt;
&lt;p&gt;可比一
      
    
    </summary>
    
      <category term="Life" scheme="http://yqian1991.github.io/categories/Life/"/>
    
    
      <category term="She" scheme="http://yqian1991.github.io/tags/She/"/>
    
  </entry>
  
  <entry>
    <title>Fun topics in distributed system</title>
    <link href="http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/"/>
    <id>http://yqian1991.github.io/System-Design/Fun-topics-in-distributed-system/</id>
    <published>2019-04-26T19:54:05.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one after one.</p><ul><li>Micro services architecture</li><li>CDN, Caching, load balancer</li><li>Event Sourcing</li><li>Messaging</li><li>CAP<br>…</li></ul><p>Needless to say, each of these topics are complex enough and takes years to learn. What we expect is: when we are tasked with a system design problem, we will be able to have a robust architecture overall.</p><p>Well, this is important, but when after you see tons of architectures in different use cases, still there is no one fit all solution. There are always tough challenges come up. and usually, the issue that stops you is not the high level design, it’s some small problems instead.</p><ul><li>Sticky Session</li></ul><p>Now we have a lot machines behind the load balancer can serve the traffic, that means requests from a same user can be served by different nodes, hmm, do I need to sign in again every time if the node doesn’t have my session info?</p><p>Absolutely not necessary, now you may think of sticky session which routes the requests for a particular session to the same physical machine that serviced the first request for that session. But problem is not totally solved at this point yet, this method may still cause uneven loads to service which actually we want to solve by distributed system.</p><p><a href="http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/" target="_blank" rel="noopener">http://www.chaosincomputing.com/2012/05/sticky-sessions-are-evil/</a></p><ul><li>Distributed transactions</li></ul><p>When we throw out a nice distributed architecture, we are confident that it supports high throughputs with decent performance, and you also adds on that, it can handle traffic bursts by adding more machines since it’s a distributed oriented. This looks nice, <em>but how do you handle failures in payment transactions?</em></p><p>I will stuck here for a while, yes, this is not easy, in one machine environment, we can locking and waiting, but if the system is distributed, how can we make sure the transaction ACID?.</p><p>I am not a innovator, but curiosity drives me to find answers with two phase commits and PAXOS. There are tons of materials about those, just giving some I read:<br><a href="https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/" target="_blank" rel="noopener">https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/</a></p><ul><li>Distributed Tracing.<br>It’s common that a user request may cause cascading services calls in the backend, in case of data analysis or debugging, how can we stitch requests to the same origin?</li></ul><p>Using same RequestID is a common strategy, but usually, you will need another service/lib dedicated for distributed tracing. There are some popular frameworks for it, like opentracing.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Distributed system is good, but also introduces more problems you need to solve.<br>Generate a fancy system architecture is easy, what’s more important is how to solve the problems it brings.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;During the first days of learning distributed system design, we heard a lot buzzwords and technologies, and we are busy with learning one
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Distributed System" scheme="http://yqian1991.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Companies (Toronto)</title>
    <link href="http://yqian1991.github.io/Careers/Hidden-Companies/"/>
    <id>http://yqian1991.github.io/Careers/Hidden-Companies/</id>
    <published>2019-04-20T15:32:04.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.</p><p>But there is still a ton of jobs outside those popular sites. And also, most of time, we are interested in opportunities in our local area.</p><p>I did find a lot hidden companies that not easy to be aware of(based in Toronto). Usually due to the following reasons:</p><ul><li><p>They don’t have physical offices in Toronto, but support remote working:<br>Stripe<br>Github<br>DataDog<br>Zapier</p></li><li><p>They are actually good ones/Unicorns, but they only run a tiny team in Toronto, thus less advertised and less presence:<br>Etsy<br>Snap: Through acquisition of Bitmoji<br>Okta<br>iHerb<br>Pivotal (CloudFoundry)<br>Instacart: This one is getting bigger and bigger in Toronto.</p></li></ul><p>Many companies entered Toronto tech scene through acquiring local startups</p><ul><li>They are emerging startups, but some of them will become popular.<br>Dessa<br>Element AI</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There are a lot job websites we use to seek a job, like LinkedIn, GlassDoor, Indeed, Monster.&lt;/p&gt;
&lt;p&gt;But there is still a ton of jobs out
      
    
    </summary>
    
      <category term="Careers" scheme="http://yqian1991.github.io/categories/Careers/"/>
    
    
      <category term="Job seeking" scheme="http://yqian1991.github.io/tags/Job-seeking/"/>
    
  </entry>
  
  <entry>
    <title>NLP in big companies</title>
    <link href="http://yqian1991.github.io/System-Design/NLP-in-big-companies/"/>
    <id>http://yqian1991.github.io/System-Design/NLP-in-big-companies/</id>
    <published>2019-04-19T17:06:13.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out how some other companies build their platforms.</p><h1 id="Uber"><a href="#Uber" class="headerlink" title="Uber"></a>Uber</h1><h2 id="NLP-platform-to-process-customer-support-tickets"><a href="#NLP-platform-to-process-customer-support-tickets" class="headerlink" title="NLP platform to process customer support tickets"></a>NLP platform to process customer support tickets</h2><p>In practice, we may not need very fancy algorithms and some simple ones just work, at least it’s a good starting point of building an engineering product.<br>Uber tried to use NLP to process customer support tickets with a classification model(logistic regression), Data processing is always the key task before machine learning, like how to encoding ticket and transform text, category to numerical vectors, apparently word2vec can be used here.</p><p>In the future, more complex and high performance algorithms an deep learning frameworks can be adopted like WordCNN.</p><p>In terms of the foundation platform, Uber utilizes Spark + hive for big data processing and scaled prediction.</p><p><a href="https://eng.uber.com/nlp-deep-learning-uber-maps/" target="_blank" rel="noopener">https://eng.uber.com/nlp-deep-learning-uber-maps/</a></p><h2 id="Uber-one-click-chat"><a href="#Uber-one-click-chat" class="headerlink" title="Uber one click chat"></a>Uber one click chat</h2><p>This is a smart reply system that auto-reply to user messages.</p><p>From this system, we can have a sense that a typical machine learning platform usually has two components:</p><ul><li><p>Offline training:<br>Using NLP and ML pipelines to do intent detection. Here is where NLP models got applied like Doc2Vec model</p></li><li><p>Online serving:<br>A message will be encoded as fixed-length vector representation via the pre-trained Doc2vec model, after which the vector and the intent detection classifier will be used to predict the message’s possible intent.</p></li></ul><p>The system then retrieve the most relevant replies based on the detected intent and surface them to the driver-partner receiving the message</p><p><a href="https://eng.uber.com/one-click-chat/" target="_blank" rel="noopener">https://eng.uber.com/one-click-chat/</a></p><h1 id="Airbnb"><a href="#Airbnb" class="headerlink" title="Airbnb"></a>Airbnb</h1><p>Airbnb built an online risk mitigation system which it mentioned some requirements of a machine learning platform:</p><ul><li>Fast</li><li>Robust</li><li>Scale</li></ul><p>Although it used an open source framework(OpenScoring) for it, we should know the typical pipeline is the same most of the times like what we saw in Airbnb platforms above. Feel free to check out some characteristics of OpenScoring.</p><p><a href="https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60</a><br><a href="https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d" target="_blank" rel="noopener">https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d</a></p><h1 id="Zendesk"><a href="#Zendesk" class="headerlink" title="Zendesk"></a>Zendesk</h1><p>Zendesk summarizes customer support tickets to topics.</p><p>A big take-way is how they support 50k models on a daily basis with AWS Batch,<br>AWS batch supports auto scaling and job management, it also provides GPU support.<br>In terms of job management, this is a hot topic and demand need for building large scale platforms. A lot products emerges in this regards, like Airflow.</p><p><a href="https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9" target="_blank" rel="noopener">https://medium.com/zendesk-engineering/zendesk-ml-model-building-pipeline-on-aws-batch-monitoring-and-load-testing-8a7decbb5ad9</a></p><h1 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h1><p>Cortex</p><h1 id="LinkedIn"><a href="#LinkedIn" class="headerlink" title="LinkedIn"></a>LinkedIn</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In this blog post, I am trying to find some good examples of building NLP applications in reality. A good starter point is to find out ho
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Natural Language Processing 101</title>
    <link href="http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/"/>
    <id>http://yqian1991.github.io/Data-Science/Natural-Language-Processing-101/</id>
    <published>2019-04-17T14:41:53.000Z</published>
    <updated>2019-09-21T00:35:13.441Z</updated>
    
    <content type="html"><![CDATA[<p>This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.</p><h1 id="What-is-Natural-Language-Processing"><a href="#What-is-Natural-Language-Processing" class="headerlink" title="What is Natural Language Processing?"></a>What is Natural Language Processing?</h1><p>Natural Language Processing (NLP) is an important sub category in Artificial Intelligence that enabling computers to understand and process human languages, it tries to get computers closer to a human-level understanding of language.</p><h2 id="Some-research-topics-in-NLP"><a href="#Some-research-topics-in-NLP" class="headerlink" title="Some research topics in NLP"></a>Some research topics in NLP</h2><ul><li>Information Retrieval/Extraction/Filtering</li><li>Machine Translation</li><li>Document/Topic Classification/Summarization</li><li>Question Answering</li><li>Text Mining</li><li>Sentiment Analysis</li><li>Speech Recognition</li><li>Machine Writing/Content Genetation</li></ul><h1 id="Statistical-Language-Models"><a href="#Statistical-Language-Models" class="headerlink" title="Statistical Language Models"></a>Statistical Language Models</h1><p>This is to compute the probability of a sentence or sequence of words.</p><h2 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h2><p>N-gram is a popular statistical language model.<br>After building a model, we usually use cross-entropy and perplexity to evaluate the model.<br>Lower perplexities correspond to higher likelihoods, so lower scores are better on this<br>metric.</p><p>A major concern in language modeling is to avoid the situation <code>p(w) = 0</code>, which could arise as a result of a single unseen n-gram, the solution is using smoothing methods, some smoothing methods includes:</p><ul><li>Add-One(Laplace) smoothing</li><li>Good-Turing smoothing</li><li>Kneser-Ney smoothing</li><li>Witten-Bell smoothing</li></ul><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>A sentence/document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, POS tag, semantics and syntax etc, are all discarded.</p><h2 id="Probabilistic-Graphical-Models"><a href="#Probabilistic-Graphical-Models" class="headerlink" title="Probabilistic Graphical Models"></a>Probabilistic Graphical Models</h2><p>This is an important math theory/algorithm used in NLP tasks.</p><ul><li>Bayesian Network</li><li>Markov Network</li><li>Condition Random Fields</li><li>Hidden Markov Models</li><li>Estimation Maximization</li><li>Max Entropy</li></ul><h2 id="Topic-Model"><a href="#Topic-Model" class="headerlink" title="Topic Model"></a>Topic Model</h2><ul><li>Latent Dirichlet Allocation (LDA): Based on probabilistic graphical models</li><li>LSA: Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra</li><li>NMF: Non-Negative Matrix Factorization – Based on Linear Algebra</li></ul><h1 id="Some-popular-tasks-in-NLP"><a href="#Some-popular-tasks-in-NLP" class="headerlink" title="Some popular tasks in NLP"></a>Some popular tasks in NLP</h1><p>These are some tasks that may not be the solution to any particular NLP problem but are done as pre-requisites to simplify a lot of different problems in NLP. These are pretty much like reading comprehension we learn in school.</p><h2 id="Parts-of-Speech-Tagging"><a href="#Parts-of-Speech-Tagging" class="headerlink" title="Parts of Speech Tagging"></a>Parts of Speech Tagging</h2><p>Identify Proper nouns, Common nouns, Verbs, Adjectives, Preposition etc.</p><h2 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h2><p>Identify name of people, location etc.</p><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><h2 id="Morphosyntactic-Attributes"><a href="#Morphosyntactic-Attributes" class="headerlink" title="Morphosyntactic Attributes"></a>Morphosyntactic Attributes</h2><h1 id="Deep-Learning-in-NLP"><a href="#Deep-Learning-in-NLP" class="headerlink" title="Deep Learning in NLP"></a>Deep Learning in NLP</h1><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Previously, there are some other popular distributed representation of word as vectors, like Tf-Idf.<br> But they are sparse and long which is not computing efficient. Word2Vec instead is a dense vector representation of words(commonly 100-500 dimensions). and it models the meaning of a word as an embedding.</p><p>But how to get the dense vectors? Singular value    decomposition(Latent Semantic    Analysis) can be used, but a more successful way is through neural network inspired learning strategy.</p><ul><li>CBOW: Predict center/target word based on context words</li><li>Skip-grams: Predict context words based on center/target word.</li></ul><p>Other vector based models include: fastText, Doc2Vec, GloVe etc.</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>Stay tuned…</p><p>Highly recommend <a href="https://people.cs.umass.edu/~miyyer/cs585/" target="_blank" rel="noopener">https://people.cs.umass.edu/~miyyer/cs585/</a> as 101 course for NLP.<br>More advanced courses:<br><a href="https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6" target="_blank" rel="noopener">https://github.com/lovesoft5/ml/tree/master/NLP-%E5%93%A5%E4%BC%A6%E6%AF%94%E4%BA%9A%E5%A4%A7%E5%AD%A6</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is a very simple and naive introductory to summary the knowledge in natural language processing, based on my self learning.&lt;/p&gt;
&lt;h1 
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
      <category term="NLP" scheme="http://yqian1991.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Searching with bloom filter</title>
    <link href="http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/"/>
    <id>http://yqian1991.github.io/Software-Development/Searching-with-bloom-filter/</id>
    <published>2018-10-16T21:09:45.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>Our platform is sending 4 million emails per day, and many of them contains a lot user generated content which has potential risk of spam. A very important action we need to take is: if we know the sender is already marked as a bad email, we should stop sending emails for it right away.</p><p>We now have more than ten thousands of blacklisted emails stored in database, when sending an email, we need to check against this big list to see if we can send the current email out. retrieving all emails from db is not realistic since it has significant latency.</p><h1 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h1><h2 id="Let-DB-do-the-search"><a href="#Let-DB-do-the-search" class="headerlink" title="Let DB do the search"></a>Let DB do the search</h2><p>This is simple to implement and fit the current situation well since all emails are stored in database, we just need a stored procedure to take an email as input and return True if the email is in the table, otherwise false.</p><h2 id="Bloom-filter"><a href="#Bloom-filter" class="headerlink" title="Bloom filter"></a>Bloom filter</h2><p>Thinking out of the box, store with database itself may not be a good solution considering latency of DB read and write. Bloom filter is an excellent algorithm for this kind of large scale search.</p><p>The algorithm is very ideal, but when comes to engineering, we still need to consider the trade-offs based on the current service architecture.</p><ul><li>The current black listed emails are stored in database, and the write operation is done by a different service, we need to consider do we need to change the storage for bloom filter</li><li><p>bloom filter needs tuning to get low false positive rates. considering the growth rate of black listed emails, how do we adjust the size of hash functions and bit array lengths</p><p>Here is a useful tool to do the math: <a href="https://hur.st/bloomfilter/" target="_blank" rel="noopener">https://hur.st/bloomfilter/</a></p></li></ul><h1 id="My-take-aways"><a href="#My-take-aways" class="headerlink" title="My take aways"></a>My take aways</h1><ul><li>Thinking out of the box can bring you more brilliant ideas</li><li>Sometimes, excellent algorithm is not always fit</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Problem-statement&quot;&gt;&lt;a href=&quot;#Problem-statement&quot; class=&quot;headerlink&quot; title=&quot;Problem statement&quot;&gt;&lt;/a&gt;Problem statement&lt;/h1&gt;&lt;p&gt;Our platfo
      
    
    </summary>
    
      <category term="Software Development" scheme="http://yqian1991.github.io/categories/Software-Development/"/>
    
    
      <category term="Algorithm" scheme="http://yqian1991.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Compare streaming frameworks</title>
    <link href="http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/"/>
    <id>http://yqian1991.github.io/System-Design/Compare-streaming-frameworks/</id>
    <published>2018-10-16T20:06:49.000Z</published>
    <updated>2019-12-03T02:25:50.445Z</updated>
    
    <content type="html"><![CDATA[<p>The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is said that master/slave is not good words now). We use spark streaming to read messages from Kafka and rollup metrics, then send to RabbitMQ which will be used to send out notifications.</p><p>There are 20 million questions answered each day on our platform, to better keep customers engaged and provide them better insights, it’s nice to send notifications to the survey owners about the responses they collected, this needs to be happen in a timely manner, so Apache Spark seems to be a good fit.</p><p>Spark is so popular at that time, it is the only streaming framework I know, and I even thought that is the only streaming framework ever exists. Later on, I heard more and more voices that spark is not a real streaming tool, instead a batching process. Now I understand that Apache spark is for batch processing and spark streaming is for mini-batch processing. With the arising of LAMBDA framework, spark becomes popular since it can do both batch and “streaming” process.</p><p>There are some other streaming frameworks in the market, Apache Storm, Apache Samza, Apache Flink etc. It is easy to get lost when facing too many choices, it is also true that each of them has suitable use cases. what’s important is to understand the problem we are going to solve.</p><p>Here are some facts of those frameworks:</p><h1 id="Apache-Storm"><a href="#Apache-Storm" class="headerlink" title="Apache Storm"></a>Apache Storm</h1><p>Open sourced by Twitter and was initially released in 2011.</p><p>Key concepts:</p><ul><li>Spouts: (data source)</li><li>Bolts: functions</li><li>Tuple: stream</li></ul><p>Storm is able to process +1M msgs/second/node, A cluster can be configured to have &gt; 1k workers, Storm provides at least once delivery</p><p>On top of its core, Storm also provides Trident API for micro batch processing,<br>it assures exactly once guarantee.</p><p>With Trident API, you can do aggregation, merge, join, grouping, functions, filters etc.</p><p>Storm supports data transfer protocol: Avro, thrift.</p><h1 id="Apache-Flink"><a href="#Apache-Flink" class="headerlink" title="Apache Flink"></a>Apache Flink</h1><p>Flink is originated from an academic project called <code>Stratosphere</code>, it had the first release after transferring to Apache incubator at 2014.</p><p>Flink unified stream and batch processing, streaming is the nature of flink, and batch is regarded as a special case of streaming which in contrast of Spark.</p><p>Flink provides exactly once processing.</p><p>It has rich higher level API compared to Storm, also support out of order process</p><p>It is able to process 1.5M msgs/second/node</p><h1 id="Apache-Spark"><a href="#Apache-Spark" class="headerlink" title="Apache Spark"></a>Apache Spark</h1><p>Originally developed at the University of California, Berkeley’s AMPLab in 2009 and open sourced in 2010.</p><p>Spark can do exactly once delivery as well, it is working with RDD, but now suggested to use DataFrame/DataSet.</p><p>Spark is also known as hard to tune those complicate parameters for better performance.</p><h1 id="Apache-Samza"><a href="#Apache-Samza" class="headerlink" title="Apache Samza"></a>Apache Samza</h1><p>The project entered Apache Incubator in 2013 and was originally created at LinkedIn.</p><p>Samza achieves at least once delivery.</p><p>Samza is famous for its state management based on RockDB, but it’s tightly coupled with Kafka and Yarn.</p><p>It lacks some advanced features like watermarks, sessions.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The first streaming framework I got to know is Apache Spark, my team owns a small spark cluster which has 1 leader and 4 followers(It is 
      
    
    </summary>
    
      <category term="System Design" scheme="http://yqian1991.github.io/categories/System-Design/"/>
    
    
      <category term="Streaming processing" scheme="http://yqian1991.github.io/tags/Streaming-processing/"/>
    
      <category term="Data pipeline" scheme="http://yqian1991.github.io/tags/Data-pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Notes on data science self learning</title>
    <link href="http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/"/>
    <id>http://yqian1991.github.io/Data-Science/Notes-on-data-science-self-learning/</id>
    <published>2018-08-15T14:54:58.000Z</published>
    <updated>2019-09-21T00:35:13.442Z</updated>
    
    <content type="html"><![CDATA[<p>Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from two people:</p><ul><li><a href="https://www.coxy1989.com/curriculum.html" target="_blank" rel="noopener">https://www.coxy1989.com/curriculum.html</a></li><li><a href="http://karlrosaen.com/ml/" target="_blank" rel="noopener">http://karlrosaen.com/ml/</a>.</li></ul><p>Thanks a lot to them for sharing their own experiences.</p><p>This blog is a guideline that I will continue update. Side by side, I may post learning logs with details on what I learned.</p><h1 id="Stage-1-Foundations"><a href="#Stage-1-Foundations" class="headerlink" title="Stage 1 - Foundations"></a>Stage 1 - Foundations</h1><p>This is to lay a good foundation for later machine learning, which includes:</p><h2 id="Probability-and-Statistics"><a href="#Probability-and-Statistics" class="headerlink" title="Probability and Statistics"></a>Probability and Statistics</h2><p>Bloomberg ML EDU course which I am following at this moment: <a href="https://bloomberg.github.io/foml/#about" target="_blank" rel="noopener">https://bloomberg.github.io/foml/#about</a><br>More resources on statistics: <a href="https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html" target="_blank" rel="noopener">https://cims.nyu.edu/~cfgranda/pages/DSGA1002_fall15/index.html</a><br>Problems and solutions: <a href="http://karlrosaen.com/ml/hw/" target="_blank" rel="noopener">http://karlrosaen.com/ml/hw/</a></p><p>Some supporting materials:</p><ul><li>Mathematics background check: <a href="https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf" target="_blank" rel="noopener">https://davidrosenberg.github.io/mlcourse/Notes/prereq-questions/math-questions.pdf</a></li><li>Simple statistics crib-sheet: <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf" target="_blank" rel="noopener">http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf</a></li></ul><h2 id="Machine-learning-101"><a href="#Machine-learning-101" class="headerlink" title="Machine learning 101"></a>Machine learning 101</h2><p>Now you can have a glance of what machine learning is, and use your mathematics learned to understand concepts and practice on easy tasks.</p><p>Machine learning by Andrew Ng: <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a><br>Python Machine Learning: <a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition" target="_blank" rel="noopener">https://github.com/rasbt/python-machine-learning-book-2nd-edition</a></p><p>There are some good podcasts:</p><ul><li>Talking machine podcasts:<a href="https://www.thetalkingmachines.com/" target="_blank" rel="noopener">https://www.thetalkingmachines.com/</a></li><li>Becoming a data scientist podcasts: <a href="https://www.becomingadatascientist.com/category/podcast/" target="_blank" rel="noopener">https://www.becomingadatascientist.com/category/podcast/</a></li><li>The Master Algorithm: <a href="https://player.fm/series/data-skeptic/the-master-algorithm" target="_blank" rel="noopener">https://player.fm/series/data-skeptic/the-master-algorithm</a></li></ul><p>To complete this stage, 5 months are recommended.</p><h1 id="Stage-2-Applied"><a href="#Stage-2-Applied" class="headerlink" title="Stage 2 - Applied"></a>Stage 2 - Applied</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tons of resources online will get you distracted a lot, a good way is to have your own learning path and keep focus. I got this idea from
      
    
    </summary>
    
      <category term="Data Science" scheme="http://yqian1991.github.io/categories/Data-Science/"/>
    
    
      <category term="Learning Notes" scheme="http://yqian1991.github.io/tags/Learning-Notes/"/>
    
      <category term="Machine learning" scheme="http://yqian1991.github.io/tags/Machine-learning/"/>
    
  </entry>
  
</feed>
